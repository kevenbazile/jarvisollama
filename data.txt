Hello, how are you?
This is a sample dataset to fine-tune the Ollama 7B model.
Let's see how the model behaves after fine-tuning.
API's - Application Programming Interface 

What is an API?

Applications and Interfaces 

Hi, My name is Dan Appleman, and welcome to this Executive Briefing on APIs, or application programming interfaces. API is one of those terms in computer science that can sound very intimidating to those who aren't familiar with it, but it's actually very easy to understand. Let's start with the word application. You know what an application is. You use them all the time. For example, you might run Excel on your desktop, or laptop, or an app on your phone. You've used web applications. Some are complex, like Salesforce, that you may use to manage your business. And, of course, Pluralsight itself is an application. See? Application is easy. Now, let's skip over to the word interface. An interface defines how you work with an application. In Excel, you might enter a formula using a keyboard. In Salesforce, you might create a lead. In Pluralsight, you might do a search for one of your favorite authors. In each of these cases, you, the user, are working with the application's interface, the user interface, or UI, that is, the interface that just designed for users. People using applications, apps, or web applications is something everyone is familiar with. But if the person can use an application, how about one application using another application? Imagine if you have an application that could make a web request. In the previous example, searching for an author on Pluralsight, we entered the name in the search box, but take a look at the resulting URL. It has extra information in it that defines the search. An application could send that request directly to Pluralsight and get back a list of courses published by an author. That screen is easy enough for you to read, but take a look at what actually gets sent back to the application that made the request. Now, that's a lot harder to read. Trust me, it's not easy for software either. Most of what comes back has to do with appearance and user interaction, which is not what you're trying to retrieve.

The Application Programmming Interface

Applications are not users, so it makes no sense for them to use the user interface. They should have an interface that is designed just for them, one that an application can use that avoids all the extra content used for displaying information and interacting with users. We call that the application programming interface, or API. Let's look back at our Salesforce example. An application could retrieve a lead by entering its URL. But here, again, when you look at the raw HTML, what is returned by Salesforce? It's incredibly complex and subject to change at any time, so almost useless for other applications, but Salesforce publishes an API. Here is an example of the URL that an application might call to retrieve the same lead, which I'm entering into a workbench that is designed to help developers understand and build applications that use APIs. Take a look at what is returned. All the clutter needed to interact with users is gone. Only the data itself is there. In this case, it's in a format called JSON, JavaScript Object Notation, that is very easy for applications to process. Another common format is called XML, Extensible Markup Language. This particular API uses a protocol called REST, or Representational State Transfer. You can generally spot a REST interface by the fact that the operation is largely defined by the URL of the request. Though, there's a lot more to it that is beyond the scope of this course. REST interfaces almost always used JSON. Another older, more complex, but still useful protocol is called SOAP, or the Simple Object Access Protocol, in which a single URL can perform many different operations. SOAP interfaces always use XML. So there you are. An API is an interface that programmers creating software applications can use to interact with other software and applications. In fact, there are many cases where an application or software does not have a user interface at all, just an application programming interface. There are different kinds of APIs. Let's take a look at some examples. When you run an application on your phone or desktop, it uses the operating system's API, thousands of functions available to do things like creating windows, or storing, or retrieving data. For example, a Windows application uses the Windows API. Software developers building applications use APIs that are related to the language and environment that they are using. For example, a .NET developer would use the .NET Framework class library, which is what they call the .NET API. There are also many JavaScript frameworks and libraries, such as Angular and React, each of which has their own API. And of course, there are endless web applications and services that can be used by applications and by each other. In the next module, you'll learn about using or consuming APIs. In the final module, you'll learn about publishing APIs, which has its own set of issues. But before we get into that, there are two subjects that are essential to know about APIs, versioning and security.

Api Versioning 

Let's take another look at Pluralsight's web application. It's easy enough for you to understand it. There are some menu items, a search bar, a sign in option, and other content. Now, through the magic of the Internet archive, let's take a look at Pluralsight's home page from early 2016. Looks different, doesn't it? But you can still understand it. The fact that it's changed doesn't impact your ability to use it. That's because you are a lot smarter than software. When an application uses an API to retrieve information or perform an operation, it expects it to work a certain way and return a certain set of data exactly the same way, every time. It's like a contract. If the software or application changes the way an API works, any program trying to use it will probably fail. We call this a breaking change because it breaks the contract, along with every application using that API. In order to allow applications to change their behavior or add new features to their API's, it's common to use some form of versioning. In other words, an application exposes or publishes different versions of their API, and the program can connect to a specific version. You actually saw this earlier in our Salesforce example. See that v48.0 in the URL? That means we're connecting to Version 48.0 of this particular API. I can change it to a different version number and see if there are any changes. In this case, it looks much the same. We'll go more into versioning later. It's an incredibly important topic. But for now, let's take a look at a topic that is even more important. Security.

Security 

There are some API's where security isn't much of an issue. When building an application that uses the Windows API or a JavaScript framework, there isn't really a need to secure the connection between a program and the operating system or a software library, that is effectively going to become an integral part of the application. But if your API is accessed through the Internet, security is a huge issue. For example, Google has a variety of API's that can be used to work with its services. They can't just work any time for anybody. For example, you wouldn't want anyone with your email address to be able to read your calendar. Google has made sure that every API request on your data has your permission to access it through that API. There are two parts to this, authentication and authorization. Authentication is making sure that you are the person who granted access. Authorization defines what you are allowing an application to do. For example, if I schedule a Zoom meeting, it gives me the option to add a calendar entry. Before it can do so, I get a prompt. I didn't get a log in prompt in this case because my account was already logged in with this browser. But I do have to grant permission for Zoom to use the Google API to access my calendar. Looking at our earlier Salesforce example, I had to log into Salesforce before I could use the workbench to retrieve the lead. Not only that, but my account had to have the necessary permissions to view the lead, and belong to a profile that allows it to access the API. So there you have it. An API is an interface, a contract, a way for one application to work with another application or piece of software. There are two key subjects that I'm going to return to in the next two modules, versioning, which is a way of allowing API's to change without breaking that contract; and security, which is critically important, especially on web API's. Now let's dig deeper into consuming, or using, API's.

Consuming APIs

APIs ad your technology stack

If you or your organization is creating software, you are using APIs. Modern software is built in layers from components. At the bottom is an operating system, Windows, iOS, Linux, Android, and so on. Then there are the various frameworks, software libraries that implement functionality that can be used by the main application. There may also be web applications, where the applications you build connect to external services through the internet, or you may be building applications directly on top of a cloud service or application. All of those elements make up what is called your technology stack, and each of these elements has its own API. This has some interesting ramifications. First, when you're hiring software developers, you'll rarely worry about whether a developer knows how to use APIs in general. They do because all modern software developers use APIs all the time. But you will worry about whether they know how to use a specific API, the ones that are part of your technology stack. That's because it can take a long time and a great deal of effort to become skilled at using a particular software, components or framework and their associated API. In fact, most of the effort of learning any new language is not in learning the language itself, but in learning the APIs from the frameworks and class libraries that are used with it. When it comes to choosing APIs, what you're really talking about is defining your technology stack, choosing the frameworks, components, and services that you will be using in your organization. Now, how to select elements of your technology stack is a subject that is far beyond the scope of this briefing. There are a number of courses on Pluralsight that can help you with that, including my own course, Building Software That Lasts ‑ A Guide to Maintainable Software, and the course How to Get the Technology Stack Right For Your Business. That said, there are issues that relate specifically to APIs that you should consider when choosing the elements of your technology stack. Let's look at those next.

Evaluating API

Examining the API of a framework software library or web application should be part of any selection process. There are a number of issues to consider. You learned about the importance of versioning in the last module. As a consumer of an API, you'll want to know if the publishers of that API are using versioning. You'll also want to know what steps they are taking to avoid breaking changes, updates that might impact your application. This applies to any type of API, but takes different forms. Consider an operating system. It's a huge problem when an operating system vendor makes changes that break your applications. If you use Windows, you face every update with a certain amount of fear and anxiety for exactly that reason. Though not common, there are definitely cases where Windows applications have suddenly stopped working correctly after a Windows update. When it comes to using frameworks or software libraries, you may have a choice as to whether to distribute a specific version of the library or automatically use the latest. For example, a web application you create might have the option to automatically download the latest version. While this might be okay for a noncritical in‑house system, you should never do so for a production or critical system. Always use a specific version of the library and create a process to evaluate updates before going live with them. For web APIs, you should be very concerned if the publisher or vendor is not versioning their APIs, it puts your application at significant risk. When it comes to security, most of the issues relate to the implementation of the software that is publishing the API, how it is built. That is another huge subject. But when it comes to consuming a web API, there are three issues that stand out that you should absolutely pay attention to. First, is the API using an encrypted connection, HTTPS? It may not be an issue for a public API, but for anything involving even remotely sensitive or personal information, it is essential. Another thing to look for is whether the publisher includes documentation, examples, and best practices for using their API securely. If they don't, that's a red flag because you don't want your developers, who may not be security experts, to have to figure out these things for themselves. And finally, remember, there is no such thing as perfect security, so you want to make sure that whoever is publishing the API is committed to quickly fixing security flaws as they are discovered.

Updates and Maintenance 

People often think about building software as a project. You have a set of requirements, you build the software, and at some point, you're finished. You can just use the software and only change it if the requirements change, say a new feature is desired. That may have been true in the earliest days of software development, but it is definitely not true with modern software development. Your technology stack needs to be maintained, and much of that maintenance is tied to the issues we keep coming back to, versioning and security. Even if your application is only using the operating system and software components that you distribute, over the long term, there is a chance that something will change to break it, an operating system update, or new security restrictions in browsers might come along and require you to update your application. If you are using web APIs, it becomes even more critical to stay up to date and watch for updates. You will periodically want to update your application to use newer APIs, even if you don't need any of their new features. That's because it's costly for publishers to maintain APIs, so they tend to deprecate and discontinue older ones. Sometimes they'll discontinue an API or service completely. One example I ran into was Google Image Charts that was deprecated in 2012 and can be shut down any time without notice. Fortunately, they published adequate notice, and I was paying attention. So I've long since migrated our software to newer charting technologies. Even publishers with the best intentions will sometimes blow it and release a breaking change. For this reason, you'll want to make sure that your application is built with robust error handling and diagnostics, so you can detect problems quickly and reach out to the publisher to resolve the issue. You've seen that consuming APIs is really about your technology stack, which is a big subject. We focused just on the API part, looking at versioning, security, and the scope of APIs, where it comes to selecting, using, and maintaining them. You may feel I'm a bit paranoid on the subject of maintenance and versioning when it comes to consuming APIs. I suppose I am, but trust me, it's justified, as I've experienced all of these issues first hand, And if you think consuming APIs has its challenges, just wait until the next module, where we'll look at publishing them.

Publishing Apis

Should you publish an API
While every organization that builds software consumes APIs, publishing APIs is much less common. Before you decide to publish an API, there are some questions you should consider. What are you trying to accomplish? Are you building a software library or a framework that you want developers to share? Is there actually a need for automation? Maybe a user interface is sufficient. Don't create an API just for the sake of creating an API. You should have a real need or opportunity before going down this path. Who is going to use it? Is it for in house use, or will outsiders be using it? If available to outsiders or even other departments or divisions within the same organization, the costs to develop and maintain an API can be significant. Consider for a moment what is involved in creating a software library or framework. You need good design. You need to decide how you will handle versioning. You need good documentation. You need to plan on supporting it. Since your library or framework probably uses other APIs, you need to do all of the things mentioned in the previous module with regards to maintenance. If you are creating a web application, you need all of those elements, plus a security strategy and the ability to maintain, monitor, and scale your application infrastructure, whether in house or in the cloud, and the funding to keep it running. I think you see my point. Publishing an API, especially a web API, is actually a big deal. It's not something one does lightly or casually, but if you still want to go ahead, the rest of this module will go deeper into the topics I just mentioned. Let's start with design.

API design 

You might think that because every software developer knows how to consume APIs, that they are all qualified to design and publish them. That is definitely not the case. It's very easy to design a bad API. Let's take a look at some of the design issues that you or your team will need to consider. If you're building a software library or framework, you'll need to define the languages and platforms that you support. For example, a JavaScript library will obviously support JavaScript, but there's JavaScript and there's JavaScript. What versions of JavaScript will you support and which browsers and services? For web applications, you'll probably use a REST interface, but SOAP may be an option for some situations. You'll want to consider carefully what functionality you want to expose with your API. If you miss something the developers need, they may find your API useless and complain or turn to another API, perhaps one from a competitor. If you expose functionality that is rarely used, you'll have invested time and effort that is largely wasted and will continue to be wasteful because once published, you'll still need to maintain that functionality. The design will have a huge impact on how easy it is for developers to learn and use the interface. You'll want a clear and consistent naming convention, one that is as intuitive as possible to make functions and objects easier to learn and remember. You'll want to consider how the functions and objects in your API are organized, both in the interface and in the documentation. No software is perfect, and calls your API may result in errors. They may be the client's fault caused by invalid parameters to functions, or they may be due to bugs in your code, or in the case of a web application, they may be due to a failure or outage in the application itself. In all of these cases, you'll want to provide a mechanism to return error and diagnostic information to the caller. I haven't even discussed what goes into the architecture and design work that is needed before implementing the software that is exposing the API, and that's a huge subject, and it's far beyond the scope of this course. But as you can see, there's a lot that goes into designing a good API, and I haven't even gotten to the two most important topics. You guessed it, security and versioning.

Security 

Implementing a secure application programming interface is another one of those huge topics that I can't cover in great detail here, but when it comes to publishing the API, there are a number of issues that you will have to consider. First, you need to incorporate security into the design of the API itself. Consider a medical testing website where you can log in and see the results of your tests. The user interface is fairly straightforward. You log in, and you can see your test results. Now imagine the testing lab publishes an API that makes it easy for other applications to track your lab results using that API. You certainly don't want just anyone using the API to have access to all of the records. There has to be away for the calling application to certify to the labs application that it has permission to see and share your records. How this is done can be quite complex, but regardless, it has to be designed into the API from the very beginning. Making sure that an API handles authorization and authentication correctly and does not expose information that the caller does not have the rights to is essential. It is definitely an area where you will want to have someone with serious expertise on your team, as not every developer knows how to do this, and it's something you have to get right from the beginning. It's not unusual for API publishers to bring in expert consultants to help with this aspect of API design and development. By the way, this can be an issue even with software frameworks and class libraries, ensuring that they respect the permissions of whatever user is running the application. Now think about all the security features built into an operating system, and you'll see what I mean. The larger and more complex an API is, the greater the attack surface. That means the more ways an attacker can try to use the API to either extract data that it's not allowed to see or perform operations that should not be allowed. It is the responsibility of those implementing the API to validate parameters and write secure code, but beyond that, it's a good reason to keep your a API as small and simple as possible. Every feature and option you add is another potential security vulnerability. Finally, when you publish an API, you have to be prepared for the chance that people will find vulnerabilities in your application. You'll want to ensure that a reporting mechanism is in place and that your internal processes are designed to respond quickly and fix the issues. With luck, you can fix them in such a way that it does not break the applications using your API. Even using versioning won't help you here, as you may need to apply security fixes to older versions of APIs as well. I'll go into more detail on this in the next couple of clips. Right now, it's time to return to the subject of versioning, this time from the publisher's perspective.

Versioning and culture 

It should be no surprise that as a publisher you'll want to use versioning for your APIs, whether it is a versioned web API or maintaining older versions of a software library or framework. But this is just the first step. Publishing an API demands a very specific software development culture, and it all comes down to a very simple commandment. No breaking changes! You must never break your client's application. The only exception to this would be a critical security fix, and we'll talk about that scenario in the next clip. There are many different software methodologies and cultures. For example, companies such as Facebook have taken pride in the slogan Move Fast and Break Things, a mantra of constant innovation. And as you learned earlier in this module, breaking or changing a user interface isn't a terrible thing. Some users might be annoyed and frustrated, others excited about the improvement, but most will adapt. But applications using an application programming interface can't adapt. An API is a contract. A culture of move fast and break things for a development team responsible for an API is a sure road to disaster. When publishing an API, the top priority over everything other than security must be no breaking changes. If you want to change something, it has to be versioned in such a way that current consumers of the API continue to work. No breaking changes means no breaking changes. It means you can't even fix a bug if fixing it would change the behavior of the API. And when I say behavior of the API, I don't mean just the documented behavior of the API. Trust me, people will use your APIs in ways you did not expect, and that usage, even if undocumented, should not be changed without very careful consideration. When you have the right culture, you do the right things. Other teams might cut corners on automated testing. A development team publishing an API makes automated testing a priority. It's a great way to reduce the risk of accidentally creating a breaking change. It's also a good idea to provide early access to beta, pilot or pre‑release versions of builds so that those using your API have a chance to validate their application on the new builds before they are released. It goes without saying that you'll need to update your API documentation for each release, and ideally provide pre‑release documentation beforehand. But even with your best efforts, you may release a breaking change, which brings us to support and maintenance.

Support and Maitenance 
Once you publish an API, those who use it will be relying on it. It becomes a part of their technology stack. It's a safe bet that you will hear from them. Some of the things you hear will be good and welcome, others less so. There are always bugs, not only because developers make mistakes, but also because, as mentioned earlier, people will do things with your API that you never imagined. While it's nice to see your API used that way, it creates a real predicament. You learned in the previous clip that some bugs can't be fixed because they would be a breaking change. If fixing a bug would change an undocumented behavior, it too can break someone's application. Still, bugs do need to be fixed, so you'll want to establish a careful review process to determine whether a bug can be safely fixed or if the fix needs to be incorporated into the next version of the API. Then there are security vulnerabilities. Those need to be fixed, and in many cases they cannot be versioned. In other words, you may need to go back and fix older versions of your libraries or frameworks or your web API. In some cases, this may lead to breaking changes. This puts you in a terrible position. Don't fix the issue, and those using your API are vulnerable. Fix it, and their applications break. This means that you need to put into place an alert system, a way that users can subscribe to critical information so that they know what they need to do. If it is a software library, they'll need to quickly download the updated software, fix and validate their own application, and make it available to their customers or users. If it is a web application, they'll need to drop everything and validate that their application can handle the change. The possibility that security issues may require you to patch older versions of your APIs means that you have to maintain those versions. That can become costly as time goes on and versions proliferate. To minimize this cost, you'll want to establish a deprecation policy, essentially putting your customers on notice that you will stop supporting older versions of the API at a certain time. Here again, a way to notify users is essential. It may seem like publishing an API is a big deal. That's because it is. It's a commitment, and comes with a great deal of responsibility. But in today's interconnected world, the availability of APIs to automate and integrate applications has become expected. Not having one puts your software or services at a disadvantage. My point is not to discourage you from publishing APIs, rather to encourage you to invest the time and effort needed to do it properly so that both your users and your organization will succeed. That means focusing on design, security, and a development culture that emphasizes reliability in all of its forms, from avoiding breaking changes, versioning APIs, and strong documentation and communication. Thank you for watching APIs ‑ Application Programming Interface: Executive Briefing. I'd like to invite you to watch my other Pluralsight courses, including Executive Briefing courses such as Cloud Technologies and Culture of Learning. You can find them by starting at my Pluralsight page, app.pluralsight.com/profile/author/dan‑appleman. Please follow me to be notified of future courses and course updates. We hope you enjoyed this course. If you're interested in more content for technical leaders, content we keep short and focused with the up‑to‑date information you need to be informed and make decisions, but without getting buried in the details, find other courses like this at plrsig.ht/exec.

 
Updated 
**Updated Roadmap: AI Development Assistant (Framework-Focused)**

---

### **Version 1: Original Technical Roadmap**

#### **Phase 1: Planning and Conceptualization**
1. **Define Key Features**
   - Smart framework/API navigator.
   - Interactive learning mode.
   - Real-time code simulation.
   - API evolution tracker.

   **Theory:** A unique AI tool needs to solve real pain points. Learning APIs/frameworks is often harder than learning syntax, so focusing on these aspects ensures relevance.

2. **Research Patents and Licensing**
   - Look into patents related to:
     - AI-driven recommendation systems.
     - Interactive programming environments.
     - Code simulation sandboxes.
   - Use tools like [Google Patents](https://patents.google.com) and [USPTO](https://www.uspto.gov).

   **Reasoning:** Protecting intellectual property ensures that your idea isn’t easily copied.

#### **Phase 2: Development Setup**
1. **Select Tools for Development**
   - **Programming Language:** Python (for AI) and JavaScript (for frontend interfaces).
   - **Frameworks:** TensorFlow/PyTorch for AI models, React.js for UI.
   - **APIs:** OpenAI for language modeling, GitHub APIs for repository exploration.

   **Theory:** Python is optimal for AI because of its extensive libraries and community support. React.js ensures a responsive and interactive frontend.

2. **Create Initial Mockups**
   - Use tools like Figma or Adobe XD to design the user interface.
   - Focus on simplicity and intuitive navigation.

   **Reasoning:** A clear design ensures users can interact with the tool effortlessly.

#### **Phase 3: Core Development**
1. **Build the AI Navigator**
   - **Step:** Train a model to identify relevant frameworks/APIs based on input code or project description.
     - Use datasets of open-source projects (e.g., GitHub repos).
     - Fine-tune models like GPT on code datasets.
   - **Tools:** Hugging Face, GitHub Archive datasets.

   **Theory:** The AI should learn patterns and relationships between project requirements and frameworks used.

2. **Develop Interactive Learning Mode**
   - **Step:** Integrate a chatbot UI that:
     - Explains code snippets and frameworks.
     - Provides step-by-step guides.
   - **Tools:** OpenAI API for language generation.

   **Reasoning:** Real-time learning assistance helps users bridge knowledge gaps faster.

3. **Implement Real-Time Code Simulation**
   - **Step:** Create a sandbox environment using Docker to run isolated code.
     - Allow users to test API calls and frameworks without installing them locally.
   - **Tools:** Docker, Node.js.

   **Theory:** A safe and controlled environment reduces friction in experimentation.

#### **Phase 4: Testing and Iteration**
1. **User Testing**
   - Recruit early adopters from communities like Reddit (r/programming) or Discord tech groups.
   - Collect feedback on usability and accuracy.

   **Reasoning:** Early feedback ensures the product meets user needs effectively.

2. **Debugging and Optimization**
   - Use tools like Postman for API testing and Selenium for UI testing.

   **Theory:** Regular testing ensures a reliable and seamless user experience.

#### **Phase 5: Launch and Maintenance**
1. **Deployment**
   - Host the product on cloud platforms like AWS or Google Cloud.
   - Set up CI/CD pipelines using GitHub Actions.

   **Reasoning:** Scalable infrastructure ensures the product can handle user growth.

2. **Ongoing Updates**
   - Monitor API changes and update the AI’s training data accordingly.

   **Theory:** Continuous improvement keeps the product relevant and useful.

---

### **Version 2: Explain it Like I’m 5 (Basketball Analogy)**

#### **Phase 1: Planning (Setting Up Your Basketball Court)**
1. **What Are We Building?**
   - Imagine we’re building a basketball training machine.
     - It teaches you how to dribble (syntax).
     - Then, it teaches you advanced moves (frameworks and APIs).

   **Why?** Learning the basics is easy, but it’s the fancy moves that win games!

2. **Protecting Your Game Plan**
   - Just like a coach keeps their playbook secret, you’ll need a patent to keep others from stealing your training machine idea.
     - Look for patents on basketball machines (AI tools).

   **Why?** If you don’t protect it, someone else might copy your idea.

#### **Phase 2: Building the Court (Setting Up Tools)**
1. **Pick Your Gear**
   - Use Python for the brain of the machine (AI) and JavaScript for the scoreboard (UI).
   - Tools:
     - TensorFlow (AI coach).
     - React.js (fancy scoreboard display).

   **Why?** These tools are like having the best basketballs and shoes for your game.

2. **Draw Your Court**
   - Use tools like Figma to sketch out where the hoops and lines go (design the app interface).

   **Why?** A good court makes the game more fun and easy to play.

#### **Phase 3: Building the Training Machine (Core Development)**
1. **Teach the AI Coach**
   - Train the AI to recognize which moves (frameworks) are best for each player (project).
   - Use data from GitHub (watching real games).

   **Why?** The coach needs to know the best strategies to teach players.

2. **Interactive Play-by-Play**
   - Build a chatbot that explains every move as you play.
     - “Dribble like this… Now try a crossover!”

   **Why?** It’s easier to learn when the coach talks to you in real time.

3. **Practice Court (Sandbox)**
   - Create a safe practice area where players can try moves without messing up real games.

   **Why?** Practice makes perfect, and mistakes here don’t count.

#### **Phase 4: Testing (Scrimmage Games)**
1. **Play Test Games**
   - Invite friends (users) to try the machine and give feedback.

   **Why?** You’ll know what needs fixing before the big league debut.

2. **Fix Bugs (Improve Plays)**
   - Use tools like Postman to test the machine’s moves.

   **Why?** Every player loves a smooth, glitch-free game.

#### **Phase 5: Going Pro (Launch)**
1. **Big League Debut**
   - Host the training machine online so everyone can use it.

   **Why?** A bigger audience means more players learning and improving.

2. **Keep Improving**
   - Watch real games (API updates) and teach the machine new moves.

   **Why?** Staying updated ensures the machine stays the best coach out there.

---

Would you like me to break down the actual coding steps next, using these two formats, or focus on a specific phase?



Key Features of the Tool for Collaboration with Other LLMs:
Task Explanation and Delegation:
The tool can explain assignments clearly and use standardized APIs or communication protocols to break the task into smaller, understandable steps for external LLMs or even human collaborators.

Example:
If the main tool is tasked with building a chatbot, it could break the task down into:

Setting up the backend infrastructure.
Choosing a dataset for training.
Designing user interaction flows.
Each of these tasks can be assigned to different LLMs or tools.
Tool Selection and Optimization:
The tool can automatically decide which tools or APIs to use for a given job based on the task's complexity or requirements.

Example:

Need image generation? It calls a specialized API like DALL·E or MidJourney.
Need large-scale database operations? It delegates that task to a module like Redis or DynamoDB.
Need a summarization task? It picks an LLM fine-tuned for text summarization.
Manpower Scaling:
If you need additional "manpower," the tool could spin up instances of other LLMs, API calls, or pre-built microservices to share the load.

Example Scenario:
You’re developing a large-scale app with several interconnected features. The tool dynamically calls:

An LLM for debugging code.
Another LLM for writing user-facing documentation.
A separate module for stress-testing the backend.
The central AI ensures that all these instances work in harmony by synchronizing progress and results.

Learning and Feedback Loops:
As the central tool orchestrates tasks, it gathers feedback from the subtasks it delegates. Over time, it learns:

Which tools work best for specific tasks.
How to communicate assignments for faster and more accurate results.
Why This Approach Makes Scaling Easy:
Abstracting Complexity:
You don’t need to manage every detail—your central tool becomes the “coach” of your AI team. It explains tasks in simple terms, delegates effectively, and ensures high-quality output.

Rapid Prototyping:
With the ability to call specialized tools or APIs, your development process becomes faster. No need to manually integrate features; the tool takes care of it.

Flexibility for New Tasks:
The central tool adapts quickly to new tasks or collaborators by treating them as “players” on the team. Just like a basketball coach adjusts strategies based on the game, your tool adjusts workflows based on the project.

Breaking It Down with Basketball Analogy:
Central Tool as the Coach:
Your tool is like the head coach. It sees the big picture, divides the project (game) into manageable sections (plays), and assigns each part to the best-suited players (LLMs or APIs).

LLMs as the Players:
Each external LLM or module is a player with specific skills:

One is great at defense (debugging).
Another excels at offense (building new features).
Another might be an all-star at making decisions (decision trees or logic optimization).
Manpower Scaling as Substitutions:
If the game gets intense and you need more players, the coach can call in substitutes (additional LLM instances or tools). These new players jump into action without missing a beat because the coach explains the play (task) clearly.

Feedback as Practice:
After every game (project phase), the coach reviews performance (feedback loops). What worked well? Which players need more training? This helps the team improve for future games.

Implementation for Assignment Explanation and Tool Selection:
Here’s how you can achieve this:

Natural Language Task Breakdown:
The central tool uses an LLM to parse a task and divide it into subtasks. Example:
Input: "Build a weather app."
Output Subtasks:

Design the user interface.
Fetch weather data via an API.
Create notifications for severe weather alerts.
Tool/LLM Selection Logic:
Use metadata and a predefined “skills list” for each tool or API.

UI Design → Assign to Figma API or a UI-specific LLM.
Fetch Data → Assign to OpenWeatherMap API.
Notifications → Assign to Firebase Cloud Messaging (FCM).
Dynamic API Calling Framework:
Implement a system where the central tool dynamically calls APIs based on subtask requirements.
Theory Behind It: Similar to REST APIs, every LLM/module can expose an endpoint for task completion. The central tool queries these endpoints and integrates results.

Feedback Mechanism:
Use a simple feedback loop where the subtasks report progress and results back to the central tool. Example:

Subtask 1: Design UI → Completed.
Subtask 2: Fetch Data → Failed (API limit exceeded).
The tool adjusts plans based on this feedback.


Key Capabilities for Software Development and Web Apps
Tool Detection and Recommendation:

What it does: The tool identifies the best frameworks, libraries, APIs, or development environments based on the project requirements.
How it works:
Parses your project specifications.
Matches the requirements against a knowledge base of tools.
Recommends tools for specific tasks like front-end development, database management, authentication, deployment, etc.
Example Scenario:
Input: "I want to build a web app with real-time chat and user authentication."
Output Recommendation:

Front-end: React.js or Vue.js.
Back-end: Node.js with Express.js.
Real-time: Socket.IO for WebSocket communication.
Authentication: Firebase Authentication or Auth0.
Database: MongoDB (NoSQL) or PostgreSQL (relational).
Explain How Tools Communicate:

The assistant explains how each tool fits into the development stack and how they communicate.
Example:
React.js Front-End: Handles the user interface.
Express.js Back-End: Manages API endpoints.
Socket.IO: Enables real-time messaging between client and server via WebSockets.
It can even generate code snippets and diagrams to visually demonstrate communication flows.

Error Detection and Debugging:

Your assistant could act like a "debugger" by detecting issues in your project and recommending fixes.
Example:
"You’re using MongoDB, but your queries are inefficient. Try adding indexes to these fields: [field1, field2]."
Custom Code Generation:

The assistant generates boilerplate code or modules based on your requirements, reducing repetitive work.
Example:
Generate a REST API template in Node.js.
Set up authentication and authorization for a React app using Firebase.
How It Detects and Recommends Non-LLM Tools
The assistant would use a tool detection engine, which consists of:

Keyword and Context Parsing:
It analyzes the project description for keywords like "real-time," "authentication," "scaling," or "mobile-first."

Example Input:
"Build a scalable web app with a mobile-first design."

Keywords: Scalable → Cloud services like AWS or GCP.
Mobile-first → Frameworks like Bootstrap or Tailwind CSS.
Knowledge Base of Tools:
The assistant uses a curated and updatable database of tools/frameworks (open-source and proprietary), tagged by purpose and compatibility.

Example Tags:

MongoDB → NoSQL, JSON storage, scalable.
Tailwind CSS → Front-end, mobile-first, utility-first CSS framework.
Dependency Mapping:
It ensures the tools are compatible and explains how to connect them.

Example:

"React works well with Redux for state management."
"For server-side rendering, pair Next.js with React."
Can It Explain Communication?
Yes! Your assistant could explain how to make these tools work together, in simple terms or detailed technical explanations (with theory).

Example for a Web App:
Task: Build a real-time messaging web app.
Explanation of Communication:

React (UI):

Handles the front-end.
Users send and receive messages through forms and chat windows.
Express.js (Back-end):

Receives data from React through HTTP requests.
Processes and routes data to other parts of the app.
Socket.IO (Real-time):

Maintains a WebSocket connection for live updates.
React gets notified instantly when a new message is sent, without needing to refresh the page.
Database (MongoDB):

Stores message history and user data.
Ensures data persistence so messages are not lost when the app restarts.
Tools It Could Leverage to Help You Understand Communication
The assistant could also guide you in integrating tools to detect issues, communicate better, or optimize performance:

Postman:

For testing API endpoints during development.
Use Case:
"Test if the Express.js server is receiving data from React correctly."
Swagger:

Automatically generate API documentation for RESTful services.
Use Case:
"Document your backend API to make it easier for team members to integrate."
Docker:

Containerize the application for easy deployment.
Use Case:
"Ensure that React, Express, and MongoDB run consistently across environments."
Webpack or Vite:

For bundling your front-end code efficiently.
Use Case:
"Optimize your React app for faster load times using Webpack."
Basketball Analogy for Tool Communication
Let’s simplify it:

React (Point Guard):

The point guard delivers the ball (user actions) to teammates (other tools).
It decides whether to shoot (render data) or pass (send data to the backend).
Express.js (Shooting Guard):

Takes passes (HTTP requests) from the point guard (React) and scores (processes the requests).
Socket.IO (Center):

Dominates the paint (real-time updates).
Ensures that messages (rebounds) are distributed instantly to the team.
MongoDB (Coach):

Stores all the plays (data).
When players (tools) need guidance, they consult the coach for the strategy (data retrieval).
Scaling and Maintenance:
Monitoring Tools (Assistant’s Role):

Tools like New Relic or Datadog can monitor app performance.
Your assistant could integrate them and explain their results in plain language.
Documentation:

The assistant ensures every tool's role is well-documented so it’s easy to onboard new team members (or tools).
Automated Suggestions:

If there’s an issue, the assistant automatically suggests fixes.
Example:
"Your app is slow because MongoDB queries lack indexing. Add an index to these fields."



Extended reality (XR) and the Metaverse: Executive Briefing 

introduction
Welcome to the Executive Briefing on Extended Reality and the Metaverse. I'm Simon Allardice. And okay, most of you watching this are probably familiar with this kind of thing, whether you've just seen the headsets or tried them, some of you may even have a VR headset at home, but just the fact that you're in a position to watch this course, I'll assume you have heard terms like virtual reality, augmented reality, and even the metaverse. And if you're thinking, yes, I've heard those terms, I'm clear about some of them, but others not so much, that's okay, we'll take care of that. What we're diving into here are the ways we're beginning to blur the lines between the real physical world and the digital world, whether that's putting on a headset that completely immerses us in a simulated digital environment or using a device like a phone or a pair of smart glasses to overlay digital objects on a real physical environment. So, we'll begin by getting clear on terms like virtual reality, augmented reality, and a few others because they're not always used in a consistent way. But we'll quickly go a little bit deeper, not just looking at entertainment and gaming applications, but how this is being used in business, education, manufacturing, construction. We'll talk about what makes this technology work, the interesting applications being done right now where it looks like things are going over the next few years, and how you can become more involved. This isn't just about what is extended reality, what is the metaverse, but how you can make better use of it, and even though this is a very fast moving area of technology, what you should expect. Welcome to the Executive Briefing on Extended Reality and the Metaverse.

Defining Extended and Virtual Reality

Okay, the title of this course includes the term Extended Reality, XR. Now this doesn't mean very much by itself. XR is just a shortcut, it's an overview term. It's a way we can talk about several different but related technologies, including virtual reality, or VR, augmented reality, AR, and mixed reality, or MR. So let's begin with virtual reality because this is the most widely understood of them. Typically with VR, you'll have some kind of headset. When you put this on, it will block out your view of the outside world. I can't see through this, it's opaque. I'll be looking at two lenses inside the headset, and the images I see are all computer generated. Now, in some VR headsets, the computer and software that's generating those images is all built into the headset, it's all self‑contained. Other headsets don't include that computing power, they're more of a display unit, and these need to be connected to a separate PC to provide all the graphics, so we often call these devices tethered because they need this long physical cable. Now there are pros and cons to each. An untethered, standalone device like this Oculus Quest 2, this is simpler, it's easier to set up, it's more portable, it's cheaper, but a small, self‑contained device like this just can't provide the same kind of processing power that you'd get with all the hardware in a high‑end, connected PC. So, if we're using a PC with a tethered headset, we can get a better experience, including higher definition, better resolution where the images that you see are more realistic, but it's a lot less portable, it's more complicated to set up, and it's more expensive because you need a PC, as well as the headset. But whether it's tethered or untethered, we're not doing this just to have a flat, two‑dimensional display like putting a tv program in front of our eyes. With VR, we do this to be immersed in and surrounded by a three‑dimensional environment which will react to what we're doing. So all of VR headsets, tethered or untethered, contain multiple sensors to continually figure out the orientation of your head, so as you move up and down, turn side to side, back and forth, even turn completely around, it will recalculate the view of what you're seeing and it will send slightly different views to each eye so that what you see appears three dimensional, that you have perspective, things seem close or far away and you get a sense of scale. Some things are appropriately tiny, other things are huge, even planet‑sized. You consciously know it isn't real, but when it's done well we often talk about this sense of immersion or sense of presence that a virtual space feels real, or at least real enough, and that's more than just what you see, it's also what you hear and what you can touch and how you move, but we'll talk about that in a moment. Let's cover the other types of extended reality first.

Augmented Reality 
With augmented reality, we're not trying to shut out the real world and replace it with a computer generated, simulated environment. Instead, where they are, you stay looking at your current space, whether that's your office, your living room, your car, your backyard, a hotel lobby, a shopping mall, even the street outside, but where you have a way to superimpose graphics, text, video streams, other digital elements over that real‑world environment. So we're adding to or augmenting our current reality. And yes, this also requires some kind of device, but it's not always a headset. Right now, many augmented reality applications used mobile devices, phones or tablets with cameras that can show me a live video feed of the real world and also overlay computer generated images on top of it. These digital elements can appear as if they're part of the real‑world space. They can be on the floor or on the wall or on a desk and correctly reposition themselves as we move around in that space. There's just a few examples of this. Some retail sites like IKEA or Apple now let you add augmented reality objects to see what their products would look like in your home or office. But we could step outside. Well there's no apps that would superimpose the flight details of any plane that I'm pointing the camera at. Or at nighttime, there are apps to show you more about the stars and planets around you. Back inside, I could use my phone as an augmented reality tape measure and have it overlay measurement data on my camera feed. There are entertainment options where I can map my space and fill it with augmented reality effects. Or if you've seen the camera filters or lenses on Snapchat or Instagram that detect faces so you can turn yourself into an animal or into a robot or into some kind of CG character. Oh, look, this one will show you what it looks like if you shaved your head and had a great beard and looked really tired. Oh, wait. Okay, if you think the examples I just showed seem a little simple or even trivial, that's intentional. What I wanted to show is that unlike VR, which does require some level of investment into dedicated hardware and requires your focus and your time and that you're not going to put a VR headset on for 10 seconds, you're going in for a while, several minutes, even hours, but a lot of people are becoming more familiar with augmented reality with just quick, casual use cases on the devices that they already have. Nobody goes out to buy a new phone just so they can use an augmented reality tape measure. But, if the new phone you've just bought allows you to do all those extra things, why not? So, a lot of AR use is kind of bubbling up from the bottom. Now, people might not think of a Snapchat lens as being augmented reality, but it is; however, I certainly don't want to suggest that AR is just about casual use, it's not, and beyond phones and tablets, there are some very advanced head‑worn augmented reality products like Microsoft HoloLens, Magic Leap, and Google Glass. And the obvious difference between the VR headsets and the AR headsets is you have to be able to see through them to see the world. I'm going to show a few examples of business‑focused augmented reality in a few minutes, but let's take care of another term first, mixed reality.

Mixed Reality 

The term mixed reality is not as widely used or as well understood, and some people and some organizations don't really use this term very much at all. They just talk about virtual reality and augmented reality. It sometimes seems very black or white. This is AR and this is VR, and they're always very different. But in fact, there is this in‑between hybrid stage, really multiple stages where AR experiences start to become more and more like VR experiences because augmented reality can describe some fairly simple and noninteractive situations. Think of a heads‑up display that you might get in some cars now that project information like directions or even just your speed onto the glass. It is useful, it is a form of augmented reality, but it's just a piece of data floating in your view as long as you're looking in the right direction. So, we then advance to the idea of digital objects that are more aware of the environment and they can correctly orient or reorient themselves to appear on your desk or on your wall or superimposed on your existing computer screen. But the next step is where we don't just look at, but can also interact with those digital objects where I can press that digital button, I can reach out, grab that object, and manipulate it, turn it around to resize it in the space. So, if an augmented reality environment starts to add more and more completely virtual elements, more digital objects to interact with, more avatars and people that I could talk to, and we're moving more and more towards VR because you could even imagine hitting a point where what you're looking at has more virtual objects than real ones. Some people talk about the idea of a continuum or a spectrum of extended reality experiences between AR and VR. Mixed reality is a way to describe that space in between where it's more about integration between the physical and digital worlds. But again, you won't always hear it described as mixed reality. Some companies like Microsoft use that term a lot, other companies like Apple still call all of this augmented reality, even for the more advanced situations. But whatever we call it, the idea is important, but we have situations where we don't just see objects in the world, we want to interact with them, and that means that the hardware, the headset or smart glasses or any other device needs to do much more than just display some graphics on a piece of glass. It has to scan outwards and has to recognize what's around us, the walls, the floors, the furniture, other people, so it can build its own awareness of the space and also recognize what I'm doing with my hands and whether I'm reaching out to touch or grab those digital objects

Mixing Vr and the Real Word

I talked about mixed reality experiences where AR can become more and more like VR if we're beginning with the real world and adding more and more virtual objects. But, we can also go the other way. I could begin in a VR experience completely simulated and then bring real‑world objects into the virtual world. Here's an example. Right now there are several popular VR applications for productivity and working. Some are more about collaboration where you can meet colleagues in a virtual space and sit around a virtual conference table, share files and use a whiteboard. You can also construct your ideal workspace in VR. In the real world you might be traveling and in a hotel room with a tiny laptop, but in the virtual world you can have your perfect office, as many screams as you want, whatever size you want, whiteboards full of notes and diagrams. One of the problems with these applications has always been, well, you're working, and if you've just put a VR headset on, you suddenly can't see your desk or your keyboard. And even if you're a good touch typer, it kind of feels like you're trying to type with a bag on your head, you don't know exactly where anything is. But, there are more recent applications that now use the sensors in the VR headset to detect where your desk is and can recognize where your keyboard is and then generate a digital version of those real objects inside the virtual environment. So I could put on the headset, join a virtual conference room, and when I look down I'll see my desk, I'll see my hands on the keyboard, and everything feels like it's in the right place. I can type, I can take notes, I can even bring a virtual version of my real computer screen into the space. Now the way this works is that some of the sensors in VR headsets are basically tiny cameras to help in scanning the space. They can detect specific objects, but they can also do something called passthrough where you take what the camera sees and pass the image through to the screens inside the headset. So even though this headset is opaque, I can effectively see through it to the outside world. Now these tiny cameras in most current consumer VR headsets, they're pretty basic, so the images are often black and white and quite low resolution. But still, passthrough can be very helpful in keeping you oriented and grounded in your real, physical space, even when you're in VR, by bringing more of the real world in. So even if what you see is partly virtual and partly real, it's another way to have this mixed reality, this idea of this spectrum of extended reality experiences that, yes, we do have pure VR and pure AR experiences, but more and more we're moving to this idea of merging the physical and the digital, whether it's VR that incorporates more real‑world objects or AR that uses more virtual objects

What is the metaverse 

It's time we dealt with the remaining term in the title of this course and finally answer, what is the metaverse? Actually, I don't think that's a very good question, so I'm going to try and convince you that right now, the word metaverse is much more useful in an answer than it is in a question. And I'm not trying to be cryptic. Let me explain. Over the last couple of years, we've started seeing the word metaverse more and more, particularly in any articles or news stories about virtual or augmented reality. Now this word has been around and used on and off for 30 years. It came from the novel Snow Crash by Neal Stephenson where the metaverse was this fictional, virtual reality world But it exploded in popularity in October 2021 when Facebook, as an organization, renamed themselves to Meta and there were then press releases and headlines that they would be spending billions developing the metaverse. Then Microsoft started releasing videos and announcements saying what they were doing with the metaverse. Then Epic Games said they were throwing a bunch of money into their vision for the metaverse. And an awful lot of people around the world started saying, hang on, what's the metaverse? Well, you can't give a simple answer to that because, well, the metaverse doesn't exist yet. Parts of it exists, but it is fundamentally a vision for a future state of technology. We're not there yet, and we know a lot of things are going to change before we get there. However, while it is some way off, what we can say is it's a combination of many real‑world technologies that do exist right now, including virtual and augmented reality, but also cloud computing, artificial intelligence, applications like video conferencing, persistent, massively multiplayer environments, social networks, high‑speed mobile internet access. Now right now I could show you an example of any of these, but I couldn't show you one example that ties them all together. That's kind of what this is leading towards, the idea of persistent, massive, shared 3D worlds that you could join in VR and AR. So you could go into that shared space to play games or to work, meet with your colleagues, socialize with friends and family, watch a show, go shopping, play music, create art, even exercise or just relax. Now these are all things you can do right now in virtual reality, but in a very separate and disconnected way. There's no way to share anything or cross over from one experience to the other. If I meet a friend in a social app like Altspace, it might be nice to then go and see some sights with them in BRINK Traveler or go and create art in Tilt Brush. But you can't. And even if I create an avatar, that digital version of myself to use in a conferencing app like Horizon Workrooms, I can't then take that avatar over into equivalent apps like Spatial or Immersed, I have to create another version of myself again and again and again. Now there are VR apps that are working on developing these shared social spaces and these virtual meeting places like AltspaceVR, which is owned by Microsoft, and Horizon Worlds, which is owned by Meta. Now we know there's a bunch of questions that still need to be figured out, primarily who owns what, who controls access, who sets the standards for the metaverse, how is privacy going to work, how will commerce work? If I create something in the metaverse, do I actually own it? If so, how do I buy and sell it? If I want to use a decentralized cryptocurrency like Bitcoin or Ethereum, can I do that? And for all of this to become more of a reality, we need more people, we need greater levels of adoption, which means we need better devices, smaller, less intrusive headsets, better battery life, better graphics, but we also know these are coming anyway. But this is what I mean when I say the metaverse is a better answer than it is a question because we can actually begin by just talking about the technologies we have right now. We have virtual reality and augmented reality. We also have massive multiplayer shared environments like Fortnite and Roblox. We have huge social networks, Facebook, Twitter, Instagram. We all spend time in video conferencing apps and messaging apps. Some of this doesn't stay on our laptop or desktop, it moves to our phone or our watches or our voice assistants. So a more useful question is, well, in the future, when all of these things start blending together, when we just choose to use a VR or AR headset to go and play that game and then go and meet with colleagues and attend that event and write that document, what are we going to call that whole thing? And the answer is, well, right now we're calling it the metaverse. Now who knows, this word may become redundant. In a few years it might be like information superhighway or cyberspace, terms that became overused, cliched, and unhelpful. But for everything we're talking about here, all the various extended reality experiences, this is a fundamental part of the metaverse, whatever we end up calling it.

XR device Qualities 

Earlier, I said that VR headsets aren't just strapping a screen to your face, but, well, some virtual reality setups did feel a little bit like that because a few early inexpensive VR options were just made of cardboard. Now you can still buy these online for a few dollars to create a simple VR headset box with some eye holes and lenses, and then you slide in a phone or a Nintendo Switch and you use the device to generate the images you see. So you run an app or a game that can generate two images, one for each eye, so that you get that 3D stereoscopic view, and then the device itself uses its own internal sensors to change the image as you move your head around. And okay, these are very basic. What's remarkable is the fact that they worked at all, and it's only been in the last few years that we had devices that were small enough and powerful enough to even do this. There's a couple of reasons you need a good level of computing power for any serious VR application. First is the idea of frame rate and the computer needs to continually generate new images for each eye, and we measure that in frames per second, or FPS. Now if you're watching a normal TV program or movie, frame rate is often between 24 to 30 FPS, and that's fast enough to seem smooth on a regular TV screen. Now with more modern high definition or 4K resolution TV and movies, you might get 60 FPS with good equipment, and that will seem smoother, particularly when there's a lot of motion going on. But in VR, there's always motion, and 60 FPS is a bare minimum. We really want at least 90 FPS, hopefully even faster like 120. Because this image is being beamed directly into your eyes, you're much more sensitive to it. And if the frame rate is slow, our visual systems will perceive it as wrong, and it doesn't just interfere with our sense of realism or presence, it can also make you feel disoriented and nauseous. Now beyond just frame rate is also the idea of latency in that the hardware must minimize any delay, any lag between the movement of the headset and updating the image that we see. So if I quickly turned my head to the right, but for the next half second I'm still seeing the images if I'm looking straight ahead, then there's a mismatch between what my inner ears are telling me and what my eyes are telling me and it very quickly leads to motion sickness. A fast frame rate and low latency are far more important than image resolution. In a lot of high‑end video or computer gaming, it's all about resolution, it's about having more and more pixels and the ability to have more photorealistic images. Now sure, in VR and AR we'd like that, but it's much more important to have a simplistic environment with a high frame rate and low latency than to have some photorealistic environment with a slow frame rate because that will make you sick, and that's why a lot of people who haven't tried VR often see some video footage of a VR experience and think that looks pretty basic, it looks like a video game from 10 or 15 years ago because they've lowered the resolution to focus on speed and smoothness because that's much more important here. Now, most VR headsets come with some form of hand controllers with multiple buttons and joysticks. Now the unusual shape of many of these helps with tracking to more accurately understand position and orientation so that we can see our hands in VR, whether we're using them to launch applications, write on a whiteboard, swing a sword, fire a gun, sculpt an image, whatever you need. And most controllers include some form of haptic feedback, which means you have various physical sensations. They can vibrate or buzz or tap, and often there's different levels of strength of that sensation, and this is much more than just a gimmick. It really does help with that sense of presence. So if you're reaching out to touch a virtual button and you feel a tap that lets you know you've touched it, it's much more immersive. Some devices do allow you to put the controllers down and just use your hands where you're using specific gestures to open menus and launch applications, and this is easier on several levels. Less equipment, it's easier for new users to get started, but right now we're at a point where if you want fine levels of control and very exact operations, the hand controllers are more accurate than just your hands. Now, with any VR or AR device, you need to have some level of spatial awareness of the room and surroundings. Part of it is to recognize where you are in the room and partly to identify objects like furniture or walls, whether it's to position virtual objects on those or just to stop you walking into them. And some devices, like recent generations of iPhone and iPad, now include hardware for LiDAR scanning. This is light detection and ranging, it's to quickly measure the distance between nearby objects. You can use these to scan an entire room and create a mesh. This kind of looks like a fishing net and you're building a quick computer representation of the space, and the mesh continually updates as I move around. It has spatial understanding. And with this mesh created, I can add effects to this room. But if you notice how the generated objects disappear when they go behind a real‑world object, this is called occlusion. It's actually a very tricky thing to do computationally, but it's necessary for this to seem and feel realistic. But although we are getting better and better hardware, one thing we don't yet have is a single device that could cover everything from simple AR to full immersive VR experiences. They all specialize in one part of the XR spectrum. Let's take a look at the main options.

XR Hardware Lineup

While there were some early experiments in this area, the first meaningful VR headsets only appeared on the market in 2016 with the Oculus Rift and the HTC Vive. These were both tethered devices, meaning they required a powerful PC. Now HTC and Oculus are still two of the main players in this space. I'm recording this in late 2021, and HTC currently has four options, the Vive Cosmos and Vive Pro 2, which are their tethered headsets, and the Pro 2, as it might sound, is their top‑of‑the‑line, most expensive option, something that you might have in a no‑expense‑spared, high‑end VR gaming setup. Now these headsets also come with hand controllers and base stations that mount in the room and they improve the accuracy of the headset tracking and positioning in the space. Now HTC also has the self‑contained Vive Focus headset. This is targeted more at businesses than consumers. And in late 2021, they announced the HTC Flow headset. This is their lowest cost option. It's released at 499 USD. It is a tethered headset, but it's designed to be tethered to a phone, not tethered to a PC. The Flow is one of the more unusual options as it's suggested as being for kind of casual entertainment VR use, but not for gaming, more for options like wellness and meditation. Oculus, the company that released the Oculus Rift back in 2016, they were then bought by Facebook, now Meta, and over the last few years they've released two tethered headsets, the Rift and then the Rift S, and three untethered headsets, the Go, the Quest, and now the Quest 2, which is currently the only headset they actively sell. The Quest 2 starts at $299. It is a self‑contained, untethered device. It comes with two hand controllers, but also has good hand recognition built in so you can just put the controller down and use gestures. Although this is a self‑contained, untethered device, if you do have a good PC, you can connect a Quest 2 to your PC using a USBC cable and then use the PC to run more demanding VR applications that wouldn't run, or wouldn't run well, on the device itself. But Simon, you might wonder, why are you even talking about all these consumer‑level headsets if this is meant to focus on business applications? Good question, dear viewers. Well it's because if you look at what's happened with mobile phones where many companies have now moved to a BYOD, or bring your own device model, if an employee wants to use their own iPhone or their own Android phone, they can absolutely do that. That's kind of what we're expecting here. So if you own a consumer VR headset, even if you bought it for gaming or entertainment, you may be able to use it for working applications as well. Now some other popular VR headsets include the Valve Index and the HP Reverb, both of which are tethered devices. There are also things like the Sony PlayStation VR, but as this is entirely focused on gaming applications, I'm not going to talk much about it here. If you go to Amazon or eBay, you'll find a huge amount of very inexpensive VR headsets. Most of these are just slightly more advanced versions of the cardboard VR headset and that they're really just empty shells where you put your phone in to actually do all the heavy lifting. But at the other end of the scale, there are advanced VR headsets that don't focus on consumers, but on businesses and specific high‑end applications, like research, design, simulations, military applications, and so on. Companies like Varjo, XTAL, and StarVR. I'll completely admit, I haven't tried any of these products. I'd like to, I'm an early adopter and I've owned multiple VR headsets, but just as one example, the XTAL 8K is $9000, and that's a little bit pricey for my personal VR budget. We also have several head‑worn devices for augmented reality experiences. Now the main devices here are the Microsoft HoloLens, now on version 2. There's also the Magic Leap, where version 2 of this is expected very soon, and the Google Glass. This actually started as a consumer‑focused device, didn't find much success there, and quietly moved to a business enterprise focus where it's had more of an impact. Now right now, none of these dedicated AR devices are really targeted at consumers. They're all targeted businesses, manufacturing, healthcare, education and training. And they're often called smart glasses, but the manufacturers themselves don't really use that term. Microsoft calls the HoloLens an ergonomic, untethered self contained holographic device. Magic Leap from Google just call theirs wearable computers. On the horizon, we know Apple has been investing huge amounts of resources into VR and AR for many years. They've been acquiring companies, they've been filing patent applications for a decade, they've added augmented reality features to iOS devices, and developed a lot of supportive technologies like the LiDAR scanners I talked about earlier. So we fully expect Apple to release some kind of dedicated extended reality device. Most of the rumors, it will be more of an augmented or mixed reality device rather than full VR. People even call it Apple Glasses, but it's Apple and they're excellent at keeping things a secret, so until they actually announce something, we wait. And while we're talking about upcoming hardware, Facebook, or now Meta, have teased two forthcoming devices, a pair of a AR glasses code named Project Nazare and also their next VR headset, which is currently code named Project Cambria, and this will be a high‑end, premium‑level device rather than an inexpensive Oculus Quest type of product. So, some interesting products available right now and more coming down the pipeline.

Headset Features

Now if you read any product descriptions about headsets, there's a few terms you'll often see. First is FOV, or field of view. That's because when you put any headset on, it will reduce the amount of what you're able to see. You'll lose some of your peripheral vision both horizontally and vertically. Now the inexpensive Oculus Quest 2, this has a horizontal field of view of about 90°, which is on the lower end. If you take the HTC Vive Pro 2, that has a field of view of 120 degrees, but it costs significantly more because to expand the field of view and maintain the same level of quality, you have to increase the size and resolution of the little screen inside the headset, and that's expensive. Now, related to field of view is something called foveated rendering. This isn't something you have in most consumer‑level headsets right now, but you can expect it in the next generation of equipment. This is the idea that your field of view isn't all the same, meaning that if you get a page of text, it's readable if it's right in front of you, but if you move it off to the side and don't look right at it, you can see it, but you can't read it, you don't have the same level of visual acuity for anything that's not in the center of your vision. So, if you had a headset that doesn't just track your head position, but also keeps track of your eye position inside the headset to know where you're actually looking, then it can use more of the computing resources for what you're directly looking at for more visual detail there and less on what's in your more peripheral vision so you get better results with the same level of hardware. Now, if you've ever looked at the lens of VR headset, you'll see that they typically don't look completely smooth and clear. You usually see a series of concentric circles or grooves etched into the lens. It's almost like the lenses that you see on lighthouses. It's kind of for the same reason, which is that lighthouses use these lenses called Fresnel lenses because they can be smaller and lighter and placed closer to the light source than normal lenses would be. And like so many things in VR, it's not all benefits. There are a couple of downsides. A positive point is that if we didn't have lenses like this, then we'd be trying to focus our eyes on a tiny little video screen that's an inch away, and we couldn't do it. We'd have to make the headset much larger to push that screen further away or we'd have to have much thicker, much heavier lenses. So, that's a good thing about it. But one of the downsides is that if you have a lot of contrast in a VR environment, say you're in a dark room with a single light source, and depending on the angle, the light can suddenly start hitting all those different little steps in the lens and it starts to glare and blur. It's kind of like somebody has smeared Vaseline on the lens, it's even called God Rays. I have to exaggerate the effect here because you can't capture it directly. It happens between the screen and the lens and your eyes, but it's similar to this. Also in the early years of VR, there were a lot of complaints about something called the screen‑door effect. This was often from the early headsets having lower resolution where certain graphics or combinations of color, you become more aware of the actual pixels that made the image up. It's kind of like pressing your face close to an old monitor where you see all the pixels, it feels like you're looking through the mesh of a screen door. This is less of an issue these days, not just because of better hardware and higher resolution, but also because the developers of VR experiences have started to recognize what kinds of situations lead to God rays or lead to the screen‑door effect, and they just avoid doing those things.

Moving in VR

Now VR experiences are often described as having two main modes of use, stationary VR and room‑scale VR. With stationary, I'm expected to stay in one place, either standing or sitting down. Now I might turn even completely around, I can lean forward and back, but I'm just not going to take several steps in any direction, whereas room scale, as it sounds, is where I can walk around in my physical room and have that movement mirrored in the virtual space. Now some VR experiences are naturally stationary. If I'm in the cockpit of a flight simulation or sitting at a conference table or using VR to learn piano, it doesn't make sense to walk several feet in one direction, but if I want to step around a VR sculpture or play a game of virtual tennis or walk around a simulation of a new kitchen, then it would be a preferable experience if I can walk around in that space. But room scale does require a couple of things. You need a room where you can walk freely around without bumping into anything, and this is one of the problems with room‑scale VR adoption. It might be a preferable experience, but most people just don't have a spare empty room to use just for VR. Yeah, okay, it doesn't have to be a dedicated room, but you do need a completely empty area of at least 2 m x 2 m, or 6 ft x 6 ft, you usually want more than that and a bit of a buffer on all sides. Now some of the headsets I showed come with base stations. Those are used to improve the accuracy of positional tracking in room‑scale VR, they do a much better job of figuring out exactly where you are in a space, and that makes it more realistic in the environment. But one of the things you need to do with any VR equipment is before you begin, it's going to ask you to define the boundaries of the available space. This is usually done by taking one of your VR controllers and you mark out the edges of your room, and when you're immersed in the experience, if you start moving towards the edge of your physical space, then the virtual barrier or grid will appear in your view to remind you you can't go any further than this or you're going to walk into the wall or fall down the stairs or bump into the table. Some setups like the Oculus call this your guardian, others call it their chaperone. It's the same idea. Now you might wonder, well, what if the movement that I want to do in VR is more than just room size? What if I want to explore that entire building or climb that mountain? Well, usually more significant amounts of movement use the hand controllers. Now in the early days of VR, we tried using these like conventional video game controllers, so you just push forward on the joystick and you just start moving forward through the space. But in VR, that's too much visual stimulus and most people became motion sick very, very quickly. One technique is that when moving in VR, the edges of the screen are temporarily dimmed so you don't have so much motion in your peripheral vision. But it's more common to use a teleportation idea where you use the controller to point at a new location to move to and you're instantly transported to that spot. Now, when we talk about augmented or mixed reality, we really don't have the same issues because you're already aware of the space that you're in. But we've talked about the types of experience, the qualities of headsets, the kind of hardware, we've covered quite a bit of terminology and jargon, so with that, let's now explore a few more real‑world applications of this.

XR Application and use cases 

Virtual reality is often demonstrated with games. While that's not our focus, we can recognize that what works so well with VR and entertainment, that ability to be inside an environment, to shut out the world and be immersed in what you're doing. That's exactly what makes it so useful in business. But one thing to keep in mind. When you just see a still image or watch video of a VR experience, what you're not seeing is the single most important thing about it, which is that sense of immersion, of being inside the experience. So as I go through a few more examples, it's useful to not just look at what's happening in that, but also think about what would that be like to actually be present in that environment. Now there's a few general applications of extended reality that could apply to any business. First, education and training. Now we already have a lot of educational experiences available on any XR platform. Some focus on exploration, like putting yourself at the top of Everest or touring the great museums of the world, but there are also apps for teaching specific subjects like anatomy or astronomy or geography. But of course these ideas can all be applied to corporate education and on‑the‑job training, even just your own personal development. Now some people think about VR and education and imagine some simulated classroom, and actually I think that's kind of missing the point because with VR we can get out of the classroom and into the experience, into the environment. Think of VR apps like Mondly for language training. They'll put you in specific situations like a train journey or a taxi ride or checking into a hotel and use voice recognition so you can rehearse and take part in the conversation. And then you've got companies like Walmart and Verizon who are actively using VR for workplace training or new employee onboarding. They'll use it to simulate different in‑person customer service interactions, they'll use it to train people on new equipment or store layout. Verizon even trains employees on what to do in the event of a store robbery. And we could move from the more general into the much more specific technical options like pilot instruction. We've now gone from these massive, expensive flight simulators to something that can realistically be done with consumer‑level hardware. Now, while I said that we don't need a simulated classroom, well, one of the places where that can be useful is public speaking. Using VR to rehearse and increase your comfort levels of speaking in a classroom or lecture hall or conference room. And while we're talking about ways to improve our communication, we can also think about any data that we're presenting. Now your data can be a lot more persuasive, you can actually put people in the middle of it, and even more useful for yourself if you have more ways to view it, to interact with it, to manipulate it and bring it to life. Now earlier I talked about VR conferencing applications like Horizon Workrooms from Meta where you and your colleagues can bring your avatars into a conference room, you can talk, you can use hand gestures, you can draw on the whiteboard and share documents. If some of your colleagues don't have VR headsets, they can dial into that room and use their webcam like a normal video conferencing application. And as I mentioned in an earlier clip, you can add a virtual version of your real computer keyboard into the space to make it easier to type and take notes. Now moving across to kind of specific types of business, like construction, we can see how extended reality can be used across multiple different stages and aspects of a large business process, everything from the initial planning, design, and architecture, these days all major architectural software applications either generate or export into virtual environments. Because just as people used to build physical models of buildings, we can now generate virtual models. We can view them at many different sizes, we can step inside them, we can even simulate specific weather or lighting environments to see how that will look. We could use extended reality during the building phase itself, letting the engineers understand and see more about the infrastructure of the building. It can be used to simulate the current state of construction sites so different teams can have virtual visits, and VR can be used after the build for virtual walkarounds, which is happening a lot in real estate now. If we jump over to healthcare, there are already multiple educational options for simulating operations and procedures and simulating scenarios. It's also starting to be used for more diagnostic situations like 3D radiography. And other simulations are being used in treatment of issues like anxiety, PTSD, and autism spectrum disorders to either rehearse situations or practice social interactions. And it's even being used in personal healthcare. There are popular VR applications for exercise and for meditation. The difference between VR and AR applications might sometimes seem trivial. One's in a virtual room and the other's in a real room. But it goes much deeper than that. A pure VR experience, we might simulate a virtual repair of a piece of virtual machinery, but we could use AR to help in a real repair of a piece of real machinery, perhaps overlaying schematics or diagnostics. We could even bring in additional assistants. You could connect two individuals in different locations and allow them to work in the same part digital/part real space. There are experiences that naturally work better or even only make sense in augmented or mixed reality. And over the next few years, we expect that that will become the most common option in business and enterprise environments, particularly once we get more head‑worn. Our devices

Closing 

Some of the objections I've heard include isn't this just a gimmick, a passing phase, a technological fad? Or, you know these are never going to take off, right? They're too big, too bulky, too inconvenient. Or even, do you seriously think normal people are going to carry one of these around all the time? And actually, I'm not talking about VR or AR here, I'm talking about mobile phones and the same objections we heard again and again with the first generations of these. They're too heavy, too bulky, too inconvenient, too hard to use, not useful enough, only something that a techno nerd with too much money would buy and they'll never be popular. It will never be normal to have one of these. Well, you see how that turned out. Okay, I'm poking a little bit of fun at this. But it is technology. These will become smaller, lighter, cheaper, faster, more powerful with more features, better quality, more applications. There will be more and more compelling reasons to have them. And sometimes, as with mobile phones, it might just become a little more convenient to have one than to not have one. But as the hardware gets better, it becomes more useful to have better applications, which means more people join, which means you get better hardware, you get that tipping point, that positive feedback loop, and whether it takes 5 years or 10 or 20 to have this deeper integration between the hardware, the massive virtual spaces, all the applications that we need throughout our day, the ever‑present, high‑speed internet, whether we end up calling that the metaverse or just the place you sometimes go to work, sometimes go to meet friends, sometimes go to play games or watch an event, it is coming. Hope you enjoyed the course. I'll see you next time.



Blockchain Executive Briefing

Introduction 

Okay, wait a second before we even begin, let me clear up a couple of misunderstandings. This will be a short introduction to Blockchain: what it means, how it works, and most importantly, why anybody cares. What are the practical use cases for this? What are the situations and problems where Blockchain can actually help? Now, usually when I do these executive briefings, I like to talk about a subject as if you're completely new to it. But here I'm going to take a slightly different approach, and that doesn't mean I expect you to understand the Blockchain already. In fact, I hope you know nothing about it. If you could say to me, Simon until 30 seconds ago, I'd never even seen the word. That would be great. That would make my life easier. But I find most people have heard something about Blockchain, but often not directly. Meaning they didn't set out to learn about it, it has just crossed their radar once or twice or a dozen times, it keeps coming up. You might have seen Blockchain mentioned in a business article here or a new story there. You've heard somebody talk about it in a meeting or seen it as a bullet point in a corporate strategy document, but nobody's taken the time to properly explain it and what that can lead to is an awareness about it that I'll describe as, "Yeah, I've heard about Blockchain, it's something to do with X." So I'll hear comments like, "I don't fully understand what Blockchain does, but it's something to do with financial transactions." Or, "Isn't Blockchain a way of making electronic contracts." Or, "I've read that Blockchain is something to do with Bitcoin, I'm just not sure what the difference is." And I could go on, it's not that any of these are completely wrong, but they are limited and incomplete understandings. They're all missing the big picture and a few of these ideas can even hold you back from recognizing what else this can be useful for. So even if you are coming to this completely fresh, which is great, I'm going to talk about common misunderstandings, because even if you don't have any preconceptions about Blockchain, I guarantee you some of the people you work with do. So, in the next few minutes we're going to cover the most important ideas. We'll get past a lot of the hype around Blockchain, I'll go over the terminology, the most common words and phrases that are associated with this, not just what Blockchain means, but also terms like distributed ledger, smart contracts, and consensus. We'll see several of the emerging Blockchain platforms and we'll talk about what that even means, and we'll explore multiple, real‑world business scenarios to make it easier for you to recognize where Blockchain might be useful for you or your organization. Okay, now show the thing. I'm Simon Allardice and welcome to the Executive Briefing on Blockchain.

The early days of bitcoin

Blockchain is a fairly new technology. It's a little over 10 years old and everything we're going to talk about here has its origins in this white paper published in late 2008 by Satoshi Nakamoto. We'll come back to that name in a couple of minutes. Now this is only eight pages long, you can download it from the web, it's freely available. Now you don't need to go and find it and you probably shouldn't unless you actually enjoy heavily mathematical computing white papers, this is full of formulas and code examples and phrases like, rearranging to avoid something the infinite tail of the distribution. And I'll admit this isn't my idea of light bedtime reading, but there are a couple of things I'd like to point out about this. First, this paper is entitled, Bitcoin: A Peer‑to‑Peer Electronic Cash System. And yes, I said Bitcoin, not Blockchain. The word Blockchain, the way we use it now as a single word, does not appear in this original paper. But make no mistake, this is about Blockchain. The word block appears everywhere in this document as does chain, and I will talk about what those words mean here in just a moment. But what this paper was describing was all the underlying technology, the software that had to be defined and formalized and implemented to make Bitcoin, the cryptocurrency, possible. And all that underlying technology described here is what we now call Blockchain. You might then ask, "So is Blockchain all about Bitcoin?" No, absolutely not. Let me say that again. No. Blockchain is not just about Bitcoin. Blockchain is not only about cryptocurrency, it's not only for finance, and these days, most businesses interested in Blockchain are using it for reasons that have nothing to do with Bitcoin or any other cryptocurrency. Now, just to be clear, I don't need you to have experience with cryptocurrencies like Bitcoin. We're going to talk about them a little bit here because of this history, but it's not going to be our main focus. But this Bitcoin paper was where the technology we now call Blockchain began, but as has happened with so many technologies, it might have been invented to fix one specific problem and then people figured out it could be applied to a whole bunch of different problems. Sidebar for an example of an equivalent situation, a technology we now use very differently from the way it was first intended. Here's one you already know. So I just showed the Bitcoin white paper, this is a different white paper from 1989 and it's entitled, Information Management: A Proposal and the abstract says, this is about the management of general information about accelerators and experiments at CERN. It discusses the problems of loss of information, about complex evolving systems, and derives a solution based on a distributed hypertext system. And yes, this is the proposal by Tim Berners‑Lee that describes what would become the World Wide Web. The point is that the original purpose of the World Wide Web, the reason it exists in the first place, was as a way for some scientists at the CERN labs in Switzerland to share information about their experiments. I don't think I need to point out, we found a few more uses for the Web than just this. And in the same way, yes, Blockchain was invented to support Bitcoin, but it's now been used and applied in a lot of different situations. And because it's such a new technology, we are still realizing more and more places where it can be useful.

Common Misunderstandings

Now, the fact that Blockchain was developed for the Bitcoin project. We have that history, that association, between Blockchain and cryptocurrencies that can lead to a few of those misunderstandings, because a lot of reporting about cryptocurrency is sensationalist. Blockchain‑based cryptocurrencies are of nearly 2,000%. Some experts say there's no limit to how high this could go. Cryptocurrency prices have plunged almost 85% in the last 48 hours after a frenzied sell off around the world. And if most of what you've heard about Blockchain was related to cryptocurrency, you'll have seen stories about money laundering, dubious get‑rich‑quick schemes, there's been bankruptcies and thefts and even disappearances. So you might even wonder, is this what Blockchain is about? Do I even want to get involved? I mean, is anyone actually doing serious work with Blockchain? And yes, they are. And there's an ever increasing amount of infrastructure and commercial services starting to appear to support organizations working with Blockchain. For example, IBM now offer a Blockchain platform, as does Microsoft, as does Amazon. These companies aren't doing this because they've seen a few get‑rich‑quick schemes and want to jump on board. They're heavily investing in Blockchain because they see the demand for it and the future of it. But we can admit that yes, Blockchain did not follow some slow, conventional, normal, slightly boring adoption into the business world. It's been an unconventional story. I mentioned this original white paper was written by Satoshi Nakamoto, we don't actually know who that is. You see after publishing this paper, a person with this name contributed to the original Bitcoin mailing list for a couple of years and then went silent, and no one's been able to figure out if there actually is, or perhaps was, a Satoshi Nakamoto. Or if that was a pseudonym for someone, or even a pseudonym for several people. There's been a lot of conjecture, many different names suggested, but right now we don't know. Now, we don't actually need Satoshi Nakamoto for all this to work. The implementation of Blockchain is completely public and open source. But yes, still a little odd. But despite all this, there is incredible value here. But okay, we've been going for a few minutes, we've talked about some history and some misunderstandings, but we haven't yet hit, but what does this do? What problems does this solve? And I believe if you want to understand the nature of Blockchain, not just recite a definition of it, but actually get the point, it's very helpful to take a step back, way back. And I'm going to say not just forget about Blockchain for a moment, but forget about computers for a moment. And let's talk about how we might have done things 200 years ago

Ledgers and distributed Ledgers 

For hundreds of years we have used ledgers to keep track of accounts and transactions. Now this ledger is from the 1800's, but I could go to an office supply store today and be able to find a paper ledger that's basically identical, and they're great for creating a permanent record. Not just describing how things are right now, but all the history that led to this point. So to track a bank account, you begin with writing down an initial balance, but from that point on you wouldn't ever go back and change that first entry. You just keep adding new entries to the ledger for each new debit or new credit. So the ledger supports this full history. So we can use these to track transactions, the date, the amount, what are the parties involved. A transaction could be something trivial like the purchase of a dozen eggs, but it could be the purchase of an acre of land, or a house, or a company. A ledger could also represent the details of a loan and outstanding money owed. It could be a continual record of who owns certain assets or keep track of physical possessions being lent from one person to another. But very often the details of any transaction aren't just recorded in one physical ledger, but in multiple. So if my company decides to buy a horse and cart from your company, that's something I'd record in my ledger as a purchase, an expense, and something you'd record in your ledger as a sale. But there are issues with physical ledgers. They could be damaged. They could be lost. They could be filled out incorrectly. Someone could write an entry in the wrong place, or miscalculate a balance, or forget to write an entry at all. And there could be intentional deception. Someone might change an old entry after the fact. But perhaps the biggest problem is that if we're dealing with transactions involving different parties, the ledgers can be inconsistent. So if I borrowed $100 from Alice, I could say, "Look at my ledger, it shows I've paid everything off." And Alice then says, "Look at my ledger, it has no record of a payment in January. You still owe me." What do we do? Maybe my records are correct. Maybe hers are. Any discrepancy is a real problem if we don't have some way to settle it. So what's happened historically is that the more important the transaction becomes, the more likely you are to involve a trusted, and hopefully neutral, third party to mitigate risk. So instead of a direct transaction between two parties where there is often a benefit for one to deceive the other. We both agree we'll use this third party. We agree they are the single source of truth about the details of this transaction and any money or documents will flow through them, rather than directly between the buyer and the seller. This is also institutionalized now in commerce that it's part of the landscape. It's very easy for us to go numb to all the different intermediaries involved in, even our personal day‑to‑day transactions. Just think of the banks and the credit card companies involved in any purchase we make, or say the escrow company in a real estate transaction. And of course all these third parties need to get paid to be that middleman, that intermediary, which adds a cost and often a delay to every transaction. But here's a thought exercise, if much of the need for an intermediary comes from the basic problem that to avoid disagreement now or later, to avoid fraud, something needs to act as that single authoritative source of truth. But in a transaction, it doesn't make sense for me to just trust in you and the version of events you've written in your ledger. And it also doesn't make sense for you to just blindly trust in me and my version of events. So imagine, what if there was only one ledger? That I have one and you have one, but whatever I write in, my ledger magically appears in yours and whatever you write in your ledger magically appears in mine. If either you or I write a new transaction, a new entry in this magical ledger. Once it appears, it's set. Neither of us can delete it or change it. Now for the moment. Let's postpone the how of this and just focus on, what could you do if that was true? What could you do if we could have a shared, incorruptible, secure, and authoritative ledger? Where the ledger becomes the source of truth and that because it includes history, it can be the authoritative record of proof that, yes you actually do own the asset that you say you own, and that yes, I have the asset I say I have. And we could decide on a transaction that exchanges one for the other. We could both could immediately have an identical copy of the transaction in that ledger and it couldn't be changed. Well then, we wouldn't need a middle man. Everything becomes cheaper and quicker if there's no inherent cost to a transaction, we could make more transactions, whether small or large or whatever. And this magical ledger doesn't just have to be between two people, we could make it between three or 10 or 1000. We could make it completely public. And the thing is, we don't have to trust each individual party. We don't have to like them or even know them, as long as we can all trust the ledger as that source of truth. And this is what Blockchain does. And you'll hear the term distributed ledger or shared ledger, the entire point of this technology is to create a distributed digital ledger, a shared network of information that is duplicated across many different computers. And when any person in this adds a new entry, a new transaction to this ledger, it is automatically copied across all the different participants. What we call the different nodes in the system. Now with the Blockchain that's used for Bitcoin for example, and has all the transactions for that currency, there are actually tens of thousands of different notes, different computers that have a copy of the same data. And what makes Blockchain very different from a conventional database is that this distributed ledger isn't owned by just one personal organization. There is no single administrator, nobody is in charge, because the data isn't only on one specific computer in one server room in one company. It is decentralized. So any changes to it aren't done in one master location and then blasted out to all the subordinate locations. No there is no master, there is no subordinates, everyone is a peer, and new transactions can be added at any point in this network and there will be automatically copied peer‑to‑peer. Now there are some applications of Blockchain that don't need to be public and where we might even want some extra layer of control, we'll get to that a little later. But it's this ability of Blockchain to act as a single shared source of truth that works incredibly well for financial transactions, but can also make it useful for so much more. But first, let's get a little bit deeper into how Blockchain can actually do this.

Building a Blockchain

Earlier I said that in that original white paper, the word Blockchain doesn't appear, at least not as one single word, but the word block does. So first, what is a block? Well I'm going to have to answer that twice, because the first time it's going to sound kind of vague. A block is the name we use here for a small amount of data. I told you it was going to sound kind of vague. Okay, what do I mean a small amount of data? How much data is a small amount and what is that data? Okay, so if we were dealing with financial transactions where each individual transaction is small, it could just be the details of when, how much, who's it from, who's it to. Well, in a physical ledger we can't get away from the fact that our transactions are grouped page by page. We add new transactions one after the other and when the page fills up, we go to the next page. Well when using Blockchain for financial transactions, like with Bitcoin, we can think of a block as kind of the equivalent of a page. That first, it is simply a way to batch together some recent transactions. Each block can hold the details of several hundred to a couple of thousand transactions, and when new transactions happen, they get batched together into this block and this block is then added after any existing blocks, a blockchain. And the chain idea is vital here, because the Blockchain grows by having more and more blocks appended to it, just added on the end, and they're always added in sequence. I can't add a new block in the middle. I can't rearrange the existing blocks. I can't just delete an earlier block or edit it. The term sometimes used is that Blockchain is append only. And back to that idea of this physical ledger where you would never change the details of a bank account by just flicking back through some old completed pages, finding an old entry, and just changing it. No, you just keep adding new entries, new debits and credits to the ledger. You append onto what is already there. Even in a situation where you found out that there was an earlier incorrect transaction, you don't delete it, you would add a new transaction that would reverse the old one. That's the way that you have to do a ledger, and this is also what applies here. Building out this as a chain allows you to have both that state of how everything is now and all the history that led to this point. But, you might ask, if this Blockchain is actually shared across multiple computers, potentially even thousands of computers and they all have a copy of the same data, all the blocks. What's to actually stop someone taking one of those earlier blocks and just changing it? You know, just faking a new transaction, or deleting one, or changing the details of an old one? Good question. Let's talk about that.

Crytograpgic hashing in blockchain

Now at this point you might think, okay, if a block is a way to store a small amount of data and store it sequentially one block after the other, well, is that really such a big deal? I mean, couldn't we already do that? So, yes, there is a bit more to this idea of a block. In blockchain, every block also uses what's called a cryptographic hash. Now this isn't something unique to blockchain. Cryptographic hashing, or just hashing, is a common task in programming, particularly with anything to do with security or encryption. Now if you're already comfortable with the purposes and benefits of hashing, feel free to skip ahead a couple of minutes. If not, just give me a moment to explain why this is so important. So, think of the word hash as we might use it in cooking, like hash browns or corn beef hash. A hash is where you take some ingredients, chop them up into little bits, mix it all up, and get the result. That's kind of what we're doing here, but with data. We take the raw ingredients, our data, and chop it up into little pieces, we perform various calculations on those different pieces, and then combine them together to end up with a small value, a result that often looks very meaningless and random but was actually based on that original data. Let me take you through an intentionally simplified example. For data, I'm going to use just one basic sentence. I'll then take each individual letter of this and give it a number based on where that is in the alphabet, like A is 1, B is 2, to get a row of numbers. From this, I'll choose to take every second number and add all those numbers to give me 218. I'll then take the remaining numbers and add all those up to give me 245. I'll then decide to take both of those numbers and just reverse them in place. And then, why not, let's take these and multiply them together, 440104, and then I'll decide to convert every second number of this back to a letter. So, this 4 becomes a D, this 1 becomes an A, and so on. So my result here is 4D0A0D. Now, if it seems like I just made up a bunch of unpredictable and almost meaningless calculations, yes, I just made that all up. Real hash calculations are more involved. But the principle is that whatever result I get from doing this is entirely dependent on that original data and that this result is repeatable, meaning if I took the same data, that same sentence, and go through the same process a year from now, I would still get exactly the same result. But, if I made even a tiny change to the original data and go through the same process, I would get a totally different result. So if I change just one letter of this sentence, the i of bitcoin to an a, well now the numbers are different, and that means the result of adding them all up is different. So the result of reversing them is different. The result of multiplying them is different. And instead of that first result of 4D0A0D, we get an entirely dissimilar 6E0D. And when you hash data, you will get a result that's smaller, often much, much smaller than the original data. Instead of one sentence, I could have hashed the contents of an entire 500‑page document and ended up with a hash value that might look something like this. Now, it is important to know that hashing is not encryption, and what I mean by that is with encryption, it's the idea where we can encrypt some data to get a scrambled and unreadable result, but we could later decrypt it to get the original data back. Here, that is not what we're doing. Hashing is not reversible, and it's not meant to be. So if all I had is the hash value, there's no way for me to convert or decrypt it back to the original data. An analogy I've made before is that to take a hash value and expect to get the original data back from it is about as useful as beginning with a plate of corned beef hash and expect to turn it back into a cow and a bag of potatoes. You can't. You've lost too much in the hashing process. So then, why? Why do we do this if we can't get the original data back? Well, because the point of a hash is that it can act as a short signature or fingerprint of the data. It becomes an additional way to verify that the original data hasn't been tampered with. Because when you're storing data in multiple places or sending it from place to place, as long as you also have a copy of the hash value, you could then find out if that data has been tampered with anywhere. So, six months later or a year from now, five years from now, you could take that data, you could hash it again, and then you compare the new hash value against the old hash value. And if they match, you're good. If they don't, you know the data has been tampered with since you originally saved it. And that's why hashing is so useful in computing. And in blockchain, this is done at multiple levels. Each transaction in that block will also have its own hash based on the data in just that transaction, Not only that, but each block also has its own hash for the block itself. And, as every new block is created, one of the pieces of data it stores is actually a copy of the hash value of the previous block in the chain. Now, what this all means is that in a blockchain if you try and change one tiny piece of earlier data, like trying to change one transaction in any of those earlier existing blocks, well then the hash for that transaction will be different, which means the hash for that entire block no longer validates, which means that next block which had a copy of that hash no longer matches, which then has a domino effect on all subsequent blocks because none of them match, and this cryptographic hashing is the mechanism that makes blockchain very resistant to tampering, even when we share it between dozens, hundreds or thousands of different computers.

Terminology Sidebar 

A quick sidebar about terminology, you'll sometimes see the term immutable used with Blockchain. Immutable meaning unchangeable, that you can't mutate. You can't change the data in a blockchain, and it's a slightly contentious term. Technically you could change a block in the blockchain. It's more that there's really no point, it will be apparent that you've tampered with it in a way that it's no longer consistent within itself, as well as no longer consistent with all the other copies in the Blockchain network. The more important idea is that Blockchain is by design tamper evident, and that that's the point of this technology. It's not a happy coincidence. This is part of the goals of that original white paper. And when you have multiple copies of that Blockchain distributed ledger, if the data is shared across multiple computers and if there is tampering with one of them, it doesn't break everything. It's built into the system. But as I mentioned, Blockchain doesn't have a single administrator, you don't have to have one copy that's in charge of all the other copies. So what happens is that Blockchain automatically implements what's called a consensus mechanism. To simplify that, if there were 10 copies of the Blockchain data and eight copies of the data, agree with each other based on those cryptographic hashes, well then the eight copies have consensus and that's the version that will be automatically replicated across the other nodes in the network. Now, another word that you'll often see when discussing Blockchain is just trust. I do find that one a challenging word, because it can mean different things to different people. See with Blockchain, it's not that we dislike trust, it's that when implemented correctly, a Blockchain solution can just minimize the need for it. It's not about having to build a network where we have to develop explicit trust between all the players in it. Now, it's more that if we can trust the data, we don't then need to have to trust every individual involved, and we wouldn't need that intermediary, that central authority to allow us to conduct a transaction. We can trust that the system itself is enough to verify that.

Applicants of Blockchain 

Even though Blockchain began as that underlying technology behind Bitcoin, it doesn't have to be about explicit financial transactions. A block can contain other kinds of data, be used for other reasons. And the idea of a ledger is still valid here because they weren't always just about money, ledgers were also used to track any kind of important record, like births and marriages and deaths. They might be a record of the ownership of property without specifying an actual value for it. And you could even use a physical ledger to write down a recipe, or directions, or draw a blueprint. But before we get into an example, let's quickly revisit and refine our use of the word blockchain, because we could use it to refer to the concept, the idea, this general technology area, as in, learning about blockchain, but we could also mean one specific implementation of this. We could talk about the blockchain used for Bitcoin, which is completely different from the blockchain used for ethereum, which is different from the Blockchain you might implement to manage your company's supply chain. So to be clear, there isn't just one blockchain, we can have entirely independent blockchains created for different purposes. And each of those blockchains is still a distributed ledger that could be shared across hundreds or thousands of computers, or perhaps just shared with just a few, depending on what that particular blockchain is intended for. When we talk about different blockchains, you'll often hear them described as either public or private. People might say Bitcoin uses a public blockchain, so it doesn't restrict any access, it doesn't require you to authenticate with a user id and password, and no one's in charge, anyone can get to the Bitcoin Blockchain. But when considering a blockchain for business use and having this massively available public access isn't often a requirement. You probably don't need or want it publicly accessible. You don't want just anybody able to add new transactions to your Blockchain. So in many business situations, public or private isn't the most important question. It's more about whether that Blockchain should be permissioned or permissionless. Now, a permissionless blockchain, like the Bitcoin Blockchain is set up that way by design not to restrict access, you don't need to be granted permission to use the Bitcoin Blockchain. But in business use it is more common to have a permissioned blockchain, where it is still a distributed ledger, but you can also control access to it by providing a unique identity for each allowed user. So for corporate and business applications, one of the attractions is the ability to implement different levels of permission. So some users might be allowed to just read from the blockchain but not append to it, or even just read from specific sections of the blockchain where others could append new data. And having permissions to blockchains is what can open this up to a lot more applications. For example, healthcare. Blockchain could be used as a way to manage medical records as a shared ledger that acts as that authoritative, single source of truth, and shared between individuals, medical professionals, hospitals, insurance companies, but with tight permissions to control what parts of your data that say a medical professional can see and even how long they would be allowed to see it, and still maintain compliance with privacy and regulatory rules about medical data. Because right now medical records are often stored in multiple disparate systems. And the problems with that can range from something mundane like insurance being denied because of an out‑of‑date patient address that one place has, to different medical professionals having conflicting or ambiguous medical histories for a patient, different lists of medications, or different lists of allergies. So there's a lot of potential for improving communication and even patient outcome by making sure everyone has trustable, up‑to‑date information. Beyond healthcare, people are also looking at Blockchain as a way to manage digital assets, like documents or media. Blockchain could act as that trusted record of when something was created, who it was created by, who it was purchased by. There's also a lot of work being done on using Blockchain to vastly streamline digital rights and royalties. Now, that's a couple of high level scenarios, but let's dive a little deeper into one specific example

Scenario: Supply Chain Management 

Something that's becoming almost a classic example of Blockchain for business is how it can be used for supply chain management. And it's a useful example even if it's something you're not personally interested in, because it's a total shift away from that cryptocurrency, financial use of Blockchain. Now, supply chain management is by nature just a continual challenge in logistics, because there can be so many different participants. You have providers of raw materials, there's factories, suppliers, transportation, warehousing and storage, inventory management, insurance, there's customs if you're crossing borders, distributors, wholesalers, retailers, even the end customer. And to track and manage the progress of materials, supplies, components, and products, through any supply chain is immensely challenging. You're often trying to orchestrate many different companies and trying to get their internal systems to interact with each other. And even trying to track just one item, if that's even possible, might require having to authenticate and query across multiple independent computer systems just to try and figure out where something is. With Blockchain, instead of trying to integrate all the various computer systems and databases with each other, we can flip this. We can just have that shared digital ledger automatically replicated across all participants, where each participant can keep their own systems but also have the ability to add entries to this ledger. And as things move through the system, the ledger can be updated by multiple parties with the current state of that item and everyone who needs to see it, can see it. And it doesn't just lead to better tracking of items, but that itself leads to better data about inventory, better predictions about resources, and how those resources will be needed, and where and when, and everything is time stamped. So there's better identification of any inefficiencies or choke points in the system. And it would also vastly simplify the addition of any new participants into the supply chain. To bring in new players or replace existing ones. They don't have to figure out how to integrate their systems with half a dozen other vendors. They just need to be able to talk to the Blockchain. And beyond tracking the data, there's also ways that Blockchain can help automate a lot of business processes, and that uses something called smart contracts. Time to talk about that.

Smart Contracts and platform overview 

One aspect of Blockchain that's rapidly growing in popularity is smart contracts. Instead of just storing raw data on a Blockchain, we could instead store a contract, an actual agreement, and this doesn't just mean a digitized PDF. It means a mini computer program, a predefined list of rules to very specifically control the business transaction. And what makes a smart contract smart is that once on the blockchain it will be executed automatically, based on those rules. For example, a smart contract might define that as soon as Person A transfers an amount to Person B, the system will then automatically transfer the record of ownership of some asset. Or it could define rules based on a specific date, or a specific time, or on the value of an asset. And the thing is, as smart contracts are written in code so they require no intermediary to make them happen, they're fully automatic. And like anything else in the Blockchain, once it's placed there, it can't be changed. So, smart contracts are a feature that's already built into many of the Blockchain platforms and services you can now find, and it's time we talked a little bit about those. Earlier I briefly showed a few of the commercial Blockchain services from IBM, Amazon, Microsoft, and I'd intentionally pick those as names you would recognize, even if you've never set foot in the Blockchain world. But as you step more into this, some of the other names you'll see for Blockchain platforms and services will include Ethereum, Hyperledger Fabric, and Corda. And while they all take a slightly different approach, they're all trying to provide and simplify all that underlying Blockchain technology so that you, or your organization, your teams, your developers, could build your own applications on top of that infrastructure. So they take care of that distributed ledger technology, but they don't define what you're going to do with it. Now, this course is far too short to get into any kind of worthwhile comparison between Blockchain services. So I will leave that as an exercise for the viewer, but let me wrap this up. I was recently asked, "Simon, do you think we'll still be talking about Blockchain in five or 10 years?" And the question was asked in the sense of, did I think this was all a lot of hype and snake oil, something that would just end up on the technological scrapheap. And, no. We may talk about Blockchain less in the future, but not because it goes away, more because it becomes part of the technology landscape. Take something like TCP/IP, you use this every day. You're using it right now, but most people don't think about it, they don't have to, it's just how the internet works. And I think Blockchain is going to be the same way, it is going to be a vital part of more and more systems developed over the next few years. Because despite all the hype around things like cryptocurrencies, the reality is that having a shared, distributed ledger that's tamper evident with a high level of trust, this is incredibly useful and we've only begun to realize all the different situations that could be applied. So yes, Blockchain is important and it's not going away. I hope you enjoyed this executive briefing. I'll see you next time. We hope you enjoyed this course. If you're interested in more content for technical leaders, content we keep short and focused, with the up to date information you need to be informed and make decisions, but without getting buried in the details. Find other courses like this at plrsig.ht/exec.

Surveying Blockchain Technology

Course Overview 

Hi. My name is Scott Driscoll, and welcome to my course Surveying Blockchain Technologies for Enterprise. I'm a software developer that's been utterly fascinated by blockchain's ever increasing influence on our world. Blockchain started as the technology behind bitcoin and Ethereum, but is now being tested and deployed at hundreds of major corporations in industries ranging from supply chain to banking to healthcare. Why are all these companies interested in the technology behind cryptocurrencies? In this course, we're going to answer this by breaking down what blockchain is and why it's useful, surveying a wide range of enterprise applications, and also revealing a number of different enterprise blockchain software tools. By the end of this course, you'll have a tangible understanding of how blockchain can help businesses increase efficiency, reduce cost, especially compliance cost, and even capture new markets. You'll also get a clear view of the challenges blockchain faces including why these gains will likely require wide-spread industry collaboration. This course is mostly high level and designed to be helpful to business people and developers alike without any prerequisites, but some familiarity with bitcoin and Ethereum may be helpful. From here, you should feel comfortable diving deeper into the blockchain world with courses on smart contracts and specific enterprise blockchains like Hyperledger or Quorum. I hope you'll join me on this journey to learn about the rapidly developing world of enterprise blockchain with this course at Pluralsight.

Introducing Blockchain: Improving Supply Chain Food Safety

 What is Blockchain? How is Enterprise Different than bitcoin

Hello. My name is Scott Driscoll. Welcome to Surveying Blockchain Technologies for Enterprise. This course will give you a tangible introduction to what blockchain is and how it can transform enterprise by surveying a broad array of different applications and software tools. Blockchain, the technology behind the digital currency bitcoin, is now being brought to bear on a range of enterprise applications beyond currency in almost every industry from supply chain to banking, energy, healthcare, and many more. At a high level, blockchain provides a new way for organizations to interact that doesn't require as much trust. This, in turn, promises improved efficiency, security, transparency, and even the opportunity for new business models. Let's dive into a specific application area, supply chains, to get some tangible insights into what blockchain is, how it can benefit enterprise, and some of its challenges. IBM and a group of retailers including Walmart have recently launched a pilot using blockchain to help increase food safety. A current outbreak of Salmonella in papayas gives us insight in to the motivations these companies have to improve the status quo. As of September 11, 2017, according to a U. S. government health organization, 210 people across 24 states have been infected with illnesses beginning in May and June. Based on the times of the press releases, it appears that it took several weeks between identifying strains of Salmonella in patients to identifying the likely source, a farm in Mexico. While the source of an outbreak is being tracked down, lives are at risk, as well as substantial money. The government initially recommended that an entire type of papaya from Mexico not be eaten. Later, after more tracing, the ban was restricted to specific brands and, finally, serialized lots of papayas. Given the cost and health risk, why did it take so long? To trace a specific papaya back to its source, the government likely had to contact retailers, distributors, multiple brands, importers, and farms all individually. One can imagine the situation gets even more complicated as products are mixed with other ingredients and long shelf-life products, as was evident in another recent outbreak of E. coli in SoyNut butter. Instead, what if there were a single database that kept track of every single papaya's journey? The government and companies would be able to immediately see the source of contamination and pull only the risky papayas. This sounds great, but even the most basic questions about how this might work reveal myriad complications. Who would hold that data? One retailer wouldn't want another to hold it. And, given all the recent security breaches at government databases, it's unlikely anyone would want the government to operate the database. Who would be able to see the data? Retailer A wouldn't want retailer B to know how many papayas it was buying, what price it was paying, and from what distributors. Finally, who decides what the right data is? Consider what happens when a retailer orders 10 crates of papayas and the supplier delivers 9 crates. Now the retailer system shows 9 whereas the supplier system may show 10. The retailer's accounting office will likely need to call the suppliers to request a credit on the invoice. Here we see that each organization up and down the supply chain maintains their own records, with good reason. One organization's records certainly may not reflect the truth according to another. Reconciling inevitable differences represents a major cost in almost any industry. Blockchain represents a new way to share data that addresses these questions. Blockchain can be thought of as a shared database, one in which control is truly shared between different organizations and is especially helpful when those organizations don't necessarily trust each other. In our papaya example, imagine if each papaya had a serial number and current owner. As the papayas move through the food supply chain, the owner is updated, providing immediate visibility of every papaya's path. This also looks like an accounting ledger, which is where another common name for blockchain comes from, distributed ledger technology, or DLT. So how does this work? What does it mean to share a database? For starters, changes to the database must be approved by everyone. In that case where the retailer and supplier disagreed about how many papayas were delivered, one or the other wouldn't be able to independently update that record. Rules are established that must be obeyed in order to accept changes. For instance, either both the supplier and retailer need to sign off on the number before entering the change or, perhaps, a third-party auditor needs to sign off on the number delivered. In this way, we effectively avoid any reconciliation. Valid data was required on the way in to the database. In addition to regulating the entry of new data, a blockchain database has protections against tampering. Every new change to the database is accompanied with a kind of data fingerprint called a cryptographic hash. Hashes provide a short representation for a larger amount of data. A hash of the entire Bible can be only 32 bits, but changing even a single letter in the Bible would generate a completely different hash. See how different the hashes are from these two similar sentences. In addition to this marker that tells us whether a single bit changes, blockchain goes further to link these markers together. Every time new data is added, a hash is calculated on that new data along with the previous hash. Because each hash builds on the previous one, the data forms a chain where any change in the historical data will immediately be obvious. For efficiency reasons, new records are usually added in groups called blocks, which is where we get the name blockchain. In addition, new data must be accompanied by a digital signature, another cryptographic tool that makes sure only authorized people can change data and that there's also a record of who did what in case of any wrongdoing. Finally, each peer maintains their own copy of this common ledger where they independently check the rules, hashes, and signatures as new data arrive. All these components contribute to a database where every participant has truly shared control and strong assurance that data won't be tampered with. The end result is more trustworthy data even though it's not on your own database. And with this new later of trust in data, we unlock all sorts of efficiencies and possibilities. With a single shared ledger, it would be trivial to track down the path of a papaya in seconds. Trusting data also unlocks automation possibilities across what I'll call trust boundaries. A new order for food could be placed automatically when stock runs low at a retailer. Or shipping insurance could automatically be paid when a shipment is late or a temperature censor indicates frozen goods have melted in route. All these types of automation are possible without blockchain, they require a certain amount of trusts in the inputs and, therefore, procedures to cross-check and verify information. Blockchain reduces and, in some cases, eliminates the checks and controls required with that trust, thereby unlocking automation. Now despite this promise, there is reason for skepticism. Couldn't a group of companies get together and form a consortium group that runs a database in which everyone is given controlled access? Why blockchain? There is a spirited debate going on about this. Is blockchain really different than a distributed database? And if so, where does it add value? Even determining exactly what a blockchain is and isn't is an evolving question. Hundreds of enterprises from almost every industry are currently running pilots to hone in on exactly what blockchain should look like and where it can deliver value. While many pilots recently begun to show promise, there are many details to be decided in any implementation, and adaptations must be made from the blockchain behind bitcoin to one that works well for enterprise. For example, in bitcoin, all data is viewable by everyone, whereas in enterprise data must be kept private, and not only from the outside, but also selectively within the network. In our papaya example, different retailers wouldn't want each other to see how much fruit was ordered, but it would be valuable to give governments or auditors access to this data. Performance is another area where the original blockchain falls far short of enterprise needs. Bitcoin can only handle a few transactions per second while modern databases handle millions. The blockchain behind bitcoin is also very expensive, by design in fact. Various fees and costs are needed to prevent abuse while keeping the system completely open. Almost all enterprise blockchain platforms in development hope to exchange openness for better privacy and performance while still retaining the benefits of decentralization. Our last module, Surveying Enterprise Blockchain Tools, will detail a broad range of these and the tradeoffs each has made. But first, let's better understand the problems at which blockchain is being targeted in the module Surveying Enterprise Applications.

Surveying Enterprise Blockchain Use Cases 

Improving Supply Chains with system wide transparency

In this module, we're going to look across a broad array of industries to see example enterprise applications where blockchain promises substantial value. Despite blockchain's strong potential, many have commented that blockchain technology does not neatly fit into existing business models. It's not, for instance, just a different database that can be swapped in by IT alone. Blockchain adds value by creating a new way for separate organizations to work together, so realizing its benefits often requires innovation in business models in tandem with technology innovation itself. We're going to cover a number of areas including supply chains, energy, finance, banking, and healthcare. Along the way, we'll also touch on IoT, or internet of things, and insurance. We began looking at food safety in the supply chain and how blockchain could help track the provenance or history of ownership of a specific item from farm all the way to table. While giving government authorities extra visibility can help track down a safety concern, almost every other party in a supply chain can also benefit from extra insight. Let's start at the farm, the furthest point from end consumers. While consumers may be willing to pay a premium for features like sustainable production, GMO free, and simply better taste, farmers cannot necessarily capture those premiums. Food typically goes through many layers of middlemen like distributors, packers, processors, and retailers before it reaches consumers. Irrefutable documentation of the value added at each step not only will give consumers more confidence and willingness to pay, but will translate into higher pay for those adding the value. A company called ripe. io seeks to use IoT sensors to help in this documentation effort. This starts at the farm with humidity, temperature, and even color sensors on crops, continues with temperature sensors on trucks and in packing facilities, all the way to processors that record exactly which foods go into final products. With increased visibility, added value can be better rewarded. These quality markers can gain additional power as third-party auditors or agencies add additional confirmations or measurements to the chain. More data isn't just about fairer payments, but also about unlocking opportunities. Ripe. io says that 10% of crops are thrown out in peak seasons. A company called Sweetbridge says only 75% of all global assets like trucks, warehouses, etc. are used at any given time largely because of siloed data. With a better-connected network, there is ample opportunity for better utilization. This is especially true for smaller companies. While large companies like Apple and Walmart already have good visibility from retail inventory all the way down to raw material stocks, smaller players do not always have access to such data and cannot react efficiently. For example, is an uptake in orders due to a temporary marketing campaign or a general increased interest in a product? Unknowns like these often lead to oversized reactions far down the supply chain known as bullwhip effects. With better access to data like retail store inventories, marketing campaigns, the entire chain can better match supply with demand. A company called Everledger is also using the blockchain to improve transparency, but with diamonds rather than food. The goal is to prevent blood diamonds, diamonds whose sales fund wars, and also prevent theft and insurance fraud. Everledger says over 2 billion pounds of loss is due to insurance fraud every year. To address this, Everledger records a fingerprint of high-value diamonds on the blockchain based on inherit characteristics. As the diamond changes hands, these changes in ownership are also recorded. And importantly, any reported thefts are recorded. This way, a potential buyer can check the blockchain to make sure they're not about to buy a stolen diamond. And if police recover diamonds, they can be returned to the original owners or insurers. While a thief could try to modify a diamond to obscure its history, they wouldn't be able to do so without also damaging the diamond. Everledger is branching out to the art world too and even luxury goods as a way to reduce counterfeits and protect brand image. While provenance is important in almost any industry, it is vital in the drug industry where counterfeit drugs can lead to deaths. In these cases, blockchain technology's immutability is the key to the traceability. No one can make a change to a record after it has been entered.

Immutability on private vs public blockchains 

It's worth diving a little deeper into the details to understand some nuances in both the confidentiality and security of these systems. Everledger actually uses two different blockchains to store data. Data hashes are stored in a public ledger, like bitcoin, but additional details, of specific police reports for examples, are stored on a consortium chain that only provides access to select members. Why use two chains? Consider the immutability claim of a chain run by a group of, say, 10 companies. In private chains like these, immutability is a result of a single company not being able to change a record without the other nine agreeing to it. But if all 10 agree, there's not a technical reason they couldn't all rewind, change some data, and reenter the data. It turns out immutability, like many other properties of blockchains, is on a sliding scale. Now if transactions are never used outside of a small consortium, immutability may not matter. But if a large network of police departments and insurance companies want to trust it, immutability, or the inability for data to be changed without someone realizing, is vital. The most immutable blockchain currently running is the bitcoin blockchain. Hundreds of thousands of people maintain copies of the ledger and would notice any change. Furthermore, bitcoin's ledger is also protected by a concept called proof-of-work. Without going too far into the details, know that adding new records requires a substantial amount of computing power. This computation is not a technical requirement for a distributed system, but rather a way for everyone to differentiate the real ledger from potential fake ones without needing to trust any central authority and operate in an open, anonymous network. If presented with several different ledgers, the real one is the one with the most work. You can look at the bitcoin ledger and calculate how many millions of computer hours were needed to generate it. Even a large government would struggle to generate a competing ledger with more work than the bitcoin network, now the world's largest computer. Most private chains do away with this expensive function since internal members are more trusted, but with it goes some of the immutability. Everledger seeks to provide a middle ground where hashes of data on the private chain are stored on bitcoin. In this way, sensitive details are kept secret, but the system is able to leverage the much greater immutability of a public chain, which acts as a kind of notary.

Fine-grained privacy, digitization and automation 

Even within a private chain, there are many cases where data visibility needs to be nuanced. Consider financing and supply chains. Many suppliers must obtain loans to produce goods since payment isn't sent until after delivery. It would be beneficial for those suppliers to show financiers an indisputable record of their past performance including metrics like quality, delivery time, etc. However, those suppliers would certainly not want to reveal customer and performance data to competitors. A blockchain with selective privacy controls could reveal data as needed, as in this case to obtain a loan, but also to government authorities such as customs and tax officials. Now, while this may sound simple on the surface, there are complications. If data is private, how can everyone on the chain verify that data entered has complied with rules? More on this in the next module. The sheer number of participants in the supply chain makes data sharing and the movement of goods complicated. There are governments, importers, shippers, custom brokers, freight forwarders, ocean carriers, and many more. And many of these still require original paperwork, such as bills of lading, to accompany goods through the system. IBM and Maersk did a study of fresh flowers and found that over 200 documents were generated for a single shipment of flowers. A proof of concept system was developed to replace all these documents with a single blockchain-based system where all participants could access digital versions of these documents. For export, three government agencies electronically sign-off on chemical treatments, origin, and duties. Information about these approvals and a truck's departure is visible to the port so they can begin preparing immediately. Delays due to waiting on paperwork were reduced and participants were able to move the shipment faster by acting on data in real time. Now, is blockchain technology needed to replace all this paperwork or just a centralized database? I think a key consideration is the question of trust. Who would run this database and what assurance do all these different actors from export officials and government to shipping companies to end purchasers have that the database operator is faithful to their interest? Blockchain reduces this need for trust across very different parties while also unlocking the benefits of digitization. In addition to reducing the cost and chance of fraud with physical paper, digitization also provides opportunities for automation. Consider shipment insurance for perishable goods. If refrigeration is ever lost along the route of, say, seafood, the entire shipment could be lost. A shipper might have insurance to guard against refrigerator malfunction in cases like these. Before payouts can be made, claims must be investigated. Now, consider a blockchain-based world where information is more reliable. Perhaps a temperature sensor on a truck reports an errant condition directly to the chain and additional third parties verify the state of goods before and after shipment, all writing to the same database visible by the insurance company. It's hoped that by increasing the voracity of information from sensors and multiple parties not only will fraud go down, but systems can be automated. Blockchains can not only store data, but also process data through programs called smart contracts. These are programs that operate on the blockchain with high certainty. Members running a blockchain all run the smart contracts themselves to verify results. A smart contract could automate an insurance payout based on temperature data or arrival time, removing the cost of verifying and processing completely. Smart contracts get more interesting the more connected they are to the real world. One example of this is a smart lock to a warehouse. A smart contract could automatically transfer access to the warehouse and its contents upon payment of an invoice. While the legal framework is still under development, there are even more possibilities if we can tie legal, real-world ownership to records on a blockchain. This is called asset tokenization. Tokens are records of ownership, somewhat like deeds, but are easily transferable on a blockchain. The company Sweetbridge hopes to utilize tokenized assets to allow companies to provide pure-to-pure loans without any banks involved. Loans would be backed by tokenized assets like buildings, equipment, or even accounts receivables, all of which could be controlled by smart contracts that automatically transfer those assets if a loan is defaulted on. The World Bank and Sweetbridge note that 50% of small businesses lack access to financing and on average wait 42 days between invoice and payment. Sweetbridge hopes smart contracts can fulfill this massive, unmet need, but it will likely be some time before the legal world recognizes a blockchain smart contract that truly owns and controls physical property.

A smarter, More Efficient Energy Grid, Peer to Peer Markets 

Let's now move on to another industry that's increasingly becoming more decentralized, energy. As more green energy comes online, such as rooftops, solar panels, and wind generators, energy distribution is becoming more distributed and complicated. There's no longer a top-down model of a single generator and many end consumers, but rather a distributed network where consumers may also be suppliers. The grid is ill-suited for this new configuration, both electrically and financially. Siemens and a startup called LO3 piloted a micro grid in a Brooklyn block where owners of solar panels could directly sell power to neighbors via a blockchain ledger rather than back to the utility. The ledger enables direct pure-to-pure purchasing and also provides a secure record of transactions. There is also interest from local businesses buying green local power and giving those customers special credits. Now certainly the power company could deliver this service on a website, but blockchain removes the need and cost of a third party to stand in the middle. Blockchain can also unlock efficiencies beyond just enabling a pure-to-pure network. Let's look at the financial side of the energy market to understand how. Most people pay for electricity only after they use it, mainly because it would be infeasible to buy electricity one minute at a time. This introduces a risk that someone will use a whole month of electricity and not pay for it. A company called Grid+, made of people that also worked on that Brooklyn project, is making a device that will enable minute-by-minute purchases using the blockchain. Grid+ acts as a local utility, buying power from the wholesale market and selling it to its end customers who have these devices. The smart devices pay for power in near-real time. Grid+'s device gets even more interesting when customers start installing home batteries. The devices could buy power when it's cheap, sell power back to the grid when it's expensive, and even connect to thermostats and other appliances to make further automatizations. Interestingly, Grid+ is not using a private network, but rather the public Ethereum network. We've already mentioned several problems with public blockchains including performance, privacy, and cost, but another is the volatile price of their tokens. You can think of this as a kind of foreign exchange risk. If you enter into any smart contract over a period of time, say a month-long apartment rental, you might end up over paying if the price of a public network token, like Ether, goes up 25%. Grid+ has developed a range of technologies to overcome the public network drawbacks including payment channels to boost performance and reduce transaction fees, a stable token to shield customers from swings in price of Ether, and a physical device to simplify the management of cryptographic keys for the end user. So, with all these needs, why not just use a private network? For Grid+, it's important that their end customers retain what's known as user agency or control over their funds. Running on Ethereum provides a much stronger guarantee of user control. It also eliminates another major category of risk for Grid+ called custodian risk where they hold and control user funds. If the end users have true control over their funds at all times, Grid+ avoids that risk and its inherit regulatory oversight. Immutability and user agency are two aspects that private chains may struggle to port from public ones. These kinds of smarts on the edge of the grid can not only help individual customers save money, but also reduce overall system cost. Utilities must build transformers and power lines to handle the peak loads. Dramatic changes in loads, whether due to air conditioners turning on or clouds blocking solar panels, also cause large cost. Turning on and off powerplants is costly and the bigger the swings, the costlier. If demand can be leveled out by exposing real-time price incentives to end users, there could be substantial savings for the entire system. The additional visibility and secure record of energy production and consumption can enable other goals as well. For example, if you produce energy from renewable sources, your energy could be tagged as green. Other individuals, companies, or government incentive programs could pay a premium for that green energy, somewhat like carbon credits. Now this kind of world depends on smart meters and switches to fairly document energy flows. What's to stop someone from hacking a meter to defraud the system? Luckily blockchain's secure record can provide a clear trail of fraud in this case. There will also likely be providers that audit meters to inject additional trust into the system. Now, blockchain will not solve all problems as the grid becomes more decentralized and smarter. Even if I can buy power directly from my neighbor, somebody still has to pay to maintain the powerline going between our houses and the connection to the larger grid. But programs on blockchain could automatically reserve a portion for utility providers. And to be clear, some of the benefits I mention here are due to digitization, not just blockchain. But blockchain has the potential to help automate the marketplace of money and energy and do it in a cost-effective and secure way. I'll mention one other project that's now up and running and beginning to show the possibilities. Innogy, a subsidiary of RWE, the European energy giant, has recently launched an electric car charging system where consumers can purchase and sell electricity through an Ethereum-based network. The company called Share&Charge interestingly makes no mention of blockchain on its website. It only mentions the convenience of the app and benefits of using its network. Blockchain merely provides the back-end plumbing. This is exciting because it shows the power of blockchain and IoT together. We can even imagine a future where the car itself becomes connected. Imagine a self-driving car that runs its own business collecting fares from riders and using those fares to pay for power at charging stations as needed.

Equities, Finances, Banking, and Payments

Let's now look to the financial world where some of the first experimentation with blockchain for enterprise took place. Just as paper bills of lading are still relied on in the supply chain world, stock issuance in the private equity market is also largely still paper based. In 2015, Nasdaq began testing a product called LINQ that digitizes stock issuance for pre-IPO companies. Rather than sending around paper certificates, stocks are issued digitally and stored on a blockchain-based ledger. This makes the current and historical ownership of shares easily visible. Now one could argue here that blockchain is not needed to digitize stock certificates, but Nasdaq sees this simplified case as an important first example of legally handling stocks on a blockchain. The larger public stock market where there are many intermediaries and regulations can yield much greater benefits. Despite public stocks being digital for some time now, there's still nearly 2 days between a buy or sell action and actual settlement. Settlement means that cash has changed hands and the security record is officially in the name of the new owner. Stocks are now largely stored in central security depositories, but investors do not interact directly with these registries. Investors act through what can be multiple layers of custodians, banks, and intermediaries, especially in international transfers. These intermediary layers handle numerous roles: checking investor identity to comply with anti-terrorism and money laundering laws, making sure those customers have sufficient capital for trades, and netting trades to reduce overall transfers. While these services are necessary, significant capital is tied up in this process. Records get confused and must be reconciled and there is risk of companies going bankrupt during the trades that must be insured against. In one shocking example of what can go wrong, when Bear Stearns collapsed, it was found out that 28% more Bear Stearns shares were on record than had ever been issued by the company according to a report by Euroclear. A single record maintained on the blockchain would eliminate the potential for these reconciliation costs and hopefully bring about tZero, or instant settlement, by giving investors direct control. Directly connecting investors could also bring benefits to other related processes like investor voting and communications. Right now, there is no easy way for a company to reach its investors. Communications must travel through layers of intermediaries. Nasdaq created another pilot in Estonia to address this where companies were able to send votes directly to share owners, and those owners could either vote or easily delegate those votes. Once data is shared on a blockchain, better opportunities for automation also arise. Smart contracts or programs that run on a blockchain could execute automated dividend payments, margin adjustments, and corporate actions. In such a highly regulated environment, this type of transformation will not be without its complications. There will be cases where these smart contract programs face situations that weren't thought of before deploying the code, and human intervention will be needed. Another regulatory complication involves jurisdiction. Currently, the location of the golden record of the stock takes precedence. In a distributed ledger, however, there is no single golden record as the record's ownership is truly distributed. The upcoming right to be forgotten law in the EU may also cause complications as blockchains are designed to be immutable. The regulatory issues are not all negative though. Currently, regulators receive reams of data from firms and must decipher this data in an attempt to ensure the market, as a whole, is not facing systemic risk. Fredrik Voss, VP of Nasdaq blockchain, uses the phrase regulatory goggles as a way to give regulators special access to the entirety of market data on a blockchain. By operating a blockchain node themselves, regulators could get access to data in real time and, importantly, the real data, not reports that must be cross checked. Moving beyond stocks, there are several other areas where financial institutions are exploring blockchain potential. Any organization involved in the financial system is facing more and more stringent customer identification requirements. In the U. S., these laws are known as KYC and AML, or know your customer and anti money laundering laws and require institutions to ensure their customers are not doing anything illegal. This requirement can be expensive and requires substantial data collection from customers, verification, and investigations into the data. Financial institutions spend anywhere from 60 million to 500 million per year to keep up with know your customer and customer due diligence regulations, according to a Thomson Reuters survey. The French bank, Credit Mutuel Arkea, is piloting the use of blockchain to share KYC documents between different divisions. For example, if a customer provided a passport to open a new life insurance policy, that information could immediately be made available to the bank's consumer credit group. The bank is even looking to provide identity as a service to third parties, such as utilities, retailers, and service providers. The State Bank of India has also recently announced a similar initiative to share customer data among banks using blockchain. If more verifiable information becomes available about customers, even on blockchains not run by banks, it could open up new lending markets, providing a credit history to previously unbanked. A blockchain track record of sales could provide sufficient creditworthiness. Similarly, small- and medium-sized businesses may be better able to provide collateral information. I'll end this section with a brief note on payments, perhaps the most obvious area of potential improvements given the ability of bitcoin to transfer millions in minutes across borders. International payments currently go through a system of correspondent, banks, a point-to-point relationship-based system. Once again, blockchains can remove the need for intermediaries. And if we look at the company side of payments, there is substantial reconciliation costs that could be saved. A report by IBM claims its own internal accounts payable group was able to save 60-80% off each invoice cost by utilizing blockchain technology. The report explains that much of the work performed by accounts payable teams results from inconsistent data between suppliers and buyers, and blockchain's single version of the truth eliminates this. The same report says IBM global financing is able to reduce dispute time from 40 days to 10 and free $100 million in capital through the use of blockchain.

More Efficient Patient-centric Healthcare

Healthcare is another area where better data visibility and interoperability between organizations would yield massive benefits. On a personal level, at least in the United States, patients must fill out new paperwork at every new provider, as each provider maintains their own records. When doctors need to share records, fax machines and phone calls are used and oftentimes data is lost or changed in these transfers, leading to expensive reconciliation or patient harm. Siloed date is not only problematic for patients, but also for the larger healthcare ecosystem. Providers, insurers, and governments all must share data in order to approve payments for treatments. In the fragmented U. S. system, billing- and insurance-related payment processing cost over $470 billion in 2014, over 15% of total healthcare spending in that year. Claims can take months to resolve, leaving providers without pay in the duration. Part of the difficulty is due to patient privacy laws surrounding health information. Patient information must be securely maintained and transferred and only released as requested by the patient or for their care. Allow me to paint a picture of a world where patient data is owned and controlled by the patient and not providers. When the patient visits a new doctor, they use a smartphone app to provide the doctor a key that provides limited access to the patient's health record for the time needed. This could include a general health overview and x-rays, but maybe not genetic information or an AIDS test result. When the doctor accesses the information, a record of that access is permanently recorded in a blockchain along with any new test results and analysis done by that doctor. Needing an operation, the patient gives permission for the insurance company to view their record to provide preapproval. Preapproval is given automatically and without any human intervention because the insurer has proof via digital signatures that the notes are from the doctor and that the doctor has the appropriate licenses. After the procedure, payment is also automatically released. The hospital follows up with the patient to make sure the patient is taking necessary drugs and attending physical therapy, after which the insurance company provides a bonus payment for following best practices. The patient automatically receives a notification about a new clinical study for their procedure and elects to submit selective data about their health in return for a payment. That study takes advantage of long-term data about the patient's health and provides more personalized care in the future to another patient with similar characteristics. While this scenario may seem magical to U. S. citizens, many companies are working towards this vision with promising initial projects. Estonian citizens have actually had their health records stored on a blockchain since 2015. A company called Guardtime provides a blockchain where any change to data is recorded, thereby providing extremely durable, auditable health records. Another project is attacking an interesting niche, provider identity. When a provider wants to get a job at a new hospital, the doctor must submit their school, residency program, and information about any state licenses they have. The hospital then needs to verify each piece of information by calling the original sources individually. A company called Hashed Health is setting up a blockchain-based registry in the state of Illinois where all these different groups can submit information once and not have to repeatedly answer verification calls. Hospitals can trust records in the registry because digital signatures guarantee the data was produced by the entity claiming to have produced it. It's an interesting starting place because it doesn't involve sensitive patient data and can begin to show immediate benefits to the healthcare ecosystem. Several other companies are attacking the payment problem head-on with blockchain. A company called Gem has launched a project that reduces claim settlement from 48 days to 5 minutes. They take an interesting approach, never storing actual patient data on the blockchain, but just pointers to that data along with permission policies. All records are linked to a specific patient ID, thereby allowing long-term views of the patient's health. It even provides some amount of automation, sending out explanation of benefits based on preconfigured rules and approval of claims. These examples paint a promising picture of blockchain's potential, even in the extraordinarily regulated and entrenched healthcare industry. Extra visibility into more data should help research and even enable new payment schemes like value or outcome-based payments. Currently, there is little data available about outcomes when patients aren't taking part in explicit surveys. By better managing this data, blockchain may provide markets and incentives to collect this data. This could also include the flood of data from IoT devices like heart rate, glucose, steps, etc., now much of which is never utilized. There are, however, still many questions to answer. Existing health systems will not simply plug into new blockchain networks any more easily than they would to other systems. Integration work and agreement on standards will still be needed. Also, blockchains may be good at preventing data modification and recording who changes or accesses data, but duplicating data across many different nodes, even if encrypted, may increase potential for leaks.

Enterprise, application survey summary 

Blockchain technology provides a single version of the truth collaboratively maintained by untrusting groups. As we've seen, the potential benefits cut across a wide variety of industries. Blockchain eliminates the need for intermediaries to stand between people trading stocks, energy, health data, or money, enabling faster, cheaper, less risky transfers. Increased data visibility empowers regulators in food safety and finance, enables smarter decisions in the supply chain, and also opens new markets from customers looking for environmentally-friendly food or energy to bankers offering loans based on information outside traditional credit scores to health researchers looking for more data. Importantly, that information is only valid if it is indeed true. Blockchain ensures this with irrefutable signatures on data, a storage structure that makes tampering nearly impossible, and strict rules on new data entering the blockchain. Mistakes, fraud, and counterfeits can be drastically reduced. A flood of information is being generated from IoT sensors in our homes, cars, and bodies, and blockchain is poised to help create a market for that data while hopefully enabling people to truly own and control it. Finally, when companies and assets are digitally connected and can trust information, smart contracts on blockchains can automate processes that used to require manual intervention. But all these benefits do not come without significant challenges. Blockchain networks become more valuable as more members participate, but this requires wider-spread agreement on standards and protocols. After agreeing on standards, connections must be made to existing enterprise IT systems. While blockchain promises better insights for regulators from the financial world to the supply chain, regulators need to be involved in blockchain's development if blockchain is to be trusted. There are holes in current laws that simply don't contemplate what it means to have a shared database or to own a tokenized asset. And what happens when mistakes are made? What if a smart contract deletes an asset or someone loses a password? Any blockchain implementation will have to have human-level recovery and legal fallbacks. Finally, while sharing data unlocks numerous benefits, it must be carefully shared not to reveal commercially sensitive financial details to competitors or personal genetic information. The blockchains behind bitcoin and Ethereum display all data to everyone in order to provide trust in a resource that anyone can use. Providing that openness comes at the cost of very slow performance and high fees. Luckily, enterprises generally don't want or need such openness. As we'll see in the next module, a wide range of tools are just now coming out of development that provide blockchain technology with the performance and privacy enterprises need.

Suverying Enterprise Blockchain tools 

Achieving Consensus: Performance, Security or Scale?
Back in 2015, when companies first started investigating enterprise use cases, many simply copied the public bitcoin or Ethereum code base, hoping to make few modifications to fit those systems into enterprise applications. Problems regarding performance and privacy immediately became apparent. Luckily, groups have been working to optimize for the enterprise world, and this module will give an overview of several of the more prominent ones including Hyperledger, Coco, Corda, Quorum, Stratumn, Guardtime, and BigChainDb. This is by no means an exhaustive list, but we'll cover a range of different technical approaches to highlight important tradeoffs in security, privacy, and performance. Along the way, you'll learn how these systems actually work to achieve consensus and privacy. I'll conclude the course with some comments on what are still controversial topics, including private versus private blockchains and whether a private blockchain is really more than a distributed database. Before discussing specific projects, I'd like to provide some background on consensus, the key algorithm behind any distributed system. Consensus is the process by which nodes reach agreement or stay in sync with each other, and choices about how this is done profoundly affect a network's performance and security. Interestingly, the main technical innovation of bitcoin was actually its consensus algorithm. But algorithms used by many modern databases were largely developed in the 1980s and 90s. The benefit that bitcoin's algorithm and blockchain in general bring to existing consensus algorithms is the ability for peers to work on networks with a lower level of trust. Also, in the case of bitcoin, open or permission-less networks where anyone can join at will. Even in networks where nodes trust each other, it's surprisingly hard to keep separate nodes in sync with each other. Information packets can get lost, corrupted, and computer clocks might be different. Adding the possibility of external and internal attacks further increases the challenge. I find it useful to consider a concrete example of why consensus is difficult. What if Alice has $100 in her bank account, but mails 2 checks out for $75 to Bob and Carol at the same time. This is known as the double-spend problem. The first person to cash a check should get paid while the second person's check should get rejected since Alice no longer has sufficient funds. With a single bank, it's easy to check whether Alice has already written a check. But in a distributed network, it's much harder. Information may reach different nodes at different times, and time stamps can't necessarily be relied on. Suppose Bob's check reaches some nodes first and they update their ledgers and Carol's check reaches others first and they update their ledger, not realizing that Alice's account has already spent that money. Alice just turned $100 into $150 in parts of the network. Both of those checks were valid, but agreement on the ordering of them was the critical part. Interestingly, for the network to remain valid, it doesn't even matter which check was considered first, just that all the nodes agree on that order. As long as all the nodes pick an order and agree to it, Alice can't double spend and only one of Bob or Carol will have been paid. In a hostile network, reaching agreement or consensus is known as the Byzantine General's Problem. It's based on the story of an army of generals that must all uniformly decide to attack or retreat, as a split effort would lead to an outmatched defeat. Just as with a distributed computer network, coming to agreement is hard, especially if there are traitors in the mix and messengers that might get corrupted. With that intro into the problem of consensus, I'd like to give you a flavor for some of the solutions. I'm going to stay fairly high-level, but I think it's useful to peer inside the algorithms to some extent to better understand tradeoffs. We've learned that part of getting a distributed group to share a database is to collectively make decisions on transaction order. One way is to simply appoint one leader to decide that order. All transactions are sent to this leader who picks the order and then sends that order back out to all the other nodes. Now this doesn't mean that the leader can write anything they want to the database. Each node will still verify that transactions are valid themselves. For instance, does the transaction sender have sufficient funds and are digital signatures that prove authenticity valid? To help spread power, a new leader might be selected every block. In bitcoin, the leader is decided through a kind of lottery. As transactions are generated by end users, they go into a pending pool. Next, all the nodes operating the bitcoin network select from these pending transactions and begin competing in the lottery that determines which of their selections will get officially confirmed. To compete, each node tries to solve a special math problem by guessing random numbers. It essentially boils down to rolling dice until you get a number that works. The first node that gets a correct result broadcasts their group of transactions. Other nodes check to make sure this result is correct along with the validity of the transactions, and then these get added to the end of the blockchain. The algorithm is called proof-of-work, and because that work costs money in terms of electricity and computers, it's too expensive for any one person to take over the network. While this scheme is an amazing way for a massive group of anonymous people to participate, it's very expensive. Over 10 million U. S. dollars of electricity is burned every day at the time of this course. The Ethereum network currently uses proof-of-work, but plans to switch to proof-of-stake where participants have to put up bonds that can be lost if they misbehave. While proof-of-stake promises a cheaper consensus, there are still open technical questions. While lottery systems provide a powerful way for systems with massive scale to reach consensus, even in hostile networks, they come at another cost, slow finality or settlement, or a long time between when a transaction is created and when it's considered confirmed. It's recommended to wait an hour after sending a bitcoin transaction, which is problematic for in-store purchases or stock trading, but potentially okay for slower supply chain applications. Let's go back to the leader selection process to see why. All nodes are rolling dice until they get a correct number and then broadcast a block. Because nodes are spread all over the world, some nodes might not hear about a new block immediately and build on the previous end of the chain. So you might receive multiple blocks at around the same time, which is known as a fork. The general rule is to build on the longest chain, but this is sometimes ambiguous. Worse, the branch you decide to build on might suddenly become replaced by two new blocks on a different branch. A malicious person with a substantial amount of nodes on a network could take advantage of this to replace a transaction, effectively undoing a payment. This is why it's recommended to wait over an hour until a transaction gets buried further into the chain where it's less likely to change. While bitcoin's algorithm is scalable and open to anyone, transactions are uncertain initially and take a while to become final. In many applications, this kind of delay is unacceptable. Another type of consensus doesn't have this delay and involves a more immediate voting system. Suppose we have a network of 10 nodes. Several nodes publish a proposed list of transactions for the next block, say three different transaction sets, A, B, and C. All the nodes make a preliminary vote, say two for A, four for B, and four for C. Upon seeing the votes, another round of voting occurs where everyone votes for the version that already has the most votes. The two nodes that voted for A might switch to C, making the score four B, six for C. Now there's a clear majority and a final round of voting results in 10 for C. This transaction set is now locked in and final. The downside is that each node had to talk to every other node multiple times and this quickly becomes slower as the number of nodes increases. Even with fast computers, the speed of light is a major limiter for scaling this type of consensus around the globe. So we start to see some properties of consensus algorithms that we must choose between, openness, scale, transaction finality time, and cost. Voting-based algorithms provide fast finality for small numbers, but slow down as the size of the network grows. Voting algorithms are also less open. Another important characteristic is the ability to handle a certain number of hostile or faulty nodes. Such systems are said to be BFT, or Byzantine fault tolerant, in reference to our Byzantine General's Problem. Both of the algorithms I've described, lottery or voting, can be designed to be BFT. Now, most high-performance distributed database systems that power the majority of our web are not BFT, but only crash tolerant, which means they can only handle nodes losing connections occasionally. These systems are also typically closed and only open to a small number of preapproved participants. As we'll see next, a wide range of tools now exist that allow you to select tradeoffs that best match a given application.

Hyperledger: Fabric, Sawtooth, and Composer 

Hyperledger is an organization under the Linux Foundation that sponsors a range of blockchain projects and developer tools. In a positive trend shared amongst almost all the larger enterprise blockchain tools, it's completely opensource. Blockchain connections between companies become more useful the more places they reach, so it makes sense that organizations like Hyperledger promote openness and seek collaboration on standards and design decisions. While IBM contributed most of the initial code, there are now over 100 organizations involved from financial organizations like J. P. Morgan, to technology companies like Intel, to automotives including Daimler, stock exchanges, and, recently, the enterprise software giant SAP. Interestingly, R3 is also a member, another group of financial institutions that have developed their own blockchain solution called Corda that we'll cover later. Hyperledger Fabric is the flagship blockchain project within Hyperledger and recently released a version 1. 0 in July of 2017. It directly takes advantage of the protected environment most enterprise chains will run in, offering a consensus algorithm that's fast and scalable, but not Byzantine fault tolerant, only crash tolerant. The algorithm is based on the Kafka system originally developed at LinkedIn to handle their massive amount of data flowing to and from numerous applications. Another interesting design choice Hyperledger projects make in general is to separate out verification and ordering. Unlike bitcoin where every node verifies every transaction, in Hyperledger, only nodes affected by transactions need to sign off on them. This design choice improves network speed and also makes it easier to plug in different consensus algorithms for the ordering part. And this brings us to some of the other blockchain projects within Hyperledger, Sawtooth, Indy, and Iroha. Sawtooth uses a very special kind of intel processor to achieve many of the properties of the bitcoin network, including scalability and BFT, without the large cost of proof-of-work. The processors have what's known as a TEE, or trusted execution environment. This environment or protected enclave provides a protected place for code to run that can't even be touched by the operating system. The TEEs also digitally sign the results. This means I can give someone that I don't trust code to run and be reasonably sure that any results they send me were not manipulated. In the proof-of-work lottery algorithm in bitcoin, nodes compete to become the next leader by solving special problems. Instead, on a TEE, nodes just generate a random number, wait that amount of time, and broadcast their block. With proof-of-work, everyone could check the results to make sure no one was cheating and taking the leadership role too often. With TEEs, the signatures on the results prove that the random number was fairly generated within the TEE. This provides the fairness of bitcoin without the substantial cost. There is, however, still latency in transaction finality since this is a lottery-based algorithm. Hyperledger Iroha and Indy utilize a voting scheme that provides fast transaction finality, but can't scale to extremely large numbers. As opposed to Fabric, all these systems are Byzantine fault tolerant, which means they can handle malicious or faulty nodes. Hyperledger's separation between verification and ordering provides a solution to the problem of revealing transaction details to only some members of a network. A seller could create a transaction with a buyer and encrypt it before passing it to the rest of the network for ordering consensus, thereby hiding the pricing details to competitors. Because the transaction only involves that seller and buyer, as long as both of them sign off on it the rest of the network considers it valid without needing to see the details. Later on, the seller may wish to prove her track record to a bank so she might share the encryption key with banks only. I want to finish talking about Hyperledger with a note on some powerful developer tools also under production. Anyone looking into incorporating blockchain will need to experiment and also integrate blockchain technology with their other IT systems. The cost of both of these can be substantial, but Hyperledger has released tools to speed up these processes. Hyperledger Composer enables developers to configure blockchain networks using familiar JavaScript to define participants, process rules, and integrations with other systems. I'll briefly describe one of their sample applications available at composer-playground. mybluemix. net. It represents a vehicle lifecycle system. The participants include individual car buyers, manufacturers, regulators, insurers, police, and standards testing companies. This is a great blockchain application because it represents a case of completely separate organizations that all must share data. Now, blockchain doesn't remove the need for every participant to communicate with agreed-upon standards, but it does remove the reconciliation process that would inevitably be necessary when separate organizations maintain records. By having one view of the truth replicated on a node at each organization, information is kept more up-to-date and accurate. Here we can see the blockchain playground portion where you can set up the types of participants, the rules for how data gets added, including which participants need to sign off on various transactions, and also permissions. In addition to the blockchain portion, their demo provides some examples of how you might connect the blockchain application to other applications. For example, there's a mobile app that a buyer uses to select car color and then purchase and a web application dashboard that the regulator uses to monitor the vehicle registry. It has a suspect vehicle report showing a vehicle that was written-off by an insurance company, but then registered to a new buyer, a possible case of insurance fraud. These applications connect to nodes running the blockchain application to create transactions and also query data for analysis. Any blockchain application will need to similarly integrate with existing ERP and authentication systems and the outside world. Overall, Hyperledger provides a powerful toolset that has been developed with significant consideration towards helping developers get started and integrate with the existing IT world.

Microsoft coco Framework 

Microsoft has recently announced an enterprise-focused blockchain framework that promises performance approaching standard databases along with substantial privacy enhancements. It also takes advantage of the trusted execution environments, or TEEs, on special processors like Intel's SGX line. We mentioned these previously as a way to pick the leader in the Hyperledger Sawtooth project. They enable you to trust the output of code running on machines that you don't trust. Coco takes advantage of this property by running a majority of the blockchain system within these environments. If we can trust that the code running on all the machines hasn't been tampered with, consensus algorithms that don't protect against malicious participants can be used. Another benefit is that non-deterministic functions can be run. We saw this with the Hyperledger Sawtooth project where a random wait time was calculated for each node and nodes would broadcast a block after their generated wait time. Everyone can trust that the wait times were generated fairly by looking at the digital signatures generated within the TEEs. In contrast, on the public Ethereum network, smart contracts cannot rely on random number generation. Every node runs every smart contract to check results for itself, therefore code must be written so that the results will be repeatable by anyone that runs it or in other words, deterministic. TEEs enable you to trust that someone else generated a fair, random number. In addition to faster consensus and the ability to use random numbers, TEEs provide compelling privacy advantages. Consider our example of the seller not wanting to reveal pricing data to competitors. All transactions remain encrypted unless inside the TEE where even the owner of the code cannot see the decrypted data. All nodes can run verification code themselves on the raw data, but never see it. This also opens up the possibility for even finer-grain privacy rules. Before, we mentioned that the seller might want to prove her track record with banks to get a loan. While she could give banks full access to all her transactions, she might also only want to give access to quantity information and not pricing. With a TEE, the program can decide exactly which data is delivered across the protected zone to the host system. This kind of fine-grain control is something that enterprises already have in their existing systems and will expect to have. It opens the door to many use cases, such as documenting practices that help the seller market himself. For example, proving that the seller pays a good wage to its workers without revealing who the workers are. One final feature of the Coco framework is its first-level support for system governance. By governance, I mean deciding things like which members can access the network and what version of code to run. Coco outlines two levels of nodes: members and participants. Members can vote on participants and code versions while participants can only operate on the network, not change it. Members also maintain portions of a master decryption key. If a major problem happens on the chain, they can combine their keys to decrypt the entire database outside of any TEE. Interestingly, Coco is not a complete solution, but rather an underlying framework to which you would need to bring your own smart contract code, which could be a ledger like Ethereum or Corda.

R3 Corda Blockchain

Corda is another opensource blockchain project that focuses on privacy. Originally targeting financial services, Corda is a product of R3, a firm with investment from over 100 banks and technology companies. In May of 2017, a second round of $107 million U. S. was added with investment from SBI Group, Bank of America, Merrill Lynch, HSBC, Intel, and Temasek. A 1. 0 version was released on October 3, 2017. Corda began designing their framework with specific financial use cases like stock and derivative contracts. A typical stock example is a payment versus delivery contract for a security where Bank A would agree to deliver a security in return for a cash payment in three days. An example derivative contract is an interest rate swap. Suppose Bank A has a given fixed rate loan for 1. 5% and Bank B provides a loan for a floating rate. To avoid the risk that payments would go down if the interest rate falls, Bank B could enter a contract with Bank A to swap floating payments for fixed ones. These two examples give us some insight into one of Corda's staring assumptions, most contract information only needs to be shared between the participating banks. Unlike many of the other tools, transaction information is not shared with the wider network, even in encrypted form. There is no global ledger to which all transactions are broadcast and added. Rather, each node holds only the data relevant to transactions in which they are participating. Now there still needs to be a way for nodes to verify information about transactions. What if Bank A tries to send an asset to both B and C, how would they know it wasn't trying to double spend? Corda provides a notary service in this case. The notary can act on the full details of a transaction, or where privacy is more important, just on a hash or fingerprint of that transaction. The stronger privacy case does open the possibility for some attacks that would break the network, however. Like many other systems, the consensus algorithm is pluggable, so different users can decide what tradeoffs between security and performance they want. Like Hyperledger, Corda also seeks to provide developers with familiar tools to write smart contracts in. Corda uses a Java virtual machine to execute contracts, a very familiar environment for enterprise developers.

JP Morgan Quorum: Enterprise Ethereum

Quorum, developed by J. P. Morgan Chase, is another blockchain initially targeting the financial world. It seeks a similar set of goals as Hyperledger, Corda, and Coco, high performance with privacy rules tailored for an enterprise world. Quorum initially targeted financial problems like long settlement times and providing regulators with a transparent view of risk inside institutions. Unlike the systems I've discussed already, which are largely built from scratch, Quorum is designed to be as close to the public Ethereum network as possible, pulling in updates as public Ethereum evolves. It differs in two main ways from public Ethereum. Quorum supports private transactions and a voting-based consensus mechanism rather than Ethereum's proof-of-work scheme. Quorum does still support public transactions and smart contract execution, but it adds a new class of private transactions and corresponding considerations for consensus on those private transactions. For a private transaction, data is encrypted so that only the nodes party to that transaction can decrypt it and run the internal smart contracts. Other nodes on the network receive a hash of the transactions, but cannot run it themselves to verify it. In this manner, nodes will end up with two ledgers: a public one that is generated by running all public transactions and a private one that's a result of private transactions. Quorum differs from Corda in that all nodes record a hash or fingerprint of every transaction, whereas in Corda, only notaries will be aware of private transactions. The Quorum method ensures a wide-spread, tamper-proof record that transactions occurred even if the contents are hidden. This concept of only processing and storing relevant transactions is a form of transaction sharding, a technique in the database world where the larger database is stored in pieces rather than fully replicated in every node. It intuitively makes sense that nodes that aren't part of a transaction shouldn't need to expend resources to check it. Hyperledger Fabric also offers a form of this by separating the validation of transactions from the ordering consensus. Only nodes that are part of a transaction need to run and verify it. Because Quorum is closely based on Ethereum, it naturally comes with a built-in token, whereas the other systems do not have native tokens. Quorum also benefits from technologies and development tools being developed for public Ethereum.

Gaurdtime and Stratumn

I want to briefly mention these two companies because of their prominence in the space and their focus on the use of the blockchain as a powerful notary service. Guardtime is one of the oldest and largest enterprise blockchain-focused companies siting over 150 cryptographers on its staff and having been founding before bitcoin was introduced to the world. They have an impressive track record of working on many large projects including storing the health records of all the citizens of Estonia and recently a new project for the U. S. department of energy to help secure the energy grid by detecting attacks. Their keyless signature infrastructure technology stores timestamped hashes of the state of a system in a blockchain, notarizing a clean state. In this way, any unauthorized changes can be quickly and precisely detected, thereby providing a hacker detection system. Guardtime notes that hacks are only a matter of when for most systems, so their recommendation is to set up systems that can detect and respond to attacks quickly. Their systems only support a small number of nodes to provide BFT at the same time as high performance and fast transaction finality. Stratumn provides proofs of process, similarly based on hash chains that prove a record of who, what, when, where, and why a process happened. This can be used to prove that an agreement was carried through with a partner or to prove compliance with regulators. Interestingly, both Guardtime and Stratumn systems provide a kind of proof that doesn't reveal any private data. Hashes of data are unique to that data, but don't provide any way to go back to it.

conclusion: public blockchain vs private vs database

While there are many, many more private blockchain projects focusing on enterprise, I hope this selection gives you a good idea of the different ways groups are approaching blockchain. We see a surprising amount of commonality: open source projects, development tools that are friendly to enterprise developers, high performance requirements, and an emphasis on privacy. This not only includes strict controls on who can use the system, but also per-transaction privacy rules and ideally data within those transactions. We see there are many tradeoffs to be made on the consensus side, the key algorithm that enables distributed nodes to agree on one version of the truth. Smaller networks are easy to make fast and secure enough to handle some faulty or malicious nodes, but as we scale to larger networks, tradeoffs have to be made in terms of transaction settlement or finality and security. Interestingly, many of the projects provide the ability to swap out different consensus mechanisms so you can make these tradeoff decisions without switching the overall framework. Another key aspect to deploying blockchain is governance, and by this, I mean a system for deciding on the standards, code, and access. This is something that any network will need to perform on an on-going basis. More recent frameworks like Coco have governance voting systems built in from the start. I want to conclude with some thoughts on the still controversial differences between public blockchains, private ones, and databases. Initially, companies tried to use bitcoin or Ethereum code out of the box to bring all the promises of blockchain to the enterprise world. As we've seen throughout this course, these include better access to more reliable data, removing intermediaries, and automation across company borders. All of these hopefully yielding higher efficiency, reduce fraud and reconciliation cost, and new markets. But performance and privacy concerns were immediate roadblocks. Bitcoin and Ethereum can only handle a few transactions per second and broadcast every transaction to the entire internet. So enterprise developers have created their own systems, trading openness and sometimes scale for performance and privacy. But many people argue that by making blockchain systems private, you're fundamentally removing the value provided by public blockchains and are really just putting fancy clothing on existing distributed databases. Now, many of the projects discussed in this course have just been released to the world, so this debate will soon benefit from some real outcome data. But I do think there's some merit in the naysayer's arguments that's worth keeping in mind. In many respects, private blockchains are indeed very close to existing distributed databases. Hyperledger Fabric achieves its high performance by building on top of one of those, the Kafka system. Another company, BigChainDb, made similar design decisions. Rather than try to get a blockchain to perform as fast as a database, they started with a high-performance distributed database and added a blockchain layer on top of it. This blockchain layer requires digital signatures and adds hash chain connections to new data, thereby providing a more immutable and trustworthy record of data than the database would by itself. But there are some aspects of public blockchains that private ones don't share. We mentioned immutability before with the Everledger diamond-tracking example. While keeping full transaction details on a private chain, they stored hashes of those transactions on the public bitcoin network as a way to guarantee those transactions never change. It would be very difficult for a private group of companies to prove to an outsider that data had never changed. This may be even more relevant to regulators who may need to run a node themselves to be assured of data immutability. Another aspect from the public blockchain that enterprises have left behind is a token and currency. Many of the enterprise blockchain projects use this as a bragging point to help separate themselves from the criminal uses of bitcoin, such as tax evasion and illegal drug buying. But the outlook on tokens is beginning to change. Many companies are crowd sourcing funds and ICOs, or initial coin offerings, which provide a new way to connect with hybrid investor customers. In Ethereum, tokens provide powerful incentives to maintain good behavior. In a small network of 5-10 entities, the legal system might be sufficient to maintain good behavior, but what about an enterprise network on the supply chain with tens of thousands of participants? Tokens provide a way to easily exchange value that's also programable. A seller could require a buyer to post a deposit in a smart contract as a form of trustless escrow. In some sense, tokens are already being used by enterprises for things like airline miles or loyalty points or coffee stars. And as we saw with the Grid+ electricity buying network, tokens can provide a way for companies to reduce custodian risk, giving their users true agency or control. But a user's control depends on the immutability of the network. A single airline can change the formula for how many points are needed to buy a flight, but Grid+ cannot simply add new Ether to its devices without paying for it. Now I think privacy and performance considerations will outweigh the immutability and token benefits of public networks in the near term, but it's likely public networks will address these issues in the future. There are many technologies under development in public networks that will speed performance. Many of these work by enabling nodes outside of the network to do most of the processing, but providing systems that still ensure that work is valid. We saw some examples of this in Hyperledger by only requiring nodes involved in a transaction to process it. For cases where the entire network does need to check a transaction, check out lightning networks, payment channels, and a company called TrueBit. TrueBit is making a game-like checking system where transactions are executed by only a few nodes, but checked by others that get a reward if they find a problem. And on the privacy front, new encryption technology, like homomorphic encryption and zero knowledge proofs provide a way for nodes to check transactions without seeing the raw data. Right now, these methods are limited, but the technology is evolving rapidly. I hope this course has shown you that the characteristics of blockchains are not black and white, but rather sliding scales of performance, security, and privacy. Private blockchains may very well go the way of private intranets in the 1990s, but I believe that is still some time off. I'd like to make a final note on tools making it easier to get started. Blockchain is as new as the internet was when people were still debating whether customers would ever buy things online. So what it is and where it fits are evolving rapidly. Luckily, it's getting easier to test out blockchain to find out where it will shine. Both Microsoft and IBM provide blockchain as a service offering where you can stand up new blockchains in a single click on their cloud. I showed some of these tools when talking about Hyperledger before, but here is a listing of Microsoft's options, which include pure Ethereum, Quorum, Hyperledger, and many more. Once you test out your business case, you can run the code in their clouds or on-premise if you don't want to trust a cloud operator. Another great first step is to join a group like Hyperledger or the Enterprise Ethereum Alliance. It's a rapidly moving world. I encourage you to jump aboard to help shape blockchain to fit your business needs. I want to thank the Epicenter podcast for providing immeasurably helpful information on numerous enterprise blockchain projects.


break and test 

set up experiement and outline base archetechture 


linear and probabilistic process 


Code from Richard 100% rate 

import torch
import torch.nn as nn
import torch.nn.functional as F

# Base-LCM Architecture Components
class PreNet(nn.Module):
    """
    Maps input embeddings to the model's hidden dimension after normalization.
    """
    def __init__(self, input_dim, hidden_dim):
        super(PreNet, self).__init__()
        self.linear = nn.Linear(input_dim, hidden_dim)
        self.scaler_mean = 0.0  # Placeholder for robust scaler mean
        self.scaler_std = 1.0   # Placeholder for robust scaler std

    def normalize(self, x):
        return (x - self.scaler_mean) / self.scaler_std

    def forward(self, x):
        x = self.normalize(x)
        x = self.linear(x)
        return x

class PostNet(nn.Module):
    """
    Maps hidden state outputs back to the embedding space with denormalization.
    """
    def __init__(self, hidden_dim, output_dim):
        super(PostNet, self).__init__()
        self.linear = nn.Linear(hidden_dim, output_dim)
        self.scaler_mean = 0.0  # Placeholder for robust scaler mean
        self.scaler_std = 1.0   # Placeholder for robust scaler std

    def denormalize(self, x):
        return x * self.scaler_std + self.scaler_mean

    def forward(self, x):
        x = self.linear(x)
        x = self.denormalize(x)
        return x

class TransformerDecoder(nn.Module):
    """
    Standard Decoder-Only Transformer.
    """
    def __init__(self, hidden_dim, num_heads, num_layers, ff_dim, dropout=0.1):
        super(TransformerDecoder, self).__init__()
        self.layers = nn.ModuleList([
            nn.TransformerDecoderLayer(
                d_model=hidden_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout
            )
            for _ in range(num_layers)
        ])
        self.pos_encoder = nn.Parameter(torch.zeros(1, 512, hidden_dim))  # Positional encoding

    def forward(self, x):
        seq_len = x.size(1)
        x = x + self.pos_encoder[:, :seq_len]
        for layer in self.layers:
            x = layer(x, x)  # Self-attention in decoder layers
        return x

class BaseLCM(nn.Module):
    """
    Base Large Concept Model (LCM):
    - PreNet: Maps input embeddings to hidden space.
    - TransformerDecoder: Autoregressively processes embeddings.
    - PostNet: Maps output back to the embedding space.
    """
    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim):
        super(BaseLCM, self).__init__()
        self.prenet = PreNet(input_dim, hidden_dim)
        self.transformer_decoder = TransformerDecoder(hidden_dim, num_heads, num_layers, ff_dim)
        self.postnet = PostNet(hidden_dim, output_dim)

    def forward(self, x):
        x = self.prenet(x)
        x = self.transformer_decoder(x)
        x = self.postnet(x)
        return x

# Testing the Base-LCM architecture
def test_base_lcm():
    batch_size = 4
    sequence_length = 10
    input_dim = 256  # SONAR embedding dimension (e.g., pre-encoded sentences)
    hidden_dim = 512
    num_heads = 8
    num_layers = 6
    ff_dim = 2048
    output_dim = 256  # Output embedding dimension (same as input)

    # Random input to simulate SONAR embeddings
    input_embeddings = torch.randn(batch_size, sequence_length, input_dim)

    # Initialize and test Base-LCM
    model = BaseLCM(input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim)
    output_embeddings = model(input_embeddings)

    print("Input shape:", input_embeddings.shape)
    print("Output shape:", output_embeddings.shape)

if __name__ == "__main__":
    test_base_lcm()

!pip install geoopt

Collecting geoopt
  Downloading geoopt-0.5.0-py3-none-any.whl.metadata (6.7 kB)
Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from geoopt) (2.5.1+cu121)
Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.26.4)
Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.13.1)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.16.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (4.12.2)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.4.2)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.1.4)
Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (2024.10.0)
Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.0->geoopt) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->geoopt) (3.0.2)
Downloading geoopt-0.5.0-py3-none-any.whl (90 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.1/90.1 kB 7.1 MB/s eta 0:00:00
Installing collected packages: geoopt
Successfully installed geoopt-0.5.0

import torch
import torch.nn as nn
import torch.nn.functional as F
from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings
from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer

# Base-LCM Architecture Components with Hyperbolic Space
class PreNet(nn.Module):
    """
    Maps input embeddings to the model's hidden dimension in hyperbolic space.
    """
    def __init__(self, input_dim, hidden_dim, manifold):
        super(PreNet, self).__init__()
        self.linear = nn.Linear(input_dim, hidden_dim)
        self.manifold = manifold
        self.hidden_dim = hidden_dim

    def forward(self, x):
        x = self.linear(x)
        x = self.manifold.expmap0(x)  # Map to hyperbolic space (Poincare Ball)
        return x

class PostNet(nn.Module):
    """
    Maps hidden state outputs back to the embedding space from hyperbolic space.
    """
    def __init__(self, hidden_dim, output_dim, manifold):
        super(PostNet, self).__init__()
        self.linear = nn.Linear(hidden_dim, output_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.manifold.logmap0(x)  # Map back to Euclidean space
        x = self.linear(x)
        return x

class TransformerDecoder(nn.Module):
    """
    Standard Decoder-Only Transformer operating in hyperbolic space.
    """
    def __init__(self, hidden_dim, num_heads, num_layers, ff_dim, manifold, dropout=0.1):
        super(TransformerDecoder, self).__init__()
        self.layers = nn.ModuleList([
            nn.TransformerDecoderLayer(
                d_model=hidden_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout
            )
            for _ in range(num_layers)
        ])
        self.manifold = manifold
        self.pos_encoder = ManifoldParameter(torch.zeros(1, 512, hidden_dim), manifold=manifold)

    def forward(self, x):
        seq_len = x.size(1)
        x = self.manifold.expmap0(x + self.pos_encoder[:,:seq_len])  # Ensure curvature is retained
        for layer in self.layers:
            x = layer(x, x)  # Self-attention in decoder layers
        return x

class HyperbolicLCM(nn.Module):
    """
    Base Large Concept Model (LCM) with Hyperbolic Hidden Space.
    - PreNet: Maps input embeddings to hyperbolic space.
    - TransformerDecoder: Operates in hyperbolic space.
    - PostNet: Maps back to Euclidean space.
    """
    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim, manifold):
        super(HyperbolicLCM, self).__init__()
        self.manifold = manifold
        self.prenet = PreNet(input_dim, hidden_dim, manifold)
        self.transformer_decoder = TransformerDecoder(hidden_dim, num_heads, num_layers, ff_dim, manifold)
        self.postnet = PostNet(hidden_dim, output_dim, manifold)

    def forward(self, x):
        x = self.prenet(x)
        x = self.transformer_decoder(x)
        x = self.postnet(x)
        return x

# Cosine Similarity for Accuracy
def compute_accuracy(predicted, target, threshold=0.5):
    cos_sim = F.cosine_similarity(predicted, target, dim=-1)
    correct = (cos_sim > threshold).float()
    accuracy = correct.mean().item()
    return accuracy

# Adding noise to target embeddings
def add_noise_to_embeddings(embeddings, noise_level=0.1):
    noise = torch.randn_like(embeddings) * noise_level
    return embeddings + noise

# Testing the Hyperbolic-LCM Architecture
def test_hyperbolic_lcm():
    batch_size = 4
    sequence_length = 10
    input_dim = 256  # Input embedding dimension
    hidden_dim = 512  # Hidden dimension in hyperbolic space
    num_heads = 8
    num_layers = 6
    ff_dim = 2048
    output_dim = 256  # Output embedding dimension
    epochs = 5  # Number of epochs for training
    noise_level = 0.05  # Noise level for targets

    # Initialize the Poincare Ball Manifold
    manifold = PoincareBall(c=1.0)  # Curvature = 1.0

    # Random input to simulate embeddings
    input_embeddings = torch.randn(batch_size, sequence_length, input_dim)

    # Initialize the Hyperbolic-LCM Model
    model = HyperbolicLCM(input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim, manifold)

    # Define the Riemannian Adam optimizer
    optimizer = RiemannianAdam(model.parameters(), lr=1e-3)
    criterion = nn.MSELoss()

    # Create Target Embeddings with Noise
    target_embeddings = add_noise_to_embeddings(input_embeddings, noise_level=noise_level)

    # Training Loop for Multiple Epochs
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        output_embeddings = model(input_embeddings)
        loss = criterion(output_embeddings, target_embeddings)
        loss.backward()
        optimizer.step()

        # Compute Accuracy
        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold=0.2)

        print(f"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Accuracy: {accuracy * 100:.2f}%")

if __name__ == "__main__":
    test_hyperbolic_lcm()

Epoch 1/5 | Loss: 1.0234 | Accuracy: 0.00%
Epoch 2/5 | Loss: 0.9577 | Accuracy: 50.00%
Epoch 3/5 | Loss: 0.8828 | Accuracy: 100.00%
Epoch 4/5 | Loss: 0.9442 | Accuracy: 60.00%
Epoch 5/5 | Loss: 0.8602 | Accuracy: 100.00%

!pip install torchtext
Collecting torchtext
  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)
Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)
Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.1+cu121)
Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)
Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)
Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)
Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)
Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 23.2 MB/s eta 0:00:00
Installing collected packages: torchtext
Successfully installed torchtext-0.18.0

Collecting torchtext
  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)
Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)
Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.1+cu121)
Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)
Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)
Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)
Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)
Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 23.2 MB/s eta 0:00:00
Installing collected packages: torchtext
Successfully installed torchtext-0.18.0

Found existing installation: torchtext 0.18.0
Uninstalling torchtext-0.18.0:
  Successfully uninstalled torchtext-0.18.0
Collecting torchtext
  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)
Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)
Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.1+cu121)
Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)
Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)
Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)
Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)
Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 30.4 MB/s eta 0:00:00
Installing collected packages: torchtext
Successfully installed torchtext-0.18.0

import torch
import torch.nn as nn
import torch.nn.functional as F
from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings
from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer

# Base-LCM Architecture Components with Hyperbolic Space and Pyramid Structure
class PyramidLayer(nn.Module):
    """
    Represents one pyramid layer: compresses dimensionality in hyperbolic space.
    """
    def __init__(self, input_dim, output_dim, manifold):
        super(PyramidLayer, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.manifold.expmap0(self.linear(x))  # Map to hyperbolic space with compression
        return x

class HyperbolicCube(nn.Module):
    """
    Hyperbolic Cube: Multiple pyramid layers forming a cube-like structure.
    """
    def __init__(self, layers_dims, manifold):
        super(HyperbolicCube, self).__init__()
        self.manifold = manifold
        self.pyramid_layers = nn.ModuleList([
            PyramidLayer(layers_dims[i], layers_dims[i+1], manifold)
            for i in range(len(layers_dims) - 1)
        ])

    def forward(self, x):
        for layer in self.pyramid_layers:
            x = layer(x)
        return x

class PreNet(nn.Module):
    """
    Maps input embeddings to the hidden dimension.
    """
    def __init__(self, input_dim, hidden_dim, manifold):
        super(PreNet, self).__init__()
        self.linear = nn.Linear(input_dim, hidden_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.linear(x)
        x = self.manifold.expmap0(x)
        return x

class PostNet(nn.Module):
    """
    Maps output back to the embedding space.
    """
    def __init__(self, hidden_dim, output_dim, manifold):
        super(PostNet, self).__init__()
        self.linear = nn.Linear(hidden_dim, output_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.manifold.logmap0(x)
        x = self.linear(x)
        return x

class HyperbolicLCM(nn.Module):
    """
    LCM with a Hyperbolic Cube as the hidden space.
    """
    def __init__(self, input_dim, hidden_dims, num_heads, num_layers, ff_dim, output_dim, manifold):
        super(HyperbolicLCM, self).__init__()
        self.manifold = manifold
        self.prenet = PreNet(input_dim, hidden_dims[0], manifold)
        self.hyperbolic_cube = HyperbolicCube(hidden_dims, manifold)
        self.postnet = PostNet(hidden_dims[-1], output_dim, manifold)

    def forward(self, x):
        x = self.prenet(x)
        x = self.hyperbolic_cube(x)
        x = self.postnet(x)
        return x

# Cosine Similarity for Accuracy
def compute_accuracy(predicted, target, threshold=0.1):
    cos_sim = F.cosine_similarity(predicted, target, dim=-1)
    correct = (cos_sim > threshold).float()
    accuracy = correct.mean().item()
    return accuracy

# Load GloVe Embeddings Manually
def load_glove_embeddings(file_path, vocab_size=5000):
    """Load GloVe embeddings from a file for a small subset."""
    embeddings = {}
    with open(file_path, 'r') as f:
        for i, line in enumerate(f):
            if i >= vocab_size:
                break
            values = line.split()
            word = values[0]
            vector = torch.tensor([float(x) for x in values[1:]], dtype=torch.float)
            embeddings[word] = vector
    return embeddings

# Prepare Input Embeddings
def prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=300):
    """Randomly sample embeddings from the loaded GloVe vectors."""
    selected_vectors = torch.stack(
        [glove_embeddings[word] for word in list(glove_embeddings.keys())[:sequence_length]]
    )
    input_embeddings = selected_vectors.unsqueeze(0).repeat(batch_size, 1, 1)
    return input_embeddings

# Testing Hyperbolic-LCM Architecture
def test_hyperbolic_lcm():
    batch_size = 4
    sequence_length = 10
    input_dim = 300  # GloVe embedding dimension
    hidden_dims = [512, 256, 128, 64]  # Pyramid structure dimensions
    output_dim = 300
    epochs = 25
    threshold = 0.1  # Cosine similarity threshold (lowered)
    glove_file = "glove.6B.300d.txt"  # Path to GloVe embeddings file

    # Initialize the Poincare Ball Manifold
    manifold = PoincareBall(c=1.0)

    # Load GloVe Embeddings
    glove_embeddings = load_glove_embeddings(glove_file, vocab_size=100)
    input_embeddings = prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=input_dim)

    # Initialize Hyperbolic-LCM Model
    model = HyperbolicLCM(input_dim, hidden_dims, num_heads=8, num_layers=6, ff_dim=2048, output_dim=output_dim, manifold=manifold)
    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)  # Lowered learning rate
    criterion = nn.MSELoss()

    # Create slightly perturbed target embeddings
    target_embeddings = input_embeddings + torch.randn_like(input_embeddings) * 0.01  # Reduced noise level

    # Training Loop
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        output_embeddings = model(input_embeddings)
        loss = criterion(output_embeddings, target_embeddings)
        loss.backward()
        optimizer.step()

        # Compute Accuracy
        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold)
        print(f"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Accuracy: {accuracy * 100:.2f}%")

if __name__ == "__main__":
    test_hyperbolic_lcm()
Epoch 1/25 | Loss: 0.1043 | Accuracy: 15.00%
Epoch 2/25 | Loss: 0.1039 | Accuracy: 20.00%
Epoch 3/25 | Loss: 0.1036 | Accuracy: 25.00%
Epoch 4/25 | Loss: 0.1032 | Accuracy: 30.00%
Epoch 5/25 | Loss: 0.1028 | Accuracy: 30.00%
Epoch 6/25 | Loss: 0.1025 | Accuracy: 30.00%
Epoch 7/25 | Loss: 0.1021 | Accuracy: 30.00%
Epoch 8/25 | Loss: 0.1018 | Accuracy: 40.00%
Epoch 9/25 | Loss: 0.1014 | Accuracy: 40.00%
Epoch 10/25 | Loss: 0.1011 | Accuracy: 42.50%
Epoch 11/25 | Loss: 0.1007 | Accuracy: 60.00%
Epoch 12/25 | Loss: 0.1004 | Accuracy: 67.50%
Epoch 13/25 | Loss: 0.1001 | Accuracy: 97.50%
Epoch 14/25 | Loss: 0.0998 | Accuracy: 100.00%
Epoch 15/25 | Loss: 0.0995 | Accuracy: 100.00%
Epoch 16/25 | Loss: 0.0992 | Accuracy: 100.00%
Epoch 17/25 | Loss: 0.0988 | Accuracy: 100.00%
Epoch 18/25 | Loss: 0.0985 | Accuracy: 100.00%
Epoch 19/25 | Loss: 0.0982 | Accuracy: 100.00%
Epoch 20/25 | Loss: 0.0979 | Accuracy: 100.00%
Epoch 21/25 | Loss: 0.0976 | Accuracy: 100.00%
Epoch 22/25 | Loss: 0.0973 | Accuracy: 100.00%
Epoch 23/25 | Loss: 0.0970 | Accuracy: 100.00%
Epoch 24/25 | Loss: 0.0967 | Accuracy: 100.00%
Epoch 25/25 | Loss: 0.0965 | Accuracy: 100.00%

import torch
import torch.nn as nn
import torch.nn.functional as F
from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings
from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer

# Base-LCM Architecture Components with Hyperbolic Space and Pyramid Structure
class PyramidLayer(nn.Module):
    """
    Represents one pyramid layer: compresses dimensionality in hyperbolic space.
    """
    def __init__(self, input_dim, output_dim, manifold):
        super(PyramidLayer, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.manifold.expmap0(self.linear(x))  # Map to hyperbolic space with compression
        return x

class HyperbolicCube(nn.Module):
    """
    Hyperbolic Cube: Multiple pyramid layers forming a cube-like structure.
    """
    def __init__(self, layers_dims, manifold):
        super(HyperbolicCube, self).__init__()
        self.manifold = manifold
        self.pyramid_layers = nn.ModuleList([
            PyramidLayer(layers_dims[i], layers_dims[i+1], manifold)
            for i in range(len(layers_dims) - 1)
        ])

    def forward(self, x):
        for layer in self.pyramid_layers:
            x = layer(x)
        return x

class PreNet(nn.Module):
    """
    Maps input embeddings to the hidden dimension.
    """
    def __init__(self, input_dim, hidden_dim, manifold):
        super(PreNet, self).__init__()
        self.linear = nn.Linear(input_dim, hidden_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.linear(x)
        x = self.manifold.expmap0(x)
        return x

class PostNet(nn.Module):
    """
    Maps output back to the embedding space.
    """
    def __init__(self, hidden_dim, output_dim, manifold):
        super(PostNet, self).__init__()
        self.linear = nn.Linear(hidden_dim, output_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.manifold.logmap0(x)
        x = self.linear(x)
        return x

class HyperbolicLCM(nn.Module):
    """
    LCM with a Hyperbolic Cube as the hidden space.
    """
    def __init__(self, input_dim, hidden_dims, num_heads, num_layers, ff_dim, output_dim):
        super(HyperbolicLCM, self).__init__()
        self.curvature = nn.Parameter(torch.tensor(1.0, requires_grad=True))  # Learnable curvature
        self.manifold = PoincareBall(c=self.curvature)
        self.prenet = PreNet(input_dim, hidden_dims[0], self.manifold)
        self.hyperbolic_cube = HyperbolicCube(hidden_dims, self.manifold)
        self.postnet = PostNet(hidden_dims[-1], output_dim, self.manifold)

    def forward(self, x):
        x = self.prenet(x)
        x = self.hyperbolic_cube(x)
        x = self.postnet(x)
        return x

# Cosine Similarity for Accuracy
def compute_accuracy(predicted, target, threshold=0.1):
    cos_sim = F.cosine_similarity(predicted, target, dim=-1)
    correct = (cos_sim > threshold).float()
    accuracy = correct.mean().item()
    return accuracy

# Load GloVe Embeddings Manually
def load_glove_embeddings(file_path, vocab_size=5000):
    """Load GloVe embeddings from a file for a small subset."""
    embeddings = {}
    with open(file_path, 'r') as f:
        for i, line in enumerate(f):
            if i >= vocab_size:
                break
            values = line.split()
            word = values[0]
            vector = torch.tensor([float(x) for x in values[1:]], dtype=torch.float)
            embeddings[word] = vector
    return embeddings

# Prepare Input Embeddings
def prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=300):
    """Randomly sample embeddings from the loaded GloVe vectors."""
    selected_vectors = torch.stack(
        [glove_embeddings[word] for word in list(glove_embeddings.keys())[:sequence_length]]
    )
    input_embeddings = selected_vectors.unsqueeze(0).repeat(batch_size, 1, 1)
    return input_embeddings

# Testing Hyperbolic-LCM Architecture
def test_hyperbolic_lcm():
    batch_size = 4
    sequence_length = 10
    input_dim = 300  # GloVe embedding dimension
    hidden_dims = [512, 256, 128, 64]  # Pyramid structure dimensions
    output_dim = 300
    epochs = 40
    threshold = 0.1  # Cosine similarity threshold (lowered)
    glove_file = "glove.6B.300d.txt"  # Path to GloVe embeddings file

    # Load GloVe Embeddings
    glove_embeddings = load_glove_embeddings(glove_file, vocab_size=100)
    input_embeddings = prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=input_dim)

    # Initialize Hyperbolic-LCM Model
    model = HyperbolicLCM(input_dim, hidden_dims, num_heads=8, num_layers=6, ff_dim=2048, output_dim=output_dim)
    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)  # Lowered learning rate
    criterion = nn.MSELoss()

    # Create slightly perturbed target embeddings
    target_embeddings = input_embeddings + torch.randn_like(input_embeddings) * 0.01  # Reduced noise level

    # Training Loop
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        output_embeddings = model(input_embeddings)
        loss = criterion(output_embeddings, target_embeddings)
        curvature_reg = torch.abs(model.curvature - 1.0) * 0.01  # Regularization term for curvature
        total_loss = loss + curvature_reg
        total_loss.backward()
        optimizer.step()

        # Compute Accuracy
        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold)
        print(f"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Curvature: {model.curvature.item():.4f} | Accuracy: {accuracy * 100:.2f}%")

if __name__ == "__main__":
    test_hyperbolic_lcm()

Epoch 1/40 | Loss: 0.1061 | Curvature: 0.5414 | Accuracy: 0.00%
Epoch 2/40 | Loss: 0.1057 | Curvature: 0.5415 | Accuracy: 0.00%
Epoch 3/40 | Loss: 0.1054 | Curvature: 0.5416 | Accuracy: 0.00%
Epoch 4/40 | Loss: 0.1050 | Curvature: 0.5417 | Accuracy: 0.00%
Epoch 5/40 | Loss: 0.1047 | Curvature: 0.5418 | Accuracy: 7.50%
Epoch 6/40 | Loss: 0.1043 | Curvature: 0.5419 | Accuracy: 10.00%
Epoch 7/40 | Loss: 0.1040 | Curvature: 0.5420 | Accuracy: 10.00%
Epoch 8/40 | Loss: 0.1037 | Curvature: 0.5421 | Accuracy: 10.00%
Epoch 9/40 | Loss: 0.1033 | Curvature: 0.5422 | Accuracy: 22.50%
Epoch 10/40 | Loss: 0.1030 | Curvature: 0.5423 | Accuracy: 35.00%
Epoch 11/40 | Loss: 0.1027 | Curvature: 0.5424 | Accuracy: 40.00%
Epoch 12/40 | Loss: 0.1024 | Curvature: 0.5425 | Accuracy: 40.00%
Epoch 13/40 | Loss: 0.1021 | Curvature: 0.5426 | Accuracy: 50.00%
Epoch 14/40 | Loss: 0.1018 | Curvature: 0.5427 | Accuracy: 60.00%
Epoch 15/40 | Loss: 0.1014 | Curvature: 0.5428 | Accuracy: 60.00%
Epoch 16/40 | Loss: 0.1011 | Curvature: 0.5429 | Accuracy: 60.00%
Epoch 17/40 | Loss: 0.1009 | Curvature: 0.5430 | Accuracy: 70.00%
Epoch 18/40 | Loss: 0.1006 | Curvature: 0.5431 | Accuracy: 70.00%
Epoch 19/40 | Loss: 0.1003 | Curvature: 0.5432 | Accuracy: 70.00%
Epoch 20/40 | Loss: 0.1000 | Curvature: 0.5433 | Accuracy: 70.00%
Epoch 21/40 | Loss: 0.0997 | Curvature: 0.5434 | Accuracy: 80.00%
Epoch 22/40 | Loss: 0.0994 | Curvature: 0.5435 | Accuracy: 87.50%
Epoch 23/40 | Loss: 0.0991 | Curvature: 0.5436 | Accuracy: 90.00%
Epoch 24/40 | Loss: 0.0988 | Curvature: 0.5437 | Accuracy: 90.00%
Epoch 25/40 | Loss: 0.0985 | Curvature: 0.5438 | Accuracy: 92.50%
Epoch 26/40 | Loss: 0.0982 | Curvature: 0.5439 | Accuracy: 100.00%
Epoch 27/40 | Loss: 0.0979 | Curvature: 0.5440 | Accuracy: 100.00%
Epoch 28/40 | Loss: 0.0977 | Curvature: 0.5441 | Accuracy: 100.00%
Epoch 29/40 | Loss: 0.0974 | Curvature: 0.5442 | Accuracy: 100.00%
Epoch 30/40 | Loss: 0.0971 | Curvature: 0.5443 | Accuracy: 100.00%
Epoch 31/40 | Loss: 0.0968 | Curvature: 0.5444 | Accuracy: 100.00%
Epoch 32/40 | Loss: 0.0965 | Curvature: 0.5445 | Accuracy: 100.00%
Epoch 33/40 | Loss: 0.0962 | Curvature: 0.5446 | Accuracy: 100.00%
Epoch 34/40 | Loss: 0.0959 | Curvature: 0.5447 | Accuracy: 100.00%
Epoch 35/40 | Loss: 0.0956 | Curvature: 0.5448 | Accuracy: 100.00%
Epoch 36/40 | Loss: 0.0953 | Curvature: 0.5449 | Accuracy: 100.00%
Epoch 37/40 | Loss: 0.0950 | Curvature: 0.5450 | Accuracy: 100.00%
Epoch 38/40 | Loss: 0.0947 | Curvature: 0.5451 | Accuracy: 100.00%
Epoch 39/40 | Loss: 0.0944 | Curvature: 0.5452 | Accuracy: 100.00%
Epoch 40/40 | Loss: 0.0941 | Curvature: 0.5453 | Accuracy: 100.00%

import torch
import torch.nn as nn
import torch.nn.functional as F
from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings
from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer

# Cosine Similarity for Accuracy
def compute_accuracy(predicted, target, threshold=0.1):
    cos_sim = F.cosine_similarity(predicted, target, dim=-1)
    correct = (cos_sim > threshold).float()
    accuracy = correct.mean().item()
    return accuracy

# Load GloVe Embeddings Manually
def load_glove_embeddings(file_path, vocab_size=5000):
    """Load GloVe embeddings from a file for a small subset."""
    embeddings = {}
    with open(file_path, 'r') as f:
        for i, line in enumerate(f):
            if i >= vocab_size:
                break
            values = line.split()
            word = values[0]
            vector = torch.tensor([float(x) for x in values[1:]], dtype=torch.float)
            embeddings[word] = vector
    return embeddings

# Prepare Input Embeddings
def prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=300):
    """Randomly sample embeddings from the loaded GloVe vectors."""
    selected_vectors = torch.stack(
        [glove_embeddings[word] for word in list(glove_embeddings.keys())[:sequence_length]]
    )
    input_embeddings = selected_vectors.unsqueeze(0).repeat(batch_size, 1, 1)
    return input_embeddings

# Pyramid Layer
class PyramidLayer(nn.Module):
    def __init__(self, input_dim, output_dim, manifold):
        super(PyramidLayer, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.manifold.expmap0(self.linear(x))  # Hyperbolic compression
        return x

# Hyperbolic Cube
class HyperbolicCube(nn.Module):
    def __init__(self, layers_dims, manifold):
        super(HyperbolicCube, self).__init__()
        self.manifold = manifold
        self.pyramid_layers = nn.ModuleList([
            PyramidLayer(layers_dims[i], layers_dims[i+1], manifold)
            for i in range(len(layers_dims) - 1)
        ])

    def forward(self, x):
        for layer in self.pyramid_layers:
            x = layer(x)
        return x

# PreNet and PostNet
class PreNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, manifold):
        super(PreNet, self).__init__()
        self.linear = nn.Linear(input_dim, hidden_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.linear(x)
        x = self.manifold.expmap0(x)
        return x

class PostNet(nn.Module):
    def __init__(self, hidden_dim, output_dim, manifold):
        super(PostNet, self).__init__()
        self.linear = nn.Linear(hidden_dim, output_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.manifold.logmap0(x)
        x = self.linear(x)
        return x

# Dual Hidden LCM with two hidden dimensions
class DualHiddenLCM(nn.Module):
    def __init__(self, input_dim, hidden_dims, hidden_dim2, output_dim):
        super(DualHiddenLCM, self).__init__()
        self.curvature = nn.Parameter(torch.tensor(1.0, requires_grad=True))
        self.manifold = PoincareBall(c=self.curvature)

        # Hidden Dimension 1: Pyramid structure
        self.prenet = PreNet(input_dim, hidden_dims[0], self.manifold)
        self.hyperbolic_cube = HyperbolicCube(hidden_dims, self.manifold)
        self.postnet = PostNet(hidden_dims[-1], output_dim, self.manifold)

        # Hidden Dimension 2: 20D bottleneck
        self.hidden_dim2 = nn.Linear(input_dim, hidden_dim2)
        self.hidden_dim2_output = nn.Linear(hidden_dim2, output_dim)

    def forward(self, x):
        # Hidden Dimension 1
        x_hidden1 = self.prenet(x)
        x_hidden1 = self.hyperbolic_cube(x_hidden1)
        x_hidden1 = self.postnet(x_hidden1)

        # Hidden Dimension 2
        x_hidden2 = F.relu(self.hidden_dim2(x))
        x_hidden2 = self.hidden_dim2_output(x_hidden2)

        # Combine outputs
        combined = x_hidden1 + x_hidden2
        return combined

# Testing DualHiddenLCM Architecture
def test_dualhidden_lcm():
    batch_size = 4
    sequence_length = 10
    input_dim = 300  # GloVe embedding dimension
    hidden_dims = [512, 256, 128, 64]  # Pyramid structure dimensions
    hidden_dim2 = 20  # 20D bottleneck
    output_dim = 300
    epochs = 60
    threshold = 0.1  # Cosine similarity threshold
    glove_file = "glove.6B.300d.txt"  # Path to GloVe embeddings file

    # Load GloVe Embeddings
    glove_embeddings = load_glove_embeddings(glove_file, vocab_size=100)
    input_embeddings = prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=input_dim)

    # Initialize DualHiddenLCM Model
    model = DualHiddenLCM(input_dim, hidden_dims, hidden_dim2, output_dim)
    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()

    # Create slightly perturbed target embeddings
    target_embeddings = input_embeddings + torch.randn_like(input_embeddings) * 0.01

    # Training Loop
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        output_embeddings = model(input_embeddings)
        loss = criterion(output_embeddings, target_embeddings)
        curvature_reg = torch.abs(model.curvature - 1.0) * 0.01  # Regularization for curvature
        total_loss = loss + curvature_reg
        total_loss.backward()
        optimizer.step()

        # Compute Accuracy
        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold)
        print(f"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Curvature: {model.curvature.item():.4f} | Accuracy: {accuracy * 100:.2f}%")

if __name__ == "__main__":
    test_dualhidden_lcm()

Epoch 1/60 | Loss: 0.1202 | Curvature: 0.5414 | Accuracy: 40.00%
Epoch 2/60 | Loss: 0.1195 | Curvature: 0.5415 | Accuracy: 40.00%
Epoch 3/60 | Loss: 0.1188 | Curvature: 0.5416 | Accuracy: 40.00%
Epoch 4/60 | Loss: 0.1181 | Curvature: 0.5417 | Accuracy: 45.00%
Epoch 5/60 | Loss: 0.1174 | Curvature: 0.5418 | Accuracy: 45.00%
Epoch 6/60 | Loss: 0.1168 | Curvature: 0.5419 | Accuracy: 50.00%
Epoch 7/60 | Loss: 0.1161 | Curvature: 0.5420 | Accuracy: 50.00%
Epoch 8/60 | Loss: 0.1155 | Curvature: 0.5421 | Accuracy: 55.00%
Epoch 9/60 | Loss: 0.1149 | Curvature: 0.5422 | Accuracy: 60.00%
Epoch 10/60 | Loss: 0.1143 | Curvature: 0.5423 | Accuracy: 60.00%
Epoch 11/60 | Loss: 0.1137 | Curvature: 0.5424 | Accuracy: 60.00%
Epoch 12/60 | Loss: 0.1132 | Curvature: 0.5425 | Accuracy: 60.00%
Epoch 13/60 | Loss: 0.1126 | Curvature: 0.5426 | Accuracy: 60.00%
Epoch 14/60 | Loss: 0.1121 | Curvature: 0.5427 | Accuracy: 60.00%
Epoch 15/60 | Loss: 0.1116 | Curvature: 0.5428 | Accuracy: 60.00%
Epoch 16/60 | Loss: 0.1111 | Curvature: 0.5429 | Accuracy: 60.00%
Epoch 17/60 | Loss: 0.1106 | Curvature: 0.5430 | Accuracy: 60.00%
Epoch 18/60 | Loss: 0.1101 | Curvature: 0.5431 | Accuracy: 60.00%
Epoch 19/60 | Loss: 0.1096 | Curvature: 0.5432 | Accuracy: 70.00%
Epoch 20/60 | Loss: 0.1091 | Curvature: 0.5433 | Accuracy: 70.00%
Epoch 21/60 | Loss: 0.1086 | Curvature: 0.5434 | Accuracy: 70.00%
Epoch 22/60 | Loss: 0.1082 | Curvature: 0.5435 | Accuracy: 70.00%
Epoch 23/60 | Loss: 0.1077 | Curvature: 0.5436 | Accuracy: 70.00%
Epoch 24/60 | Loss: 0.1073 | Curvature: 0.5437 | Accuracy: 77.50%
Epoch 25/60 | Loss: 0.1068 | Curvature: 0.5438 | Accuracy: 80.00%
Epoch 26/60 | Loss: 0.1064 | Curvature: 0.5439 | Accuracy: 80.00%
Epoch 27/60 | Loss: 0.1059 | Curvature: 0.5440 | Accuracy: 80.00%
Epoch 28/60 | Loss: 0.1055 | Curvature: 0.5441 | Accuracy: 80.00%
Epoch 29/60 | Loss: 0.1050 | Curvature: 0.5442 | Accuracy: 97.50%
Epoch 30/60 | Loss: 0.1046 | Curvature: 0.5443 | Accuracy: 100.00%
Epoch 31/60 | Loss: 0.1042 | Curvature: 0.5444 | Accuracy: 100.00%
Epoch 32/60 | Loss: 0.1037 | Curvature: 0.5445 | Accuracy: 100.00%
Epoch 33/60 | Loss: 0.1033 | Curvature: 0.5446 | Accuracy: 100.00%
Epoch 34/60 | Loss: 0.1029 | Curvature: 0.5447 | Accuracy: 100.00%
Epoch 35/60 | Loss: 0.1024 | Curvature: 0.5448 | Accuracy: 100.00%
Epoch 36/60 | Loss: 0.1020 | Curvature: 0.5449 | Accuracy: 100.00%
Epoch 37/60 | Loss: 0.1016 | Curvature: 0.5450 | Accuracy: 100.00%
Epoch 38/60 | Loss: 0.1012 | Curvature: 0.5451 | Accuracy: 100.00%
Epoch 39/60 | Loss: 0.1007 | Curvature: 0.5452 | Accuracy: 100.00%
Epoch 40/60 | Loss: 0.1003 | Curvature: 0.5453 | Accuracy: 100.00%
Epoch 41/60 | Loss: 0.0999 | Curvature: 0.5454 | Accuracy: 100.00%
Epoch 42/60 | Loss: 0.0995 | Curvature: 0.5455 | Accuracy: 100.00%
Epoch 43/60 | Loss: 0.0990 | Curvature: 0.5456 | Accuracy: 100.00%
Epoch 44/60 | Loss: 0.0986 | Curvature: 0.5457 | Accuracy: 100.00%
Epoch 45/60 | Loss: 0.0982 | Curvature: 0.5458 | Accuracy: 100.00%
Epoch 46/60 | Loss: 0.0978 | Curvature: 0.5459 | Accuracy: 100.00%
Epoch 47/60 | Loss: 0.0973 | Curvature: 0.5460 | Accuracy: 100.00%
Epoch 48/60 | Loss: 0.0969 | Curvature: 0.5461 | Accuracy: 100.00%
Epoch 49/60 | Loss: 0.0965 | Curvature: 0.5462 | Accuracy: 100.00%
Epoch 50/60 | Loss: 0.0961 | Curvature: 0.5463 | Accuracy: 100.00%
Epoch 51/60 | Loss: 0.0956 | Curvature: 0.5464 | Accuracy: 100.00%
Epoch 52/60 | Loss: 0.0952 | Curvature: 0.5465 | Accuracy: 100.00%
Epoch 53/60 | Loss: 0.0948 | Curvature: 0.5466 | Accuracy: 100.00%
Epoch 54/60 | Loss: 0.0944 | Curvature: 0.5467 | Accuracy: 100.00%
Epoch 55/60 | Loss: 0.0939 | Curvature: 0.5468 | Accuracy: 100.00%
Epoch 56/60 | Loss: 0.0935 | Curvature: 0.5469 | Accuracy: 100.00%
Epoch 57/60 | Loss: 0.0931 | Curvature: 0.5470 | Accuracy: 100.00%
Epoch 58/60 | Loss: 0.0926 | Curvature: 0.5471 | Accuracy: 100.00%
Epoch 59/60 | Loss: 0.0922 | Curvature: 0.5472 | Accuracy: 100.00%
Epoch 60/60 | Loss: 0.0918 | Curvature: 0.5473 | Accuracy: 100.00%

import os
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, TensorDataset
from geoopt import PoincareBall, ManifoldParameter
from geoopt.optim import RiemannianAdam

# Initialize DDP
def setup_ddp(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

# Cleanup DDP
def cleanup_ddp():
    dist.destroy_process_group()

# Cosine Similarity for Accuracy
def compute_accuracy(predicted, target, threshold=0.1):
    cos_sim = F.cosine_similarity(predicted, target, dim=-1)
    correct = (cos_sim > threshold).float()
    accuracy = correct.mean().item()
    return accuracy

# DualHiddenLCM Definition
class DualHiddenLCM(nn.Module):
    def __init__(self, input_dim, hidden_dims, hidden_dim2, output_dim):
        super(DualHiddenLCM, self).__init__()
        self.curvature = nn.Parameter(torch.tensor(1.0, requires_grad=True))
        self.manifold = PoincareBall(c=self.curvature)

        # Hidden Dimension 1: Pyramid structure
        self.prenet = PreNet(input_dim, hidden_dims[0], self.manifold)
        self.hyperbolic_cube = HyperbolicCube(hidden_dims, self.manifold)
        self.postnet = PostNet(hidden_dims[-1], output_dim, self.manifold)

        # Hidden Dimension 2: 20D bottleneck
        self.hidden_dim2 = nn.Linear(input_dim, hidden_dim2)
        self.hidden_dim2_output = nn.Linear(hidden_dim2, output_dim)

    def forward(self, x):
        # Hidden Dimension 1
        x_hidden1 = self.prenet(x)
        x_hidden1 = self.hyperbolic_cube(x_hidden1)
        x_hidden1 = self.postnet(x_hidden1)

        # Hidden Dimension 2
        x_hidden2 = F.relu(self.hidden_dim2(x))
        x_hidden2 = self.hidden_dim2_output(x_hidden2)

        # Combine outputs
        combined = x_hidden1 + x_hidden2
        return combined

# Helper Classes
class PreNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, manifold):
        super(PreNet, self).__init__()
        self.linear = nn.Linear(input_dim, hidden_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.linear(x)
        x = self.manifold.expmap0(x)
        return x

class PostNet(nn.Module):
    def __init__(self, hidden_dim, output_dim, manifold):
        super(PostNet, self).__init__()
        self.linear = nn.Linear(hidden_dim, output_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.manifold.logmap0(x)
        x = self.linear(x)
        return x

class HyperbolicCube(nn.Module):
    def __init__(self, layers_dims, manifold):
        super(HyperbolicCube, self).__init__()
        self.manifold = manifold
        self.pyramid_layers = nn.ModuleList([
            PyramidLayer(layers_dims[i], layers_dims[i+1], manifold)
            for i in range(len(layers_dims) - 1)
        ])

    def forward(self, x):
        for layer in self.pyramid_layers:
            x = layer(x)
        return x

class PyramidLayer(nn.Module):
    def __init__(self, input_dim, output_dim, manifold):
        super(PyramidLayer, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        self.manifold = manifold

    def forward(self, x):
        x = self.manifold.expmap0(self.linear(x))
        return x

# Training Loop
def train(rank, world_size, data, target, input_dim, hidden_dims, hidden_dim2, output_dim, epochs, threshold):
    setup_ddp(rank, world_size)
    device = torch.device(f'cuda:{rank}')

    # Prepare Model
    model = DualHiddenLCM(input_dim, hidden_dims, hidden_dim2, output_dim).to(device)
    model = DDP(model, device_ids=[rank])

    # Optimizer and Loss
    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()

    # DataLoader
    dataset = TensorDataset(data, target)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

    # Training
    for epoch in range(epochs):
        model.train()
        for batch_data, batch_target in dataloader:
            batch_data = batch_data.to(device)
            batch_target = batch_target.to(device)

            optimizer.zero_grad()
            output = model(batch_data)
            loss = criterion(output, batch_target)
            curvature_reg = torch.abs(model.module.curvature - 1.0) * 0.01  # Regularization
            total_loss = loss + curvature_reg
            total_loss.backward()
            optimizer.step()

        if rank == 0:  # Print only on the main process
            print(f"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f}")

    cleanup_ddp()

# Main Script
def main():
    world_size = torch.cuda.device_count()
    if world_size < 2:
        print("This script requires at least 2 GPUs.")
        return

    # Simulated Data
    input_dim = 300
    hidden_dims = [512, 256, 128, 64]
    hidden_dim2 = 20
    output_dim = 300
    epochs = 10
    threshold = 0.1
    batch_size = 4
    num_samples = 100

    data = torch.randn(num_samples, input_dim)
    target = data + torch.randn_like(data) * 0.01  # Slight perturbation

    # Start Training
    torch.multiprocessing.spawn(
        train,
        args=(world_size, data, target, input_dim, hidden_dims, hidden_dim2, output_dim, epochs, threshold),
        nprocs=world_size,
        join=True
    )

if __name__ == "__main__":
    main()



Agile and devops 
introduction

In any organization with active software development projects, there are two words you'll often hear, regardless of what specific platforms or technologies might be in use. Those words are agile and DevOps. And these are two different subjects, but more and more, they become part of the same conversation. Any time we talk about best practices or approaches to plan and deliver successful software, you'll hear the terms agile and DevOps. But what do these actually mean? What's the difference between them, and why are they considered complimentary? But most of all, how do they help? What value do these two ideas bring to current software development? Well, let's find out. Welcome to the Executive Briefing on Agile and DevOps.

The waterfall problem 

Agile and DevOps are easier to understand once you realize they both try to fix very specific problems, problems we encountered again and again using the software planning and development methods that were typical from, let's say, the 1950s, all the way through the 1990s. And okay, I'm not saying that every organization would plan and build software exactly the same way, but still, for several decades, if you were involved in any large‑scale software projects, you would expect it to be planned and managed using a formalized, highly structured approach with a sequential set of different phases or stages, something we now call a waterfall model or a waterfall methodology. The term waterfall wasn't the official name of anything. It's now what we call the various project planning approaches, whereas each phase of the project was completed, the results coming from that phase, and then flow down to the next things. Generally, the waterfall methods for software projects had five or six separate phases. First, often considered the most important, was requirements gathering, where the team would perform research, interview customers, and do whatever else was needed to attempt to fully and completely understand every single aspect of what the software would ever need to do, and document all of it in excruciating detail. And we're not talking about days or weeks of work. The requirements phase would often take months to complete. If you're wondering whether you get some proof of concepts or demo applications from this, no. Nobody was writing any code; nobody was programming anything yet. What you'd get from the requirements phase was basically a thick binder with hundreds of pages of documentation. This would be the source of authority to drive every decision from this point on. The requirements phase was then complete, and we move on to the second phase, software design or software architecture to now define the technical aspects of exactly how this software was going to be built. The output of this design phase was also more documentation, which then flowed down to the next phase, development or implementation, where the programmers would actually write the software. So we were several months in, sometimes years, and the project hasn't really even started to be coded yet. After development was finished, it would flow down to the fourth phase, testing, usually performed by a separate team. If they found issues or bugs, someone had to decide, should that bug be fixed? Could it be fixed without messing up the project timeline? Or could it wait and be fixed in the next version, whenever that was going to be? And after that was done, we'd reach the deployment or operational phase. And deploying a big piece of software to a new environment almost always runs into unexpected problems. But remember, the time the team first started collecting requirements and the time the software was finally deployed, years had often passed. It's entirely possible that the original requirements have changed or they weren't even properly understood. The customer, who sometimes hasn't even been talked to since the very first phase, may take one look at the software and say, that's not actually what we wanted, or that's what we wanted 2 years ago. It's not going to work now. We needed feature X, and we needed it 6 months ago. And if there's one thing a waterfall approach is not good at, it's going backwards up the waterfall to deal with new requirements or changes in the business landscape, or even just things that we learned and could have been better along the way. A waterfall approach has its place. If you're building a suspension bridge, there are reality‑based reasons to have a specific or independent planning, architecture, construction, operations phases where you want to micromanage certain parts of the project. But for software, it's too constrained, it's not responsive enough, it's not flexible enough, and it's not fast enough. And most important, waterfall doesn't support what software development is naturally good at: evolving and changing, being updated, optimized, improved, extended. So developers and businesses started looking for a better way to build and deploy software projects and being more flexible, faster, and more responsive, a more agile approach.

Introducing agile development 

I'm not suggesting we all happily used waterfall planning methodologies for 40 or 50 years before realizing there was even a problem. No! People recognized issues with that heavyweight micromanaged approach very early on. And over the years, there have been many suggested methods for planning the software development process. But particularly in the 1990s, alternative approaches started to become more popular. Methodologies with names like the Unified Process, then Scrum, and XP, or Extreme Programming. And just to be clear, these aren't technologies. They're just ideas, guidelines, suggestions for a structured approach to plan a software project. But while these different approaches, these methodologies, weren't identical, there were a lot of people trying to fix the problems of waterfall in a similar way. So in 2001, 17 of these people, including folks working on other methodologies like Scrum and XP, and well‑known, well‑respected developers and authors working on software best practices got together and wrote the Manifesto for Agile Software Development. Manifesto sounds intimidating, like we're about to get another big binder of documentation, but it's not. You can write the Agile Manifesto on one page. It's a set of values and principles. And okay, values and principles always sounds a bit vague and a little hand‑wavy and fluffy, but its specific, 4 values, 12 principles written in plain, non‑technical language. And if you're wondering what's the difference between a value and a principle here, values come first. They lead to the principles. Values are that high‑level, what do we value? What do we think is most important about software development? Then the principles become the more practical, okay, if that's what we value, how are we going to make it happen? And again, these values are all in plain, non‑technical language. Let's take a look.


The agile values 

As we go over these values, remember, the Agile Manifesto did not come out of nowhere. This was a course correction. It's aware of and trying to fix the problems that happened again and again with Waterfall project planning. So here, each of the four Agile values is written as a comparison, a contrast between two things. We prioritize this above that. For example, not just we value working software, but we value working software over comprehensive documentation. So this value recognizes that all Waterfall projects often encouraged an over focus on exhaustive documentation where more effort, more time, more priority, more attention was placed on that instead of on the actual software development. Quick sidebar. Some critics of the Agile approach have misrepresented this to say Agile doesn't do any documentation. It's not true. Documentation is great. You should have it whenever and wherever you need it, but never lose track that the entire point of doing this is working software, not just a shelf of impeccable binders. Let's take the next one. We value individuals and interactions over process and tools. Software development is not a predictable, numbers‑based, pure resource management task. People did try to treat it that way. We have five developers on one team. They say it'll take a month to implement a new feature. Well, let's just get an agency to provide 15 temps, and we'll have it all done by Friday. It doesn't work like that. There's technical knowledge, institutional knowledge, technical ability, productivity, team dynamics, multiple skill sets required, and a host of other things to take into account. And notice, the very intentional word choice here. This doesn't say we value people, which would be cliché. Here it says, we value individuals. Agile asks us to not just focus on anonymous roles to fill or jobs to be done, but to always bring our attention to the actual individuals on the team, their personalities, their whole selves. Who, exactly who is part of this? What's that person's technical skill sets? What's their business knowledge? What are their strengths? How well do they work with others? And very importantly, how do all these different individuals communicate and collaborate? But again, this value is not saying processes and tools aren't important. They are, but your individuals and how they collaborate is more important to the success of a project. Next, Agile development values customer collaboration over contract negotiation. We're not looking to find the minimum number of line items we can get a customer to agree on, then refuse any other requests because that wasn't in the original contract. These days, successful software development must be a partner to the business, not just a service to it or an enabler of it, and certainly not in competition or conflict with it. But again, this is not saying the contracts are unimportant. They are, but as an agreement and an understanding, not as a club to beat each other with. The higher value is genuine and ongoing collaboration with the customer. And finally, Agile values responding to change over following a plan. We bake in the idea that change will happen. There will be new demands, new competition, new regulation, new devices to support, new understandings, new, new, new. Well, we don't know what might happen. That's the point. And we encourage our own abilities to be responsive and flexible. But, okay, these are all good, but there's nothing here that says, how do we actually do any of this? And that's where the Agile principles can help.

Agile principles 

We're not going to go through every single one of the 12 principles, but let's take this first one. Our highest priority is to satisfy the customer through early and continuous delivery of valuable software, and this one, deliver working soccer frequently from a couple of weeks to a couple of months, with the preference to the shorter timescale. So even just these two alone is a huge perspective shift from waterfall, where we might talk to the customer early on and then disappear for a couple of years. And to support these principles requires us to take an iterative and incremental approach. Instead of massive, long phases of waterfall, we assume that here we'll have multiple cycles where each cycle involves aspects of requirements planning, implementing, testing, and deploying, and then start the cycle again, delivering something valuable, something the customer can try and test and give feedback on every time. Next, agile tells us to embrace the idea that requirements are never fixed in stone and always changing and evolving. There are several agile principles around the idea of communication between the business and the developers and communication within the team itself, trusting the team, allowing the team to self‑organize. And what these principles lead to, even when it's not explicitly stated here, are some typical practices. For example, in waterfall development, it wasn't unusual to have teams of 40 or 50 or even 100 people. Agile teams are usually small, typically 10 or so people. Sidebar: we suggest a two‑pizza team, meaning If you need more than two pizzas to feed the team, it's likely the team is too big for efficient communication. Small teams are able to collaborate more easily and react more quickly. Teams may be colocated or collaborate virtually. And there are other terms you might not see in this list of agile principles but have now become very associated with agile. You'll often hear agile teams talk about sprints or iterations, which are defined periods of time, aka, a timebox, often only 2 to 3 weeks, where the team builds and tests and releases new software. In a typical sprint, the team will take a look at their to‑do list, called a backlog. They make a quick estimate of which items they can get done during the sprint and then start working. It's all about breaking up the workload into small chunks and working on and delivering one chunk at a time. And agile emphasizes technical excellence. Quality becomes the continuous responsibility of the team itself, unlike in waterfall, where was passed to the testing team to make that judgment. This focus on quality, shorter delivery cycles, and faster feedback on working software has led to the almost ubiquitous practice in agile teams of unit testing. A unit test is a small automated test that focuses on one single behavior for operation that the software is supposed to perform, and the developers continually write and save new unit tests as they're writing the software itself, testing that becomes part of development, not something done afterwards. Developers can work on a new feature, and with a click of a button quickly run a set of automated tests to see if it's working as intended and make sure it didn't break anything else. This unit testing reduces the time to release and increases the quality by replacing larger, more time‑consuming, full‑scale application testing that waterfall projects relied on. However, while the agile principles talk about continuously delivering software to the customer, they don't talk about how we can reliably and repeatedly do that, and that's where DevOps comes in

Sidebar: is agile a methodology 

A quick sidebar moment. It's not unusual to hear people talk about the Agile methodology, but this word methodology is problematic, and a lot of folks in the Agile world try and avoid it. First, the manifesto is intentionally not prescriptive. It doesn't pretend to be a methodology. It isn't giving you this formalized project management template that you're supposed to follow. That's kind of the point. One of the values here is to stop thinking that if you just find some process in some external methodology and then follow it to the letter, it'll take care of everything. We're trying to get away from that approach, but structures and processes are still important. They can provide consistency, clarity, clarity around expectations, and promote transparency. Agile frameworks like Scrum, XP, or Crystal all support the values and principles in the manifesto, but then add some specific guidelines and practices around things like planning, facilitating meetings, managing communication, distributing workloads, or keeping track of feature requests. You'll notice that these are called frameworks, even they avoid the term methodology because it suggests the right focus. We're not trying to find a methodology to follow. We're trying to find a framework that is flexible and can support us in building something. End sidebar. Back to DevOps.

Introducing DevOps 

DevOps is a mix of the words development and operations, and like an Agile, it aims to be fast, responsive, and focused on the customer. And while you won't see the term DevOps appear in the Agile manifesto, they complement each other very well. They go together like milk and cookies. Instead of deployment and operations being stuck at that final tail end of the waterfall process, it becomes incorporated into the full delivery lifecycle, everyone working together to deliver value to the business, and DevOps has a strong focus on automating as much as possible. For example, developers may, as part of creating their software, have to create a description of what their software needs to run, what kind of operating system it needs, what components it uses, what frameworks it relies on, and so on. So let's say a developer finishes a bit of code and wants to test it. They checked the code into the source code control system. An automated process often created by the organization's DevOps team would automatically begin. That process might start by reading the description of the software's environmental requirements and then actually create that environment. The process would then deploy the new software into that environment and kick off the automated unit test. If the unit test passed, then the software is deemed acceptable and is ready for deployment. If a test fails, the developer is notified and they go back to programming. Some organizations will take successful software and run larger scale integration tests for other validations, but at the end of the day, the software is ready to deploy, and that deployment can also be automated. For example, in a cloud‑based environment, the new application code might be launched into containers or virtual machines that are emulating whatever hardware or platform we want to use. The containers or virtual machines running the old version of the application would then be deleted, and users would find themselves using the new application right away, hopefully, without realizing anything had even happened. This entire process is often referred to as a pipeline. Depending on the specifics of an organization's pipeline, you might hear terms like continuous integration, or continuous delivery, or the CI/CD pipeline. You probably encounter this even more than you realize. Something as simple as your smartphone automatically updating one of your apps is Agile and DevOps practices in action. A team has completed a sprint, it's gone through things like automated unit testing and continuous delivery to automatically push out the update, and the result is you have an enhanced app with new features and bug fixes, and hopefully, with very little or even no interruption to you as the user. So that's it in a nutshell, at least from a software development perspective. Agile is about producing small improvements to an application in a continuous series of short sprints, DevOps helps developers automate the testing of their code, and when all the tests pass, DevOps can also help automate a pain‑free deployment.

Implementing agile and devops 

Both Agile and DevOps are really philosophies. They're a set of principles and values that encourage a way of thinking, a mindset. There's no one correct way to do them. You'll find frameworks such as Scrum or Extreme Programming that bring specific approaches and practices to the table, but they're agile because their practices support the values and principles. Agile and DevOps bring new ways of working into an organization. Most organizations start by shifting a small number projects to Agile and DevOps, and doing so lets them gain some experience and evolve their practices and processes based upon what they learn. Agile and DevOps can be applied to customer‑facing software applications such as mobile apps and websites. They can also be applied to internal‑only software such as custom line of business applications. Anywhere that you have software that can benefit from smaller, more rapid, more frequent release cycles, that's where Agile and DevOps are valuable. And here's the best part, once organizations got a taste of Agile for their software development, they started wanting those benefits across the business and elsewhere in the organization. They were being Agile with a capital A, but they wanted to be more broadly agile with a lowercase a. Adopting Agile practices and leveraging Agile values and principles in other areas is often called enterprise Agile and leads to business agility. The reason Agile principles can be applied so broadly is because that's all Agile really is. It's a set of principles and values. They may have the roots in software development, but today we recognize that agility has value across the entire organization, even on internal projects. A finance team might roll out a new expense reporting system, monitor it for a few weeks, and then roll out the changes based upon what they've observed. Or a strategy team might adopt a new pricing model for their company's services, gauge the market reaction, and then adjust. A marketing team might forgo that $1,000,000 campaign of the old days and instead focus on a series of smaller campaigns that lets them sharpen their aim. The point is the Agile mindset has helped organizations become more adaptive, how to quickly roll out something small, see how it does, and then move on to the next set of improvements. Instead of having to make massive, expensive multi‑year investments, and then pivoting to react to the market, now companies can continually test ideas, features, and react more quickly to identify their best path to success. They become, well, more agile.

Agile and Devops in the enterprise 

However, there are some issues about Agile that can make it difficult for some companies to adopt. Agile is a very intentional way of working, and it applies to all levels of the organization. You might often think about Agile practices being applied at the team level, but in order for that to work, their managers have to understand Agile and support it. And to do that, the organization's executives have to buy into Agile as well. That executive support is important because moving to use Agile principles requires a compelling vision. Organizational leaders need to clearly communicate why they want to make this change and what they expect from it. They need to relentlessly share that vision and help everyone understand this is where we're going and this is why. It's also important to understand that change can be hard. And for many organizations, Agile is a big change. Agile and DevOps aren't an all or nothing proposition. Some parts of an organization may use them, while others don't. Agile and DevOps are really about mindset. You can leverage them anywhere they make sense such as on specific software projects or a specific marketing campaign. You can also use other methodologies and practices in places where they make the most sense instead.

Tools and environment overview 

Neither Agile nor DevOps impose any kind of specific platform, technology, or tool. Agile software development uses many of the same tools that developers have used for years. Integrated development environments, such as Microsoft Visual Studio or Eclipse, source code control systems like Git, and, of course, whatever languages or frameworks they've always used. Unit testing is usually a part of Agile and DevOps environments and a huge variety of languages and tools exist for creating and managing those unit tests. Although not specifically tied to Agile or DevOps, technologies like virtual machines and containers usually crop up in environments that are moving to an Agile and DevOps approach. These technologies enable faster deployment of software for testing and to production. DevOps pipelines often involve one or more sets of tools designed to facilitate automation. These may include automated testing software, continuous integration, or delivery tooling, and so on. But remember, simply having the right tools doesn't magically create Agile or DevOps. These often require a great deal of education and thoughtfulness to successfully adopt. They'll often require buy‑ins at the highest levels of the organization, and they may even require significant cultural changes, org chart realignment, and more. Tools and technologies can certainly support the use of these principles, but tools alone won't create the outcomes you're looking for. And remember, Agile principles aren't limited to software development at all. You can find other tools that can support Agile throughout the organization like Kanban, even good old white boards can be useful along with real‑time collaboration tools. It's all about sharing information within and across teams, aligning and coordinating efforts, and breaking those giant workloads into small, rapid sprints. So there you have it, Agile and DevOps in a nutshell designed to help organizations create, evolve, and deploy more quickly and efficiently. They have been changing the way we think about software development for years. Now, they're starting to change the way we think about everything our organizations do, from finance and marketing to nearly any part of the business. And with all that history behind it, we're seeing more and more organizations start to explore their benefits. As markets and consumer needs keep evolving, Agile organizations will be ready to respond rapidly and bring value to their customers. I hope you found this useful. Thanks for watching

The agile Mindset- Why i should care?

mindset makes the difference 

A software team is having the daily meeting. Like every other day, they're starting at 10:00 in the morning around the same task board in their office room. They are talking about what they did yesterday, planning the work for today, and there is always keeping the discussion in time blocks of 15 minutes. This team is basically conducting the meeting as described in one of the most popular Agile frameworks. Some of you may recognize this as the Daily Scrum. Now, let's meet a team member, Jerry. Jerry has already given his update, but a few minutes before the end of the meeting, Jerry is in doubt. Maybe he should mention potential code vulnerability he noticed in the checked out code earlier this morning. But then he's thinking, the Daily Scrum is about to end, and he would need to ask his colleagues to meet again for another discussion and maybe even postpone the regular daily release procedure. And in the end, without further analysis, he's not completely sure of the vulnerability. Let's imagine Jerry decides to keep his thoughts to himself, and the cold gets released. And unfortunately, the security breach really happens, bringing significant financial or reputational damage to the company. Or imagine Jerry saved the day instead by simply deciding to interact openly with his teammates rather than focusing too much on daily procedures. In our story, Daily Scrum meeting took place with its built‑in opportunity for discussion, but it's Jerry's way of thinking that brings two completely different scenarios. The mindset of one person sometimes has a big impact on our organization, not to mention how huge this becomes on a scale of tens, hundreds, or thousands of employees. Many teams in the organizations decide to take an Agile journey. They sign up to an Agile framework, adopt its processes and practices, and maybe see some improvements. But only some succeed in bringing the real benefits of Agile working ways. And the main reason is that, besides having all formal elements in place, only some work on the less tangible, but the extremely important and foundational element of any true Agile organization, the Agile mindset.

Doing vs Agile 

Throughout my career, I've been working with many agile teams in different organizations, and I've always been intrigued by the repeating pattern where mindset plays such an important element in success. Maybe because developing the agile mindset is more challenging, a less tangible part of the equation, and it's never algorithmic like. You can imagine success with agile as something that is based on both visible and invisible elements. Visible is easier, right? You introduce the process, experiment with best practices, improve on the goal, and if you do your homework well, you will see some results. For example, you start to use the task board, burndown or velocity chart, and you instantly improve visibility. You introduce testing earlier in the process, and you will improve quality sooner. On the other side, the invisible part, the mindset, requires more time and skills, especially if your organization is far from where it wants to be. But the level of benefits is also incomparably higher. The agile mindset brings real value, the essence, things like engaged and reliable teams and delighted customers. People often think of visible versus invisible as doing versus being agile. And we undoubtedly need a holistic approach. We surely need excellent practitioners to do agile, but even more, we need to continuously work on being agile. And it sounds like a lot to do, which I'll be honest, is. So whether you're starting, extending or improving your agile journey, it's good to make sure you truly want and need the benefits of agile in the first place, but also that you're ready to accept some trade‑offs. And this applies to everyone in the organization, starting from leaders. For example, if you want to change a more traditionally oriented organizational mindset to agile, you need to shift the primary focus from the efficiency of the process to people in your teams, or from fixed‑scope plans to adoption of change. But deciding to become agile may be the best decision you can make for your company if you want to put primary focus on customer satisfaction, engaged and happy teams who continuously grow and deliver, adaptiveness, or even to foster innovation and creativity. Okay, by now we've talked about why we should care about developing the agile mindset, but what agile mindset is exactly? It has become a buzzword among leaders in various organizations, but it's often misinterpreted or misused. So in the following modules we will first demystify the words, and then we'll talk about how to embrace the agile mindset in organizational environments.

Understanding Agile Mindset 

Human mindset 

The human mindset is the most powerful tool in the world. It's the key driver of business success or failure, but also the one that makes a crucial difference for us personally. Our mindset is the main reason why we think, act, or perform differently. And yet, the alignment of individual mindset with the corporate culture and corporate success often doesn't get enough attention. And how can we improve that? How can we understand our minds better and use that knowledge to empower our organizations? To answer these questions, let's see how the mindset is created and what it consists of. Over the years, experts have been arguing about what makes the most influence on human mindsetting; our genes, let's call it nature, or everything that has been around us in our lifetime, the environment. Nature and environment shape the mindset, but in which ways? What does mindset consist of? In the context of this course, we're going to talk about its two main elements, values and principles. Values are the foundation. It's basically what we believe is important in our life or work. Values help to form principles. The principles are built‑in rules that guide our actions and decisions. Now, values and principles are what's under the surface, but they become tangible and visible to the outside world when manifested in our behaviors. Let's make this concrete through our earlier example with Jerry. And now we're talking about Jerry who brings the happy ending, the one who prevents security breach. He obviously believes openness is important in work. This is one of his core values, and it has helped him to form his working principle, which is always proactively share important information with teammates. These two manifested in Jerry's behavior when he expressed his concerns about code vulnerability and prevented security breach. These are individual values, principles, and behaviors, but where things become interesting is when it comes to the alignment with the working surrounding. And it goes both ways. An individual can influence the environment, but also the environment highly influences the individual mindset. And this is good news because it means that strategic development of organizational values and principles can encourage desirable behaviors of people who matter the most, who make the difference between success and failure; our employees. In the next clip, we'll be focusing specifically on agile values and principles.

Agile Values 

For the start, let's clear up two common misconceptions. First, some people believe agile values and principles are prescriptive, but it's actually the opposite. In its essence, being agile is exactly about removing constraints, about the ability to create and respond to change freely. The reason for this first misconception is that people often mix the meaning of being agile with a variety of rules, tools, practices, or even decorations available in the field of doing agile today. The second misconception is that agile values and principles are theoretical. Again, it's not true, as they emerge from the practice itself. They were created in 2001 by a group of software developers who wished to change the ways the software was developed at that time, and it wasn't working anymore in new, changing, and complex environments. They got together, pulled information from the practice, discussed their common experiences, things that did or didn't work well. And then as a result, they created the statement with 4 agile values and 12 principles rooted deeply in the real life world. This statement was called the Agile Manifesto, and it turned out to be so useful around the globe that it basically changed the industry, and its usage was extended far beyond the software development. So the authors of the manifesto state, we are uncovering better ways of developing software by doing it and helping others do it. Through this work, we have come to value: individuals and interactions over processes and tools, working software over comprehensive documentation, customer collaboration over contract negotiation, responding to change over following a plan. As you can see, the agile values are far from prescriptive. They don't say you must do this or that. The manifesto never said you shouldn't have processes and tools; it just values more individuals and interactions. The same applies to working software, customer collaboration, or responding to change. Namely, it's clearly stated, while there is value on the items on the right, we value the items on the left more. The organizations with developed agile mindsets simply recognize these alternatives in real life situations, and they focus their energy on more valuable choices.

Agile principle 

In addition to agile values, there are 12 principles behind the agile manifesto. Now we won't analyze each principal separately, but they believe it's extremely important you become aware of them. So let's cover them quickly, and I'll leave you to interpret their meaning in your particular business context. Ready to take the challenge? Okay, here we go. Our highest priority is to satisfy the customer through early and continuous delivery of valuable software. Welcome changing requirements even late in development. Agile processes harness change for the customer's competitive advantage. Deliver working software frequently, from a couple of weeks to a couple of months, with the preference to the shorter timescale. Business people and developers must work together daily throughout the project. Build projects around motivated individuals, give them the environment and support they need, and trust them to get the job done. The most efficient and effective method of conveying information to and within a development team is face‑to‑face conversation. Working software is the primary measure of progress. Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. Continuous attention to technical excellence and good design enhances agility. Simplicity, the art of maximizing the amount of work not done is essential. The best architectures, requirements, and designs emerge from self‑organizing teams. At regular intervals, the team reflects on how to become more effective, then chooses and adjust its behavior accordingly. For more information, please visit agilemanifesto.org. I hope at this point in time, you have a good idea about what an agile mindset is. And now we're ready to talk about how to embrace this mindset in your organization.


Embracing the mindset 

Expectations

Okay, by now we've talked about what the agile mindset is and why it's important, and now we'll be focusing on how to embrace it. But before we dive into the subject, I'd firstly like to manage your expectations of what this means. We're typically used to understanding a plan as something that clearly defines goals or objectives, the course affection, and the specific time frame, but agile mindset adoption doesn't have one final destination and delivery date, nor the algorithm that brings us there. One of the main reasons is that the mindset is subject to continuous improvement, or in other words, if you become agile, you can never be agile enough. There are many parallel examples in life and work, but let's take one, professional growth. When somebody is starting their professional career, of course, they can't say in 5 years, I planned to gain all the knowledge there is about the work, then I will be so professional that I can stay competitive for the rest of my career, or at least it wouldn't be good for anybody to think that way. The same applies to the agile mindset growth. There is no defined endpoint when a company can say that mindset is shaped, and neglect the continuously‑changing world. So in this module, we'll not be revealing magic recipes for definite success, but we'll be covering some guidelines that can help you succeed with your own unique approach towards agility. At this point in time, it's also important to understand that you should avoid creating selective plans, or in other words, it's important to develop all agile values in parallel. For example, you can focus completely on valuing working software, but it doesn't mean much if you're following outdated plans, or the other way around, you can respond to a change, but this won't be good enough if the delivery is slowed down by the unnecessary comprehensive documentation work. So to become agile, companies often need to plan a significant change, cultural change, which is usually challenging, but also extremely rewarding. And everyone needs to get ready, including you.

Getting Ready 

You already know your organizational culture. It's probably intentionally shaped in a certain way, or maybe it happens spontaneously, but in any case, there are already some common values, principles, and behaviors in your company that affect each aspect of your business, from employees, to customers, to public image. So the first thing to be done when working on the agile mindset is certainly to evaluate how close or distant your current culture is from agile values and principles. This is already an indication of whether you'll need months or years to embrace the mindset and how complex your journey will be. But anyhow, the change is inevitable, together with accompanying challenges. People will need to get out of their comfort zone, change their working ways, and maybe even reassess their inner values. And to make it possible, you'll need to ensure they're operating in a psychologically‑safe environment, which means they feel secure to discuss problems, admit mistakes, share new ideas, or ask for help. To make this more concrete, let's imagine your company is about to start a software project for an important customer. The project leader discovers a mistake in the already‑communicated project plan. This might not be the best news, but you can react in different ways, for example, charge the project leader, who could consequently hide problems next time, or welcome the opportunity to adapt to the new situation early. Build trust by discussing things openly, and help everyone learn from this mistake. In high performing psychologically‑safe environments, people are kept accountable for the results, but also the failure is considered to be a completely normal part of the learning process. However, even with established psychological safety, not every moment is the best one for mindset adoption. To get off to a good start, choose a moment that promises success without high risks, in other words, the moment that will provide enough capacity and focus for such a substantial change and will help to build a positive atmosphere around it. And then you can start to work on awareness. Let's talk about that.

Awareness and Motivation

One of my favorite books is Don Quixote, a wonderful story about a gentleman who decides to become a knight‑errant to revive chivalry. His intention is strong. He puts on shiny armor, mounts his horse, and leaves his village in search of adventures. But there is one big obstacle to his mission, everybody else sees things differently. For instance, he's fighting the demons, but everyone thinks they're nothing more than windmills. People in his village tried to convince him of his lunacy, they're even burning his books, the main source of his ideas, and so on, and so forth. If you haven't already, I recommend you read Don Quixote, but of course I'm not here to talk about novels, and I won't interpret the ending. I just think this story reflect what is very important in our context. The mission of a person is very difficult in the world where everyone else sees things differently, or getting back to our course, what they want to say is that the key to a good start of agile mindset development is people's awareness and motivation. If we want to have a change, everyone needs to know where we're heading and why this is good for them and for the company. I will leave it up to you to think about the best communication ways to raise awareness in your specific organizational context. Maybe you will think of meetings, training, coaching, or something completely different, but what I would like to point out is that this shouldn't be something reserved for technology teams only, everyone needs to be on board, complete organization, including leadership and customers. A common mistake is to leave customers out of these initial discussions with wrong expectations. This can impose huge limitations on future organizational mindset growth. Actually, it's not even possible to be agile without being on the same page with customers about this. Motivation is also something that requires a careful approach. Again, we'll not be diving into an explanation of motivational tools, which is another topic, but what I'm trying to say is that people need strong reasons to change their values, behaviors, habits. Everyone should have a clear picture of what the promise of the new organizational mindset is in a general context, but also personally, in the context of their own position or role. And this is something where you, as a leader, can make a huge difference. Demonstrate your own enthusiasm and commitment, but also find a way specific to your organization to raise employee's awareness and get general buy in.

Continuity 

Awareness and motivation are a good start, but their continuity is a necessity. Only continuity ensures that an agile mindset eventually becomes an integral part of the organization. To achieve this, old and new behaviors that reflect agile values and principles need to be continuously and transparently recognized, encouraged, and brought into the mainstream. To explain this, let's imagine a software developer sharing an excellent idea of integrating a newly released game‑changing alternative to already used software libraries. An agile leader would recognize this person is responding to change, rather than just following the plan, welcome to proposed idea, and share the example with others. In this way, people would get encouraged to react to technology changes more and bring their ideas to the table. But of course, one leader can't know the details of everyone's day‑to‑day work. Individuals and teams who manifest agile behaviors can help with this. Change leaders need to be appointed primarily based on their motivation, influence, the ability to identify, own, and act upon the problems, rather than their position in the organizational structure. As a matter of fact, role models within different organizational groups can have a strong positive influence on collective behaviors. Okay, now let's touch upon some tools which can further help with the continuity.

Being and Doing 

At the beginning of this course, we mentioned that being agile is different from just doing agile. We've also indicated their interdependency, but now we're going to talk more about that. When a company actively works on being agile by nurturing awareness and motivation, it's actually necessary to also do agile. If wisely implemented, agile processes and practices can even foster the mindset growth. And by now we've been discussing the kind of journey where an organization first gets ready, works on overall awareness and motivation, and then rolls out the mindset adoption with accompanying process changes. But this is not the only way organizations choose to take. Some encounter a situation where they need to reverse the journey and start with the processes rollout. Typically, they change the development processes first, and then through doing, they build in some agile mindset elements. Even though it's possible to make improvements this way, the reverse approach brings its inevitable risks and limitations. For example, by not understanding the big picture, teams may fall into a trap where they execute the process in a semi‑robotic fashion. Only by doing, they can bring some short‑term improvements, but they can also easily feel resistant, miss the main purpose and, not surprisingly, fail to deliver long‑term value. So how can we make sure to align doing agile with being agile in day‑to‑day work? Simply by empowering people to ask why. Everyone should understand the purpose of their actions. Remember Jerry? Do you think he knew the real purpose of the daily scrum. In the scrum framework, this meeting shouldn't be a status update, but an opportunity for teammates to openly discuss daily progress and impediments. If Jerry had felt this, I believe he would have decided to share his concerns easily. I hope you enjoyed the course. Thanks for watching and see you next time. We hope you enjoyed this course. If you're interested in more content for technical leaders, content we keep short and focused with the up‑to‑date information you need to be informed and make decisions but without getting buried in the details, find other courses like this at plrsig.ht/exec.

Devops: the big picture 

Hey everyone, my name is Richard Seroter, and welcome to my course, DevOps: The Big Picture. I'm a director of product management at Google Cloud and a longtime practitioner and student of DevOps. I've both lived it and observed it for almost a decade. Can you buy DevOps or is this just something you should wait out until the next IT fad comes along? I think the answer to both of those questions is no. No, in fact, in this course, we're going to learn all about the essence of DevOps, why you should do it, and what starts to change as you adopt this approach to delivering more customer value faster. Some of the major things we're going to cover include the pain points with software delivery that DevOps aims to address, what DevOps is and isn't, changes you'll see in your culture, your organization, and your practices, and some of the technologies that are part of a DevOps transition. By the end of this course, you'll have the skills and knowledge to explain what DevOps is about, and importantly, you'll know which subtopics you want to explore more deeply. I hope you'll join me on this journey to learn what DevOps is all about, with this DevOps: The Big Picture course, here, at Pluralsight.

Why devops is important 

introduction 

Hey there, my name is Richard Seroter, and welcome to this module in a course introducing the concept of DevOps. This is a big picture course, and so this module helps us see DevOps through a wider viewpoint. First, we'll talk a bit about the ubiquity of software and why being good at this matters. Why do you need to be good at software? We'll look at some of the software development and operational practices that a lot of companies employed today. We'll also do an overview of Lean. How does this apply to software? I'll introduce the concept of DevOps, look at some definitions. We'll discuss how you adjust to a more product mindset, which is a key part of adopting a DevOps frame of mind. And then I'll conclude with some data that actually proves that these DevOps practices really matter a lot. Let's get into it.

Why being good at software matters 

So we expect to find software frankly everywhere. Telehealth is pretty common nowadays, as well as electronic health records. With travel, almost the entire experience is digital now. You have e‑tickets. You do ride sharing. All of the experience of going to where I'm trying to get to is powered by software. Customer support, there's artificial intelligence, there's bots that are frontline support, let alone all the customer service software. Of course, even sporting events, getting my tickets, all the sorts of digital engagement that makes sports kind of even a software experience now. Of course, our day jobs, if we're in technology or working in the enterprise space, it's full of all sorts of software. Even just the classic shopping experience, the retail experience is taken over by digital engagement. So almost all aspects of our life have some way that software is designed to make it a little bit easier. If you look at some recent data, you can see the consumers are changing their behavior based on who offers a better digital experience. Customers are swapping brands with less loyalty than they had before. At the same time then, those businesses are realizing they don't have a whole lot of time to change their business model to be more digital, to be more software driven, or else they actually realize there's going to be an impact on the bottom line. You can interpret all this to realize that we, as consumers, have more buying power than ever, we care more about the digital experience when we're choosing who we want to work with, whether that's a grocery store, a sporting event, a bank, and those companies are now realizing that that digital experiences such a key part of it, meaning they have to get better at software.

Meet our Representative company, Globo antics results 

Let's use a fictitious company to walk through what sometimes the current state looks like for a lot of people, including yourself possibly. Let's talk about Globomantics. In this case, this company might be somewhat common to you. Maybe you work at a company like this. It's a midsize enterprise. It was founded a number of years ago. They're not a brand‑new company. They're in the B2B space, they sell to other businesses, it's not direct to consumer, and they've been able to function successfully for a while now. But all of a sudden they're seeing that some of their baggage isn't playing off well to a lot of new entrants in their space. These new entrants come in with different types of experiences. They're focused more on the customer experience, different types of software, and so all of a sudden Globomantics realizes they have to change. They can't rely on how they've been doing things entirely. They'd rather evolve a bit with the times. If you look at their IT organization, this may look familiar to you. They're organized in a fairly classic fashion. I've worked in this structure before. Groups are organized by discipline. You have enterprise architecture or development. You have business analysis in their own area. Information security takes care of their own business. Operations, QA. And there are some business units that have a mini version of this structure all to themselves. This kind of structure works fine if you're optimized for cost and efficiency, that's where this came from, but it doesn't work great when you're focused on speed. All of a sudden, a whole new set of pain points start to emerge that slow down the business.

Pain points with software delivery today and the results 

Let's look at these pain points in a little bit more detail. Limited automation is hurting Globomantics. They've got a lot of manual builds. It's hard to get infrastructure, and therefore, everyone asks for more than what they need. Resizing later is super painful. So in this case, that manual process is actually causing inefficiency and slow down. They've got instead of software‑defined networking, they've got ticket‑defined networking. If you need to update a load balancer or add a firewall rule, get in line. The networking team is overburdened, and it runs through tickets in a queue. Again, that just simply slows some things down because not enough of this is automated. Some of QA's test suites is automated, which is great, but code coverage is a bit spotty, which results in many manual tests. The result, you have really long regression tests when you want to make an update, and a backlog of testable code is just sitting in QA's inbox at all times. Building new environments for production, testing, performance testing, or even development is brutal here. Aspects are automated, but each one requires administrators to then manually stitch together different services and configurations. This slows teams down and forces them to test in environments that don't really even look like production. So limited automation is causing a lot of negative impact here. Let's talk about slow delivery. The company is struggling here. Delivery speed is not up. They're seeing competitors ship much faster than they are. Everything here seems to take forever, new apps, bug fixes, software upgrades, you name it. Going from idea to production, they say it's agile, but the minimum viable product looks suspiciously like a fully built solution. There's very little incremental learning happening here. And the flow is lumpy. There are piles of project proposals sitting in front of project managers. There are stacks of apps that need testing by QA. There's a truckload of code waiting for deployment managers to push it out. There's just a lot of different work sitting in front of different teams. Part of the issue here is a lack of self‑service. The company doesn't have much here. Most every test or production change goes through one team, and development teams aren't really empowered to do much on their own. Let's talk about change freezes. There are shopping holidays where no changes are allowed. The problem is as soon as that change freeze lifts, they dump hundreds of changes into environments at once, and then you can't easily find the offending code when something inevitably fails. So as much as the change freeze is designed to increase stability, it's actually making things worse. And of course, some of those Big Bang releases are not doing us any favors either. The incremental releases are tough, and everything gets bunched into these big releases to happen a few times a year, so everyone crams as much as they can into that, inevitably slowing things down even further. So because of this, they don't experiment much. By the time an experiment would ship, they've already moved on, so you end up with a lot of very stale, static sort of stuff versus experimenting and learning continuously because it just takes too much work to ship. Let's talk about uptime. At Globomantics, uptime problems are really hurting the company's reputation. They're causing some longstanding customers to start looking elsewhere. Specifically, there's a bad change, failure rate. Way too many changes result in downtime, and they seem to make the same mistakes over and over again. We know when this happens we get even more hesitant to ship, right? If it's hard to ship or if it's something where there's pain afterwards, what do we do? We do less of it. So it's slowing us down even further. When something does go wrong, getting back online can take hours, frankly, even days. The result is a panic because there isn't much confidence that the system can come back quickly. And that panic means that we get even more distracted when it comes to figuring out what is the actual root cause. We don't have a great playbook to run when there is an issue. Part of this is because the team is always starting at zero when something goes wrong. There's no head start. There's no shared telemetry I have access to. Everything starts with basic questions, and you have to rule them out first versus immediately going to where the pain is. We just don't have great telemetry. And because the telemetry we do have is super noisy, most folks frankly ignore the alarms and then miss the actual problems. So over‑alerting has exhausted the team. It's not helping our uptime at all. And so there's a lot of attention on architecting to prevent failure versus accepting that failure does happen and optimizing instead for recovery. Let's talk next about security. Sometimes it feels like we only really care about security when something bad happens. The problem here is that many of the security concerns actually still get offloaded to an information security team, and they get applied late in the game when you're about to ship some code. With more environments, including the public cloud coming into play, they feel the pain of credentials stored and secured in different ways in different places. We're not feeling good about how we're actually keeping our credentials and secrets safe. Container adoption is actually messing with the security approach at Globomantics because now it's not just about golden OS images in your operating system, but it's container images and then trying to avoid misconfiguration and drift as I have more things running out there, so I'm not really confident about the hardening or the configuration. Now the secure software supply chain can be a real problem now because many modern custom apps rely on a lot of dependencies. When we pull in JavaScript packages from npm, when we use Maven for a Java package, when we pull NuGet packages for .NET and like, we're not always scanning those confirming that we haven't actually just pulled in a bunch of vulnerabilities or check them after the fact. I mentioned cloud adoption earlier, and we're seeing a growing chasm between how we secure each environment. This is not consistent. We're seeing a bit of a drift in how we behave. And finally, there's a sense within the company that security is less important than speed. We pay lip service to security, but information security teams often get days or hours to approve an app before shipping without giving it real time to make sure that it doesn't introduce vulnerabilities or have any easy ways to have a security breach. Finally, Globomantics is suffering from a lack of customer focus. Some of these overarching problems really stem from the fact that we haven't started with why. So for many teams, they're thinking about their particular area, not the whole product. Specifically, each department has their own measure of success. Project management teams track delivery dates and budgets, QA teams can track how many defects they found, Dev teams are tracking their features, Ops teams are measuring their uptime, and so on. Everyone defines good a different way. And frankly, there's no shared definition of done either. Each team says they're done when their part is done versus when the product actually ships and the customer starts getting value from it, so no one team is looking at the big picture. A lot of bugs and defects seem to get postponed to later to achieve a date, make sure we ship on time. But given that here at the company many teams often disband as soon as that initial production release, these delayed bugs rarely actually get addressed without a whole new project spinning up. Given that the goal is to ship, the teams aren't often thinking about lifecycle management because it's hard to ship incremental updates. They rarely instrument the features to observe adoption trends or do A/B testing to improve usability. And even support is interesting here because systems are handed off for steady state operations pretty quickly, and that means that the general purpose support team takes any problems or questions, and that's okay, but all of a sudden you just have so many more layers before it's routed to anyone who actually knows anything about the system, you're not getting a quick resolution time. Instead, you're funneling it through a generic team. So where does that leave us? What do all these problems result in for Globomantics? Well, first, they have some poor customer satisfaction. Their customers don't feel heard. They rarely see software improvements emerging, and they worry about security breaches and uptime given a pretty poor track record of operations here. And look, revenue is down. Customers have options, we talked about this, and rarely have to use one provider or another. Globomantics is seeing customers churn, they're seeing new signups decline, and you're starting to see the attrition is up, and many exit interviews are showing it's because employees are looking for growth and impact, and they're not seeing it here. Some of this is stagnant innovation. You're losing people, you're handling fire drills constantly with production systems, and there's simply less investment in really bold big bets. This is all causing more of a lack of faith in IT. That lack of velocity, any of the impactful digital experiences that we're seeing from competitors, some of these teams instead are just self‑sourcing their technologies. They're hiring contractors. They're building shadow teams just to get something done. And overall, it's messing with the culture. You're having this us versus them mentality, and we're focused on ourselves versus the customer, and it's impacting how we're performing and really hurting morale. So all of these things are common in a lot of places. The key is, how do we break out of this?

Lean, Theory of Constraints, and Inspiration of IT

For a lot of teams, this is about rethinking delivery, frankly, and we can look at different places for inspiration now. Look at manufacturing. This isn't necessarily a place we think of when we go for wild innovations, but if you look at even the last 50 years of manufacturing, all of a sudden companies like Toyota and others have reinvented how they approach a number of different things, how they think about inventory, how they think about flow of units through their system, how they think about waste, how they think about keeping their people motivated and sustainable pace, right? How do they zoom out and view the entire flow of this system? Actually, we have a lot to learn from how manufacturing works and converting that to software. And not to say that software isn't a creative task, sometimes it feels more art than science, but there's a number of things we can learn about the process of delivering that software that doesn't take away our creativity and instead simply just adds an improvement to the flow of work from team to team out to the customer and focus mostly on value. A huge part of successful modern manufacturing is about waste. And I think this is going to be an interesting conversation for us to have here because so much of this is something we don't think about, I don't think in tech, as much as we should. Look at knowledge waste. What's disrupting the flow of knowledge in an organization and then even the absorption of knowledge? But if you look at the flow, hey, constant reorganizations, teams that don't talk to each other, software systems that don't talk to each other, right? That breaks up the flow of knowledge in an organization. You're wasting things that we should know. And there's things that disrupt the absorption of knowledge, overstuffed PowerPoint presentations, constant review meetings, super complex reports that no one reads. All of this is wasting knowledge. Look at waiting waste, a terrible one. This causes some uneven processing. One team is going faster than another, and now we have unpredictable flow through the system. Sometimes you have specialized personnel, they can only do one thing, so they're sitting here waiting for something to happen so they have some work to do. We can see this all the time in tech. You're waiting for code to finish before you can test it, you're waiting for environments to be created before you can do anything, waiting for approvals before you make changes. All of this waiting is one of the worst. Over‑processing waste, doing more work than is necessary. We do this because sometimes we don't know the requirements. How can I define gold plating a feature if I don't even know what the customer needs or writing long documents that nobody reads, giant specifications that don't matter. So over‑processing waste, again, gets in the way of us delivering value. Even motion waste, another one talked about in manufacturing circles. It's toil on those who create the product. Sometimes you fix this by physically rearranging a workspace or changing your technologies or changing your process. Some examples of motion waste in tech could be long nights pushing software, repetitive manual testing, or even ergonomic things. There's a few more wastes that matter here. Transportation waste, moving products in ways that add no value, right? The moving costs, the lost time, the misdirection, damaging in transit. Even in technology, we can see this. If we're clumsily moving code between environments for no good reason, I'm wasting my time. I'm transporting things that I don't really have to. Correction waste. This is the idea of handling defects. So sometimes we rush to hit a deadline, even in manufacturing so we can say we're done and we ship this batch, but too little emphasis is based on prevention. As we know, especially in manufacturing, defects after the fact are extremely expensive. So if I have to spend a ton of time after the fact correcting things, I'm dramatically increasing my cost and wasting time. Look at inventory waste, those things that pile up and aren't producing income in manufacturing. Inventory is evil in a Lean world. It leads to other waste. It means you don't have good flow. It's lumpy processing. If something goes wrong in item 5 of a batch of 100, I don't know until I've done 100. So inventory and manufacturing is not something you're trying to have. Same with technology. And then finally, overproduction waste, making more than you need to be, asking for more capacity than you need. Sometimes this is done to keep people busy, right? We produce more than we need just to keep going. But sometimes the setup, the work, that work can be completely unnecessary, and again, we're wasting time. We shouldn't build items when there's no orders or produce things in giant batches when somebody only wants one. So all of these different ways, these eight ways that I talk about, are pretty popular things discussed in the manufacturing space, and you can squint and imagine how many of these apply back to technology. So what's Lean all about? I mentioned it in passing earlier, but this is really a way of thinking about customer value in manufacturing, thinking about ways to define the value stream, improving flow, and those sorts of things. So some of the essence of this is focusing on value. The goal of Lean isn't to cut cost, but actually to free up resources so we can focus on adding more value. That first question you ask yourself in many cases is, what value are we adding from the customer's perspective? There's a lot of attention in Lean on time. We just talked about waste. It's a huge part of Lean. You're working systematically to eliminate all the non‑value‑added processes in order to achieve your goal with the least possible effort in waste. Cycle time comes in big here. How do I reduce how much time it takes to accomplish something? This is why one‑piece flow is the ultimate in Lean production. It's eliminated most of Toyota's eight kinds of waste that we talked about there. How do I almost send one piece of thing through the entire process so I'm not overproducing, I'm not wasting time, I'm able to have clean flow through the entire process? A lot of this too is about continuous incremental improvement. Lean's about not just having one process written into a book and everyone follows it. It's making sure we're always learning using data, making sure that we are constantly improving the process, finding new ways to eliminate waste. And this isn't about burning out your people. Frankly, the opposite. Eliminating waste is actually just one third of that equation of making Lean successful. Eliminating overburden of people and equipment and then eliminating some of the unevenness in the production cycle is just as important. So how do we make sure our people aren't overburdened? And finally, some of this applies to the theory of constraints, which we're going to talk about in a moment, which is how do I think about the right places to spend my time trying to optimize? Let's talk about that. So with the theory of constraints, you might have a process here, as I'm showing, where there's a couple of hotspots. These are bottlenecks. And in the theory of constraints, you look at this and say, look, the throughput is only as fast as the constraint, right? If I have a pipe and it shrinks to a tiny pinhole at one area, the flow of the entire pipe is the amount of that pinhole. You can imagine the same in technology, right? The constraint controls the whole flow. The whole throughput of the system is pinned by that. Now what's fascinating here though, if the theory of constraints looks at this and says, well, if I optimize before the bottleneck, before the constraint, then all of a sudden all I'm going to do is create excess inventory that's going to just sit and wait in front of that bottleneck. You can imagine if my bottleneck in IT is the QA team, but then I spend my budget this year on giving my developers better laptops, then what's the point? All I'm going to do is pile more code in front of a QA team. That's my constraint. That is my limiter of throughput. At the same time, same with goes after. So again, if my bottleneck is not to pick on QA, but if my bottleneck is QA and I've invested all my money on this amazing continuous delivery software that runs after I've tested my code and packaged it, what's the point? That thing is just going to be starved for work while waiting for that QA team to process. So the idea here is that we want to elevate those constraints, spend our time fixing one of those. That could be a person, could be technology, could be a policy, could be a part of waste. How do we focus on those constraints, then move to the next one, and not accidentally spend our time on areas that don't actually improve throughput of the entire system?

How lean relates to software delivery

Now you may not have thought you signed up for a Pluralsight course on manufacturing and Lean, so how do we actually connect this back to software delivery? Can we do that? Of course we can. So some of this is reorienting towards bigger picture product thinking, right? Not just silo by silo or team by team or component by component, but true value stream considerations. It's about removing wasteful processes. Find those rubberstamp processes. I bet you have one in your organization where it seems like 100% of the time the manager approves the thing. Why do you have that step? If 100% of the time it's approved, it can be automated or eliminated, frankly. Remove things that don't explicitly add value. Take a ruthless view of it and say, how can I eliminate waste to improve throughput and make my customer happier? This one can be tricky, but might be one of the most transformational things you do, which is delivering in small batches. Learn faster, experiment, introduce less risk and change into production at one time. This is frankly a security benefit. If I can ship quickly, that means I can patch quickly. So small batch delivery and development might be one of the most transformational things you can do in your team. Now, at the same time, continuous improvement is key here, of software, of process, of our skills, right? This isn't about having fixed standard operating procedures that we just follow into perpetuity. Instead, it's about constantly figuring out, how can we make the process better? How can we enjoy this more? How do we make our customers happier? That's a key part of how this can change what we think about software. And then automation is to remove some of the overburdening of staff. Get rid of toil, repetitive tasks, things that actually make our job not fun and probably introduce risk. We should all be spending time on the things that matter the most. And then attacking some of those bottlenecks. When I take a system‑wide view, you start to see problems with flow that don't emerge when each team owns their little piece and optimizes locally. Instead, I'm looking at that bigger picture, and I'm not just saying my definition of done or my definition of good. We're able to look at the whole thing and deliver software that delivers value.

defining devops

So what then is DevOps? Well, let's look at a few definitions that I like. First, this one from Gartner Research. One of their public definitions of DevOps is it represents a change in IT culture, focusing on rapid IT service delivery through the adoption of agile, lean practices in the context of a system‑oriented approach. Talks about culture here. This isn't just technology. And the goal though is rapid delivery, its value. Let's look at Amazon Web Services. What do they say? Well, they say DevOps is the combination of cultural philosophies, practices, and tools that increase an organization's ability to deliver applications and services at high velocity. Speed again, right, but we're also seeing that mix of culture and technology. What about the folks at Puppet, some of the early creators of DevOps tooling? They say DevOps is a way to release better software. It's not just technical tools or workflows, it's also a cultural practice. DevOps produces better software faster by aligning development, staging, and deployment. This is talking about teams coming together. Again, value matters and speed matters, but it's also happening by improving things upstream and how teams work. Finally, Atlassian also does a lot of things across the DevOps toolchain. They say that DevOps is a set of practices that work to automate and integrate the processes between software development and IT teams so they can build, test, and release software faster and more reliably. Again, we're hitting on speed, reliability, collaboration, practices in tech. This is all how it comes together.

moving to a product mindset

Now one of the callouts I want to make on DevOps, and we talked about this before, is switching to that product mindset. If you get a chance to buy this book for your manager or yourself, buy Mik Kersten's great book, Project to Product. It's a revolutionary way of rethinking this process. And really, if we look at the project‑based mindset, how does this work? Well, we fund projects based on milestones. Alright, here's a first release or here's a GA or whatever it is, maybe two versions, and we're going to pay so much money for this 3‑year project. The success of that, honestly, is based on time and budget targets. Did we ship it on time? Did we spend the appropriate budget? Often, what you'll see here is teams get assembled from staff, and they're allocated for the term of the project, right? These teams come together. They might be working on more than one project at a time, but they're coming together just for this project. Some of the durable knowledge lives in the documentation. Because that team goes off on their own after they've shipped this version, oftentimes, the one thing that stays the same is the documentation, the requirement specs, the architecture specs. A project typically has a defined end date, and there's a little attention on post‑release lifecycle. If you have to do a major v2, that's a new project. That's new funding. That's a new budget. That's a new team. This is pretty common stuff. That's what I did in enterprise IT as well. This isn't unfamiliar, I'm sure, to most of you, but a product‑based mindset kind of messes with your thinking. You're funding based on the output. What is the business output I'm after? And I will fund the team to accomplish that. The success is based on business value. Am I improving business metrics? The things that actually hit the bottom line and get mentioned in the annual report, is it those sorts of things? Teams are comprised of multi‑disciplinary staff, and they're dedicated to one product at a time. These are not people who are shared among different products. This is a more durable team. Doesn't mean it's a 10‑year team, might not even be a 1‑year team. You might cycle people every 6 months as well. But the idea is that there is a single kind of staffed unit that works on this product and continues to work on it, not just after they've shipped one time. And where this gets interesting is now some of the knowledge is within the team. The team doesn't disband once the work is done. The team might be defined for 5 years. And while the people may change, the team itself maintains some of that institutional knowledge. And the key here is you're looking at a multi‑year lifecycle. You're not looking at a release. You're looking at always shipping value, right? You're learning from the last release, funneling it into the backlog for the current release. And again, would you do this for every product? Of course not. Every single piece of software in your organization doesn't deserve a product team. That would be wasteful. But there's a number of things you can imagine, I'm sure, that deserve sort of ongoing care and feeding that should be staffed by a dedicated team.

What devops is not

If I may be so bold, let's start to wrap this module up by talking about what DevOps is not. Look, I don't think DevOps is a team or a methodology. Could you have a DevOps team? Sure, but the idea is it's not just a group that I've renamed, it's not the QA team that's now the DevOps team, it's not the production team that's now a DevOps team. Oftentimes DevOps is a way of working. It's not a particular group or somebody with DevOps in their title, nor is it a methodology, right, it's not, I don't do DevOps for this project, it doesn't work that way. Frankly, it's not something you buy, even though some products put it in their names, you can't buy six units of DevOps, it doesn't work that way. This is something that's a combination of things. Look at those definitions, culture, practices, tools, it's an approach. Frankly, I don't think it's an IT‑only effort. This is something that impacts how you deliver value to market for customers. I think everyone cares about that. This impacts finance, this impacts your lines of business, this impacts all sorts of things. So if you're just doing DevOps as an IT initiative to add some automation to something, that doesn't count. It's useful, do those sorts of things, but you're not doing DevOps just because you use Terraform to build infrastructure, that's infrastructure automation. That's terrific. Has that changed how you deliver value of software? Probably not in isolation. So DevOps is not just, hey we use Ansible or Puppet, and we use Terraform, we're doing DevOps. And then frankly, look, DevOps is not easy, I'm not sitting here telling you that switching from how you do business today to doing this model is just a snap easy thing to adopt. This is hard, there's legacy systems, there's people changes, all sorts of things. I think the value is there though and it's worth doing, but no one should set off on this journey assuming this will be simple.

The data that proves Devops matter 
The accelerated state of DevOps report is one of the longest running surveys of DevOps that exists today. And the latest data that's come out talked about the difference between elite performers and kind of low performers. And look, the elite performers, those who are doing DevOps‑like practices, are shipping almost 1000 times more often. I think in any industry, if you are delivering value to customers and fixing things that much faster than your competitor, I think you're going to win because you're simply responding to customer need better and building more stable systems. But interestingly, these high performers aren't just shipping more, they actually have a lower Change Failure Rate, which is fascinating to me. So you're going faster, but you're actually doing this safer to some extent. And so again, that's a huge thing that builds trust within the team, that's going to make sure you're confident to ship more software. And then the last number may matter a ton as well to your business clients. As you look at this report, you see that security best practices, documentation best practices, overall DevOps practices impact the bottom line. We can see that connection between teams that are delivering software in a successful way and companies that are then becoming more successful. I think that's exciting. So in this module, we talked about the ever‑present software experience, why customers are basing more of their decisions on the digital experience. You and I are doing that, right? We're making decisions based on that and our companies then that we work for need to be good at this. So many of today's teams struggle with speed, uptime, security, you are not alone on this at all. This is hard stuff. A lot of this does start to change though when you truly adopt a customer focus and think in small batches. We can learn a lot from manufacturing, how they made those improvements to increase quality, improve flow, removing waste, finding bottlenecks, so important, even to our software teams. As we start focusing on customers and products versus projects, we start to see what DevOps really is, which is applying this idea of delivery, and throughput, and value to the customer, not just looking silo by silo. As we figure out that product mindset, we're going to see a transformation from just how do I ship my piece, to how do I deliver a durable product that lands in the market? I think the data proves this matters. You can look at survey data, you can look at this anecdotally, you can go to conferences and see who presents some of their progress. I think that this general way of working, whatever you want to call it, makes a huge impact on teams.

Adopting devops culture, practices and technologys 

Introduction 

Hey there, my name is Richard Seroter, and welcome to this module in a course introducing the concepts of DevOps. This module takes a look at some of the key changes you should expect to make to succeed with the DevOps approach. So what are we going to do together? Well, first we're going to explore some of the cultural changes that accompany a DevOps transformation, then we'll look at some of the organizational changes that happen when you apply DevOps, we'll look at some of the new processes that you're going to engage in when you start doing DevOps, and finally, we'll navigate some of the technology toolchain that powers that DevOps transformation. And this won't be a deep dive into each one of these, this will really tee up some things you'll see in the rest of the learning journeys here at Pluralsight for DevOps, but this should give you a great overview of the primary things culturally, organizationally, process‑wise, and technology‑wise that you're going to experience with DevOps. Now let's remember what this is all about. If we're doing DevOps, we're trying to do continuous improvement of delivery of value to customers, how are we constantly learning, how are we constantly improving, and so it's important to remember the goals are continuous improvement. Are we focused on the customer? Are we focused on value? This isn't just about automating infrastructure or things like that. And, in fact, you can't buy DevOps. There's no unit that you can buy of six units of DevOps to make your business better. That's not how this works. So DevOps does involve things you acquire from a technology perspective, but it's also about skills and the approach that we'll talk about in this module. And finally, it's really important to remember that any sort of adoption of any of these sort of approaches, including DevOps, is not some big bang, the next day you wake up and you're doing DevOps. This is an incremental improvement. This is something where you're going to be learning in stages as you pick up new technologies and practices and things like that. So this is a journey that's not going to have a defined start and end date. It's going to be something we keep getting better at with the goal of delivering better software to customers faster and making them happier and focusing on value.

The culture changes that are part of devops 

So as I mentioned, we're going to focus on four areas, culture, organization, practices, and technology. Let's start off by looking at culture. What are the cultural differences? What are those things that are going to have to be innate in your team and how they work? First is a piece of ownership. Ownership is a big part of the cultural change that you're going to make. You get to see a shared commitment to excellence. Quality doesn't belong to the QA team. Customer success is the responsibility of the sales team, engineering team, support team, everybody. In a manufacturing line, anyone can stop the flow if they find a defect. We step up and agree that we're all responsible for things like quality and excellence and ensure that it's happening. Now you can say shared commitment is the opposite of ownership, and that's fair, but my real point here is that we're not passing the buck. We all agree on what's important, and there are even positive social pressures to step up and call out bad form because we're all trying to make sure we're delivering a great service. Empowerment, though, requires accountability, right? If you empower employees to do what's needed to maintain a service, that's awesome. And at the same time, there's accountability to make sure you deliver on that. So any of us can sound the alarm if there's a problem. In a crisis, many of us take on greater responsibility to resolve the issue. And so that accountability is important though. If I'm allowed to do more, then you also take responsibility if something goes sideways. And if you own your product, you're also accountable for its success. So part of this is taking on more responsibility. Now to do this, of course, this does require trust and safety. You need to place more trust in the individuals and teams and not have a command and control structure that pushes all the decision making up. It has to be safe to fail or else people are going to avoid doing some of the worthwhile risk, so this ownership piece is fundamental. We also start to adopt product thinking, and part of this is simply orienting towards outcomes. You'll see that it's not about the tech or the project, but about the outcome you achieve and the impact that it has. But to do this, this requires some deep understanding of the customer to truly think with a product mindset. You want to be understanding what your customers are doing. Who are they? Who are your non‑customers, who haven't you gotten, and what motivates all these people? This does represent a change though. This isn't how IT has typically operated. We often focus on the project, the deadline, the milestone. Of course we want the result to be net positive for the user, but in my experience, that's not always the foremost priority in an IT project‑centric culture. When you start doing DevOps, you often elevate thinking about the service you're delivering and how to do that best, so you're thinking fully of the product as you get into a DevOps world. And we just talked about this in parts, but let's dig a little more into this customer orientation. One way to do this is thinking about jobs to be done. This is a great mental framework. When we buy a product, what we're really doing is hiring it to help us do a job. If it doesn't do the job, we fire it. It's the whole I'm not buying a hammer, but I'm buying something to hang my picture sort of mindset. If you're using a CRM system, you've hired it to keep track of your customers. Understand the customer by thinking about what jobs they're hiring your product to do, and that product might be an internal help desk, could be a Kubernetes platform for your developers, or a consumer‑facing service. So products take on different dimensions even within IT, but you're still thinking about what is the job that your customers are hiring your product for? Now, of course, to do this, this involves more regular customer interactions. You shouldn't just be making up jobs to be done out of thin air. It should be based on real user studies, observations, conversations. This only happens when you spend more time seeing what your customers need. And arguably, what matters most here is actual feedback loops. I don't want to just hear this. It has to actually be implemented. The name of the game is continuous improvement. It's not good enough just to hear it and collect feedback from customers. There needs to be a real way to incorporate that, learn, and continue iterating. So data‑driven decisions play a big part here. The continuous improvement in DevOps needs to be data driven; otherwise, we might not be improving or improving the wrong thing. So things like guesses or gut feels don't drive decisions. We've all made decisions because it felt right. I sure have. But for something to be repeatable, it should be based on data. If I ask 10 people what to do, would I get the same answer from all 10 of them because they were using the same data, or would that be very different? Ideally, it's the same. And this is quantifiable data. This means you have to actually acquire it, right? I have to go get it. It's hard to improve what I'm not measuring. What's good if I can't define bad? And so this is going to require some new input sources. This means that we might be starting our efforts by starting to collect data. It's okay if you don't have it; no better time to start than right now. Your data sources could be surveys, instrumented code, sales data, all sorts of things, but I'm collecting data that's actually helping me make confident decisions. We've talked about this a lot already, and you can't discount the importance of it though. Continuous improvement is key. Changing the culture to one that constantly learns and incorporates that knowledge is critical. So it's not just about an annual innovation week and reviewing reports every month or so. It's about creating a steady stream of feedback that you can act on. We don't just need more info though. We often need the context, the ability to turn that information into usable data and knowledge that drives changes. As I mentioned earlier, it can be hard to fix something I can't measure. If I'm not doing this already, it's okay. Let's get started. Let's capture data. Let's determine what metrics actually matter and will change your strategy. Figure out those baselines. Because part of this is being smart about where to focus. If you spend your time improving something in the wrong part of the value stream, you won't make much of a difference. We're all time constrained, nobody has infinite resources, so we need to use data to know where to target our improvements. This is where small batches and rapid feedback cycles help as I can experiment and see if my hypothesis is right without investing too much. So continuous improvement is key, but the smartest people are doing this in the best places and not just randomly making changes and fixing things. They're fixing the right things.

The organizational changes that are part of devops 

So we looked at some cultural changes, now let's look at what actually has to change logistically within the organization to make DevOps successful. First, we're going to be thinking about the team staffing and structure. Am I organized correctly to make it possible to succeed with DevOps? Now IT is often organized by functional teams. Often we group these similar skills into distinct groups, here's the project managers, the business analysts, the database administrators, the developers, here's the architects, there's the testers. The challenge is this can result in things like waste or uneven flow. While this can feel efficient, because you're avoiding duplication, which is fair, it's also suboptimal when your focus is on flow and customer outcomes. I can have waste as people wait for their domain to kick in. There's no work for the DBA there twiddling their thumbs, or it can have uneven flow as work piles up for a specific team, they might not have the same staffing level as the team pushing the work. All of a sudden QA gets a bunch of things dumped on them, everybody's waiting for that, and visibility can suffer as each team is optimizing locally. So some teams do adopt a more product‑based structure. Now, this is not a trivial change by any means, but it's worthwhile for many. In this context, you're arranging dedicated people into long‑lived product teams, and this cross‑functional team designs, builds, deploys, and runs their service for customers. That means that these are long‑lived teams, that means they're going to survive, they're not just project‑based, and these teams have dedicated people, they're not split among 10 different things, they're dedicated to that product, and they're cross functional. This creates faster flow, more customer focus, and this doesn't work for every company, definitely not every product, but it is a key structural option you should be looking at. And, of course, you need to look at your people. Do I have the needed skills for product development and so forth? Now many orgs don't naturally have all these skills, that's fine, it's about continuous learning, right? So if I'm missing the product management skills or automation skills, you'll likely use a mix of hiring and training to develop them. Adopting DevOps can be super scary for an IT org and I get that, but we need to recognize that this is also a tremendous chance to do more interesting work that matters, so it should be an exciting thing for most of your team. We also need to rethink budgeting and funding, which is, of course, super exciting stuff, but when we think back, most companies fund IT projects, not products, right, adopting a product and customer‑oriented team structure can't happen in a vacuum. Most companies I speak to don't fund products in IT, they fund a bunch of projects that have a start and end date, right? They've got milestones, goals, deliverables, and a mix of dedicated and borrowed staff. But how can I fund outcomes? How can I adopt a new model that's about funding the outcome I'm after? Do I have a new system being used by supply chain partners? Maybe a dedicated product team would be focused on goals related to reducing friction for tracking goods, or increasing visibility of the whole supply chain, or driving adoption by more customers, fund those outcomes you're after. So sometimes fixed annual budgets don't facilitate continuous learning. This can be hard to do this sort of product‑based culture with a very fixed budget. Nobody can tell you with a straight face how many virtual machines they need next year, right, we're always guessing. As we learn about our needs and what technology is capable of, we're going to need some flex to spend money in different ways. Now, of course, I'm not proposing anarchy here. We need budgets, we need predictability to run a responsible business, but as we look at funding an outcome with expectations, then they come with staff and resources, we want some flexibility with the team to redirect some of the funding as they keep learning and opportunities to adjust up and down, funding on a more frequent basis. So really this is about more flexibility, some teams are even doing internal venture capital like models where you're funding a certain goal and going off and trying to do that, just changes the conversation from just here's some big, fixed, up‑front planning that happens that locks you into a 12 or 18‑month plan, that's really hard to do as you're trying to do a continuous learning model. Another huge organizational change is putting some attention on the bottlenecks and the tech debt. Now again, it's key to know that it is ineffective to optimize anywhere but the constraint. The organization can't just focus on new features and products, it's about the overall value stream. You need to create a mindset where the organization looks at the complete value stream and optimizes that constraint. Spending time anywhere else doesn't really matter. Now what is a constraint? Again, a constraint can be all sorts of things. Maybe it's a lack of people at a key stage of your delivery process. Maybe it's a process that reduces the flow to a trickle to production, or it could even be something outside the company, like poor product demand or regulatory limitations. What is the constraint? Work on that. Now to do this, we often have to label all the work, right? The work you're going to do to improve flow and improve system quality needs to be visible. It should be part of software sprints and broadcasted to leaders in the organization. This isn't secret work. That sort of work needs to be elevated, not just new features and products. And thinking of paying off debts is key, the debt that impacts your systems are slows down your delivery can't just be paid off overnight. There's no magic product that just fixes it. Introducing Kubernetes or cloud or serverless doesn't change your debt problem. As we continuously improve, we'll chip away at that using refactored systems, automation of processes, many things, but the organization needs to start thinking about the bottlenecks and tech debt and spending real time on that, not just when we have free time. Now you may also introduce new types of teams to your organization. When you think of platform teams, they improve software delivery with workflows and platforms, you might add one of these. These dedicated groups enable your customers, not only developers, to achieve what they need to. They might be running a container or cloud platform, but also building capabilities teams use to deliver services to their customers. This could be CI/CD tools, developer kits, and more. What's unique here is platform teams treat the platform like a product. This is a big difference from traditional system administrators who often have to support many products and many times the focus is on keeping them healthy and running. A platform team does that for a single platform, and they use product thinking and customer interviews to manage a backlog, continuously scale and improve, and focus on outcomes for application teams. Now you might have also heard about site reliability engineering. SREs build resilience in product teams and they actually may share in some of the support responsibilities, right? This is a fairly new idea, it started at Google, but these teams often build systems and services to support product teams and products. Their aim is to help teams build reliable systems with measurable goals. Oftentimes they're part of the production team for a given product area. So consider a bunch of different types of teams as well that might make your DevOps adoption more straightforward. Let's talk about opening up communication within the organization. Again, as we think about organizational changes, poor communication can lead to push and not pull‑based systems. When there's spotty communication among teams, we often end up pushing work to different groups. They might not even be expecting it. The team on the receiving end then has to handle that pushed work, and then the pushing team might have to be stuck waiting around for that receiving team to act on it. We want pull‑based systems where there's open communication and flow of work, and each team pulls the work when they're ready, and to make any of this stuff work, feedback loops are critical. Part of the secret is offering two‑way communication. This isn't a broadcast, it's a conversation. So, is the organization being more open and communicative? And this can be improved lots of ways, right, some of this is tools, practices, and frankly even just intent. Chat systems in the enterprise are more common, which is great. So it's often about just defaulting to open with many of the planning and design docs, issue logs, things like that, and encouraging the sharing of knowledge, not hoarding it. Finally, we want to revisit how and what we reward, which relate to the point I just made. So don't just communicate your values, but live them and prove them, right? We've all seen or worked at those places that prominently display their values at the office. They include these aspirational words and fancy pictures, and really hard to argue ideas, and those are nice, but we all know that saying those things isn't the same as living them, that's a lot harder. So I think we know you get the behavior that you reward. We know this, right? I've seen research that shows that humans actually avoid pain more than they pursue happiness. So what gets punished at your company? What's praised? What sort of thing results in someone getting a promotion or a shoutout at the all hands? And what gets someone fired? So here, we want to discourage hero culture, and DevOps mixes this up a little bit. Your organization can't celebrate the hero, you know, the person who somehow always swoops in and saves the day during an outage, they work those long hours to get the feature done when other developers couldn't do it. On the surface that's good behavior, but in many cases the firefighter is actually the arsonist. The reason that they have to keep coming to the ops team's rescue during outages is because they won't actually automate a certain step or show others where the system flaw is, or they aren't mentoring other developers because they like feeling indispensable. This is no longer an asset. Rather, I want to give bonuses and promotions to those that sincerely up level others. They actively mentor and make themselves replaceable. They focus on doing something that helps the customer more than just doing something cool in the software system. They create cross functional teams to solve problems versus solving it by themselves. They're doing a number of things that are focused on sharing knowledge and making everyone better, and that can be tricky and harder than maybe even what you're used to, but that's how this becomes sustainable.

The practice that are part of devops 

We've talked about culture. We've talked about organization. Now let's talk about some of the practices. What are some of the tangible things you're going to find yourself doing in a DevOps world? Well, first we're going to be scaling agile delivery more. Agile and DevOps go together. There are things in common here. They're both about software and improving delivery. DevOps stretches some of those agile ideas all the way to production. Agile alone doesn't get software to prod. It helps me create smaller bits to release, but I need the rest of the value stream to improve, too. It's about delivering value completely, including things like continuous delivery and continuous integration, infrastructure, automation, and more. Agile software teams could, in some cases, actually evolve. The team doing agile software delivery may evolve to take on the whole product. I've worked in this sort of setting before. The software team actually took on overall product lifecycle responsibilities, and we staffed accordingly. So keep investing in Agile. This sort of model is going to lend itself well to continuous improvement throughout the process. We're also going to be thinking about a small batch delivery process and actually doing this for real. This is a lesson we can learn from Lean. In Lean, the ideal batch size is one, one‑piece flow, if you will. This gives me the best chance of fixing defects before creating more items. It leaves no inventory and helps you deliver what is needed at the very moment. How does this apply to software? With software, a small batch release means you learn faster. I'm shipping just small bits of code each time I can. As soon as it's committed and production‑ready, I deliver it. So I'm getting that feature, fix or experiment out quicker, and I'm limiting the defect impact if anything goes wrong. I know right where to look. If I just update one thing and something gets weird in prod, I know where to look versus if I ship all these giant changes at one time, that's a mess. So this starts to change my feature planning as well. Instead of planning annually or quarterly releases and stashing as much as I can into those releases, I'm actually going to be thinking in very short‑term horizons. What can I work on and ship today, this week? Is there a single feature we could all work on, a one‑piece flow? Now this may impact how you do source control strategy and lead you to use trunk‑based development where everyone checks into the mainline versus feature branches that have to be merged later on. Now the only way this works is if deployments are basically free. If it requires feats of strength to do it, you won't do it every hour or every day. That makes sense. You've got to drive the cost of deployment to almost 0. This means trustworthy builds due to great test coverage and reliable automation and then no downtime deployments. So getting into a mode of small batch delivery is super impactful, but it does require other changes in how you deliver to make it possible. Service‑level objectives can play a big part in focusing your DevOps work. An SLO is what you've agreed measures the performance of a service. A service‑level indicator is a particular measurement. A service‑level objective defines the objective for that measurement. An SLA, a service‑level agreement, is the agreement with the services client. So for example, an SLI might measure HTTP response times or even system uptime. The SLO goal might be 99.99% uptime or 10 ms of response time, and the SLA is the contractual promise with the customer. A really crisp SLO helps the team know what goals they're trying to achieve. It focuses the team. It doesn't become about one component in the system, but about a measurement of the customer experience that can then be shared and agreed upon. You'll also hear this term error budgets as a way to use the time outside that SLO, let's say, the .01% of uptime to experiment and not for your downtime. So this is giving the team freedom to work and understand what good looks like. But, of course, to do this, we require a level of understanding. Most stakeholders I've spoken to ask for things like 100% SLOs, but what you need to truly understand is the expectations of that customer. Have data that proves what's really needed and explain the cost of anything that's actually harmful. 100% uptime means you have no time to absorb downtime to try a better deployment or fix the system. That's actually a negative. So this requires real, true understanding of customer need, great open communication with stakeholders, but this can be a very, very powerful way to focus your DevOps work. Another process that really matters here a lot is actually expanding your test automation and getting continuous integration and continuous delivery to your deployment chained up for your company. So continuous delivery with high quality, that's the goal. Automation can be super important here. I can't reduce waste by just throwing more bodies at it. Automation can be super important, and I won't trust those automated deployments without trusting the test. Trust is key. Trust comes from multiple things. One of those is tests. If I don't think you have good test coverage, I'm not going to trust the code that can be deployed without any manual intervention or checks. It does start with tests. Unit tests, acceptance tests are important. So I need some good test coverage in the code, not 100%. But ways to also do outside in testing of the service post deployment, I really need a smart testing strategy. And so just like everything else, I need to treat that test suite like a product and continuously improve it. I have to care for it. Regular pruning of tests I don't need, improvements to ensure it stays green, relevant, and trustworthy. Finally, let's talk about making work visible. This the last practice we'll cover in this section. But, here I want to talk about visualization. Lean and Agile promote visual management, enable and control communication without needing words, see problems, observe waste, see the pileup, shorten your lead times. Some of this is by shining a light on all the work, not featured development as we talked about earlier. Everything in your software app and product team deserves visibility. Bug fixes, automation updates, resilience testing, all of that should be visible. And so whether you're using Kanban boards or something else like that, the critical thing is to keep it updated and actionable. The value here is the timeliness. To trust it, it has to be accurate. If I'm looking at a board and something says it's being developed and it was shipped three weeks ago, I'm going to lose all trust in the whole thing. This should be an accurate view of my work in progress. I should be able to see what's going on, where my bottlenecks are, what's being released, what are my priorities by judging by what we're working on, and that's helping us constantly improve outcomes and make great decisions.

The technologies that are part of devops 
Finally here, I'm going to be taking you through a whirlwind look at the technology that becomes a key part of a DevOps transformation. You'll see lots of other Pluralsight courses that dig deep into each one of these areas, but I want to give you an overview. I wanted you to be thinking about the major categories, major considerations, so you'll even know where to focus your time. So for each one of these I'll look at the what, the why, the how, and what tools might play a part here. So let's talk about value stream mapping. This is a lean method for visualizing, analyzing, and improving the delivery process. So that's what this is, it's a way to look at the full value stream. Why do you do this? Well, you do this to identify some of the bottlenecks. If I want to have cross‑organization collaboration, have some context about how I deliver value, and I do this often by talking to stakeholders, collecting the data, and then turning that into something visual in many cases, even figuring out the metrics that track what is the process between these, where is my wait time, what does good look like for the whole thing? So sometimes this can just be rough on the whiteboard, sometimes this will be in fancy tools, but the idea is I'm visualizing the entire path to production, or the value stream, and I'm able to see what's going on by capturing data about the key steps and what's going on there. Lots of solutions in this space, Atlassian, Tasktop, others, offer products here. You can do things low‑tech and you can just use a simple diagramming tool, whatever makes sense, but what is the best way to visualize your value stream. Study this topic more. It's super interesting and powerful. We'll also think about planning tools. How do I actually plan delivery of software, of services to production. So what are we talking about here? Well, I'm talking about tracking the work while planning sprints, releases, everything that gets that software into production. I'm doing this to improve transparency, reduce waste, and identify some of my blockers. I want an easy way to see, what am I trying to do? What is released coming up? What is coming up farther down the line? Who's working on what? And what's queued up and what are my dependencies? And so even doing my planning I might use Kanban boards here and do other methods to track priorities, look at that work in process, see what's going on, and figure out what is actually coming up next, how many people are maybe working on too many things. It's easy to look at a Kanban board and see, hey, there is way too much happening right now or one person is assigned to four different things. We have to shrink that down to one and get down to one‑piece flow. This stuff is so much easier when you embrace visual management and start doing that with good planning tools. And so, again, lots of good solutions in this space. You might use Trello, as I've done in the past; you might use Asana; a number of good things you can find to help your teams open up planning so a lot of people can participate and see what's going on. You'll also think about consolidated issue tracking systems. How do I track the bugs and defects that impact the quality and delivery of my software? High quality software getting to my customer constantly is the value here. That's the point. What am I doing to deliver more value? And so part of that is making sure my quality is great. So how am I tracking issues? Why do I do this? Well, I'm trying to increase the shared knowledge, and I want to use a central repository of data here to prioritize the work, improve the software, and so my real call to action here is using a shared system. You shouldn't have an issue tracking system used by your front line support people and a different one used by your developers. That might be controversial, but what happens here is you start to lose fidelity as issues and bugs copy between systems. You start to lose context, you start to lose the paper trail, and so ideally I have one system that we're using to capture and assign any of those issues. This is going to make sure we're having shared context on the same issue reports, we're all able to work on the same thing. It's going to improve your quality. A number of things in this space from Atlassian, GitHub Issues, Zendesk, others, plenty of ways, but again, look at how do you consolidate and shrink the number of issue tracking tools you have so it's not all sprawled across different places and you lose a 360 view of that actual issue. Let's talk about source control. This may be the most fundamental part of being good at DevOps. How do I manage changes to code, configs, scripts used to deliver software? That's what that's about. Why do I care here? Well, it improves my quality, because I have an actual change process, it increases my build reliability if I'm doing it well, and it encourages repeatable automation because I'm thinking about how do I automatically integrate and build this sort of thing? I have a source of truth, all of my things are there. It's not like just my source code, but even my database change scripts, my infrastructure automation, super valuable. And how I do this is I treat this as a repository for everything. Can I actually put everything it takes to build and manage these systems into declarative assets, whether that's code, configs, so I can create repeatable, fast, waste‑free processes to give value to production. Plenty of things in this space, GitLab and GitHub are doing a great job, Bitbucket, and others. And, again, try to get everything you can into source control so that you can start to create automation around deployment. Of course, as we think about then going from source code to something else, testing is such a huge part of this. So I need to continuously test the code, the system, and even the environment through automation. That's what this is about. Why do I do this? Well, I'm trying to improve the quality and resilience of this software through feedback early and often. If I'm running a great test suite, even locally, I'm catching things before I check it in, before it breaks the build, before it causes problems. So I want to keep having good tests early and often, getting feedback, then doing an integration test that catches it before it tries to go to prod, and even in production, good testing and smoke testing to make sure the outside‑in quality that my customer experiences, I'm learning before they do if something goes wrong. So you want to think about how do all the different sort of tests come into play here, whether it's things like unit and integration testing, even chaos‑based testing to improve confidence in the system and purposely trying to break things and run experiments to see what happens. That's just increasing the anti‑fragility of your system and the reliability of your system. Good things here. You might use local unit testing tools like XUnit, JUnit or even Gremlin for things like chaos testing. But look at this space. There's never been a more valuable time to have good testing skills, even if that's embedded into a product team versus in a dedicated QA team. Bring those testers closer to the actual product teams so you're doing constant testing. Then, of course, a big part of DevOps is thinking about continuous delivery. Probably the most common thing people think of when they think of DevOps is things like CI and CD or infrastructure automation. So what is CI/CD about? Well, it's about creating automation to merge, build, package, and deploy your software. That's what it's about. It's not the old days of sending zip files between teams and an operator manually installing some software on a server somewhere. It's about actually creating a fully automated path to production. Why do I do this? Well, I want fast quality feedback. Did this work or not? Is the test good or not? And I want to ship value to the customer more often. That's my goal. More value, more often, more reliably. I can't do that with a whole manual chain. It just doesn't work. I want to know if there's a defect that I can code it, check it in, see the tests run, and see it get to production in an hour or less. The best companies are making any lead time from check‑in to production in a day or less. That's what we should be aiming for here. Now, this has lots of upstream and downstream dependencies. I need good source control and testing hygiene to get those reliable builds which are fundamental to a good DevOps process. And then I need the rest of that process to be automated, because if I'm just good building that package and getting it ready for deployment, but deploying it takes a month, what's the point? That's my constraint. So I need to focus on the whole thing, whether that's change requests, whether whatever that process needs to be, I need to automate that. Plenty of things in this space. You can use, of course, tools like Jenkins and CircleCI, every cloud provider offers a first‑party CI/CD solution. This would be one of those first areas you really invest in seriously after you've done things like source code management and good test coverage. Let's talk about config management, though, a bit here, because this is also something people have thought about originally a lot with DevOps is the sort of, hey, how do I manage infrastructure at scale. And so the point here is how do I establish some consistency for all the infrastructure and applications, but using automation and not asking people to make sure that FileShare is available on every server, or people to make sure that this version of this thing is deployed everywhere. I need to automate that. Why do I do this? Well I do it to develop more stable systems that frankly don't burden humans with a ton of manual effort. Let's be selfish here. I don't want to get paged at all hours of the day because something happened. I want these systems to be a little more self‑healing and self‑maintaining. So how do I do this? Well, I start treating infrastructure as code so that the systems can be built and maintained programmatically. There's going to always be a need for human intervention at some point, but at the same time, many of the things that we're all doing manually today can be automated, from provisioning infrastructure, including virtual machines, networks, databases, file shares, all these things can be done programmatically, and they can be maintained programmatically so that if there are hiccups or issues, things can automatically get corrected. A ton of things here, whether you're using something like Terraform to build infrastructure, Pulumi, Ansible, Chef, Puppet, tools like that, as well as things from the cloud providers, help you build out infrastructure. Let's talk about public clouds, because this is actually a big part of doing DevOps well. So what is a public cloud really? I mean these are API‑driven platforms for on‑demand, elastic infrastructure and services. That's what a public cloud is, hyper‑automated, very API‑driven, tons of application and infrastructure services. Why does this matter to DevOps? A ton of reasons. It reduces some of the waiting waste because I get infrastructure instantly, things provisioned, managed, anywhere I want it. You and I can get a server in how many countries around the world in seconds or a database or a file storage. That's amazing! So as I'm thinking about reducing waste and getting value, I'm looking for things that get out of my way, and public cloud gets a lot of things out of the way. Now how do I start using this? You might start this, if you haven't already, with different sandboxes, test environments. An amazing way to stand up a performance test environment, beat the daylights out of your system and turn it off an hour later and pay dollars. That's amazing! But I also can get production‑grade environments here that are very hard to get on‑premises, and so a lot of different ways I can deliver value faster by making the infrastructure completely programmable. Of course, there's tons of solutions here, from Google Cloud, from Microsoft, from Amazon, Alibaba, others. Lots of good products in this space. Pay a lot of attention to who's giving you the most automation. Another big piece of this is how do I improve monitoring and observability? This is not just about shoving more stuff into production. That's valuable, but I'm not building a good system for my customer if I'm not also observing what's going on and troubleshooting and resolving issues faster. So this is about instrumenting the software and processes to capture data about the system state and user interactions. I need knowledge. I need to be continuously learning about what's happening in production. Why am I doing this? I'm trying to increase the transparency between teams, I'm trying to identify problems and solutions much more quickly, and also reduce the burden on operators. If I do this well, I'm not paging people every 5 minutes so that they turn it off. I'm being really smart about how do I only alert you for things that matter. So how do I do this? Well, I start revisiting what system/application/user data I'm capturing. Do I need to instrument better? How am I storing it? And then what am I doing with it? It doesn't help just to capture it all unless I can also use it when there's a problem, and quickly trace maybe one user request entirely through the system to see what happened. Or I can use that for very smart alerting, and only alert the right person at the right time for the right thing. Tons and tons of things in this space, companies like Datadog and Splunk, New Relic, Dynatrace, Honeycomb, great products in this space. A key area to invest in. Your old way of monitoring probably will not work in a true DevOps world, in a cloud world, in a dynamic infrastructure world, so look at how do I revisit that monitoring space and observability space, and improve not only the quality of data I have, but the tools I'm using to resolve issues. The last one I want to talk about is infusing some of the security practices throughout. You hear the term DevSecOps. How do I even make security part of this story? It's about enabling automated security checks throughout the software delivery process. Plenty of more things to that as we think about access permissions and others, but a lot of this is how do we simply improve the security through the pipeline? Why do I do this? Well, I'm improving software quality, of course, I'm increasing the empowerment of team members to actually tackle security and not just offload it to some InfoSec team, and I'm building confidence with my stakeholders, whether those are executives at your company, your customers, your partners. There's more faith that you're not injecting vulnerabilities into your code, there's more confidence that they're not going to have access problems later on, and there's confidence that you're doing things the right way with the best intent. So how do I do this? Lots of things here. I can invest in some of the things that integrate into your development environments. I can do vulnerability scanning of all the packages I'm bringing into my software, good secrets management and not putting credentials in code or in the clear in my source control. I can have build attestations to prove what the provenance of my software and where it came from. I can do least privilege deployment, so anyone who's worried that developers are just shipping to prod can trust that you're going through a very, very secure pipeline and only doing the bare minimum in deployment. This is actually a pretty exciting space, companies like Snyk, Twistlock, Aqua, lots of good companies doing things in security here. Another good one to explore for yourselves.

Summary 

So we covered a ton here. This is a loaded session, but hopefully, you enjoyed this module. We looked at some of the cultural changes you think about as we start thinking about customer focus. We looked at some of the org changes. You're going to think about staffing differently. You're going to look up at how you do communication. There's going to be lots of new processes here as you think about small‑batch delivery, as you think about the right ways to deliver software and do CI/CD. And there's tons of technology that can play a part here. Again, DevOps isn't about the technology. You can't buy DevOps. Just being good at CI/CD doesn't mean you're doing DevOps well, or frankly even being customer‑focused. So it's still part of a holistic story, but technology is huge here. If you're trying to do all of those previous things without modern tech, you're just making life so much harder and probably accidentally introducing complexity and waste. So technology is key, but technology's not the point. The point is awesome customer value delivered constantly to your customer and keep learning to make sure that keeps up. I hope you enjoyed this module and this course about DevOps and getting introduced to the big picture concepts. Let me know if you liked it. Drop me a note on Twitter at @rseroter, add discussion here in Pluralsight in the ask questions, and I hope to see you in a future course.

Understanding Agile at a deeper level 

Course overview 

Hi, my name is Chuck Cobb. I'm the author of the Project Manager's Guide to Mastering Agile. That will be the third book I've published to date on Agile Project Management, and I've also published two earlier books on business excellence. Agile is rapidly becoming the most widely‑used project management approach in the world, but it requires a lot of training and skill to do it effectively. Welcome to my training course on Understanding Agile at a Deeper Level. This course goes well beyond many typical Agile and Scrum courses that are often limited to the mechanics of how to do Scrum, and it provides a much deeper understanding of how to create a much more flexible and adaptive approach with Agile and Scrum. Here is an overview of the modules in this course. We're going to start with an in‑depth discussion of the Agile manifesto values and principles, then we're going to talk about learning to see the big picture and design thinking, and finally, we're going to talk about managing flow in Agile projects. This course is part of an overall curriculum for Agile business management that consists of three learning paths. This course is the first of four courses in the Mastering Agile Business Management Learning Path. The goal of this learning path is to help prepare business people to take a leadership role in leading Agile projects as a product owner, our position of equivalent responsibility. I hope you'll join me in this journey to learn Agile business management with the Understanding Agile at a Deeper Level course, at Pluralsight.

Course introduction 

Hi. My name is Chuck Cobb. I assume that you already have taken my previous courses that are prerequisite for this course and you know who I am, so I won't go through an introduction of myself again. Welcome to my training course on Understanding. Agile at a Deeper Level. This course is designed for business people who play a leadership role in an agile project, either as a product owner, a business sponsor, or a business analyst. A frequent problem with many agile projects is that many Agile teams do not take the time to do the depth of understanding that's really needed to fully understand Agile. The training for many typical Agile teams is very limited, and it only goes into the mechanics of how to do Agile. To really take an adaptive approach with an agile project, you really need to understand in much more depth the principles and values behind Agile, and that is exactly what this course is designed to teach you. This course is part of an overall curriculum for Agile Business Management that consists of three learning paths You should've already completed the Introduction to Agile Business Management learning path, which is a prerequisite for this course. This course is the first course in the Mastering Agile Business Management learning path. It is designed to provide a much deeper understanding of how to effectively lead Agile and Scrum projects for anyone in a product owner or in an equivalent level of business responsibility. The third and final learning path is called Enterprise‑level Agile Business Management and is focused on advanced topics related to applying Agile to a business at an enterprise level. We're going to start this module with a brief introduction to the course. This course is the first of four courses in the Mastering Agile Business Management learning path that are shown here. There are three other courses that are required to complete this learning path. Here's an overview of the modules in this course. After this brief introduction, we're going to start with a module on Agile Manifesto Values and Principles. Then, we're going to do a module on learning to see the big picture and design thinking, as well as a module on Managing Flow in Agile Projects. And finally, we're going to wrap up with a summary of the course. In the first module of this course, we're going to discuss the Agile Manifesto values and principles. The proliferation of many Agile methodologies in the 1980's and the 1990's led to the Agile Manifesto, which was originally written in February of 2001 at the Snowbird Ski Resort in Utah. The Agile Manifesto was a very important milestone in synthesizing the underlying values and principles behind many of the diverse range of Agile methodologies that had developed up until that point in time. The original Agile Manifesto values and principles were written around a software development environment, but they are really much more general than that and can be applied to some extent to almost any kind of project. For traditional project managers, it forces us to think outside of the box that there are different ways of managing a project and you have to tailor the approach to the project. When we discuss the Agile Manifesto and the principles behind it, we will talk about it primarily from the perspective of a software development project, but that is not meant to indicate that it is limited to software development only. Here's a summary of the topics we're going to talk about in this module. We're going to discuss the Agile Manifesto values in the first lesson, then we're going to spend four lessons discussing the Agile Manifesto principles.

Agile manifesto values 

The first lesson in this module is on the Agile Manifesto values. The four key values are as follows, individuals and interactions over processes and tools, working software over comprehensive documentation, customer collaboration over contract negotiation, and responding to change over following a plan. We will discuss each of these individually in the following slides. I want to emphasize, again, that it is very important to recognize that all of these statements are intended to be relative statements and not absolutes. For example, individuals and interactions over processes and tools does not mean that there are no processes and tools in an agile project. It means that the project relies more heavily on individual interactions to be more adaptive rather than being bound by a rigidly‑defined process and tools. Let me explain the concept of individuals and interactions over processes and tools a little further. In many traditional plan‑driven processes like the Waterfall approach, the emphasis on control required people to follow a well‑defined process. A more adaptive process requires you to control the process rather than the process controlling you. This kind of atmosphere is particularly important in an environment that requires creativity and innovation. This statement is essentially a response to the command and control project management practices that have been perceived as very impersonal and insensitive to people and rigidly‑ defined processes where the process manages you. From a project management perspective, it calls for a softer leadership approach with an emphasis on empowering people to do their jobs rather than a rigid, control‑oriented management approach that is highly directive, as well as flexible and adaptive processes. The last part of this statement regarding processes and tools might imply that there is no place in an agile project for processes and tools, but that is certainly not the case. Agile does use very well‑defined processes like Scrum, but they are meant to be interpreted and implemented intelligently by people rather than following them rigidly and mechanically. Tools can play a supporting role, but the important thing is to keep the tools in the right context. They should be there to leverage and facilitate human interactions, not to replace them. The important point is that agile processes depend very heavily on empowered people making intelligent decisions and the power of collaborative teamwork. In a similar way, working software over comprehensive documentation does not mean that there is no documentation at all in an agile project. Many traditional plan‑driven projects, and in particular, the Waterfall process, can be very documentation intensive. For example, in a true Waterfall process, documentation is a required deliverable at the end of each phase to show that you have successfully completed the requirements of that phase. Documentation in an agile project should be used intelligently where it provides value, and documentation should never be an end in itself. Many times online electronic tools that are designed to facilitate collaboration and communication can take the place of hard copy documentation. This value statement indicates a preference for working software over comprehensive documentation. This statement is essentially a response to typical phase‑gate project management processes that call for extensive documentation deliverables at the end of each phase. There was entirely too much emphasis on producing documentation to the extent that the documentation took on a life of its own and there was insufficient emphasis on producing working software. One of the problems with documentation is that it can inhibit normal communication. The typical Waterfall project was heavily based on documentation. The project team would develop an elaborately detailed requirement specification, and the software was tested against meeting that specification. In many Waterfall projects, the end user doesn't even see what is being developed until the final user acceptance testing at the end of the project. There are lots of opportunities for problems with that approach. Many users have a difficult time defining detailed requirements up front in a project, especially in a very uncertain and changing environment. And relying too heavily on documentation can lead to significant miscommunications and misunderstandings about the intent of the requirements. It's important to note that this statement doesn't imply that there should be no documentation at all in an agile project. The key thing to recognize is that any documentation should provide value in some way. Documentation should never be an end in itself. Customer collaboration over contract negotiation does not mean that contracts are incompatible with an agile approach. Very few people get a blank check to do an agile project without any expectations of what the cost and schedule of the project will be. So there's generally some kind of a need for managing customer expectations about costs and schedules. In a traditional plan‑driven project, there is typically a contract wth the customer to deliver a solution that meets defined requirements within a given cost and schedule, and naturally, to achieve that objective, the requirements need to be relatively fixed. In an agile project, there is much more of a spirit of partnership between the customer and the project team. Both sides acknowledge that the requirements are somewhat uncertain and are likely to be at least somewhat redefined or clarified as the project progresses. And both sides work collaboratively in a spirit of partnership and trust to develop a solution and manage cost and schedule tradeoffs as the project progresses. Agile has been successfully used in a broad range of contracting situations from fixed price contracts where the requirements for the contract deliverables are fairly well defined to more loosely defined contracting situations where only the broad‑based contract objectives are defined. The important thing is that the flexibility in the contract should be consistent with the nature of the requirements, and both sides need to have a mutual understanding of the level of uncertainty in the requirements and how the contract will be managed. This value statement indicates a preference for customer collaboration over contract negotiation. The typical project prior to Agile has been sometimes based on an arm's length contracting approach. Project managers for many years have been measured on controlling costs and schedules, and doing that has required some form of contract to deliver something based on a defined specification, and that also requires some form of change control to limit changes in the requirements as the project progresses. Agile recognizes that, particularly in an uncertain environment, a more collaborative approach can be much more effective. Instead of having a strong, ironclad contract to deliver something based on some predefined requirements, it's better in some cases to create a general agreement based on some high‑level requirements and work out the details as the project is in progress. Naturally, that approach requires a spirit of trust and partnership between the project team and the end customer that the team will ultimately deliver what is required within a reasonable time and budget. Without that spirit of trust and partnership, it will be almost impossible to make this approach work. Again, it's important to recognize that these values are all relative, and you have to fit this statement to the situation. At one extreme, I have applied an approach to a government contracting environment. Naturally, we had to have a contract that called for deliverables, and milestones, and expected costs. But even in that environment, the government agency understood the value of collaboration over having originally defined arms length kind of contract, and we were able to find the right balance. At the other extreme, you might have a project where the requirements are very uncertain and a much more adaptive approach is needed to work collaboratively with a customer to define the requirements as the project progresses without much of a defined contract at all. Responding to change over following a plan does not mean that agile projects are totally unplanned. A traditional plan‑driven project attempts to develop a detailed plan for a project up front prior to the start of the project and attempts to control changes to that plan to manage the scope of the project as the project progresses. An example would be a construction project where there would be a fairly detailed plan for constructing a building prior to the start of the construction. An agile project doesn't attempt to do a detailed plan for the entire project up front prior to the start of the project. It uses more of a rolling wave planning process. Some level of planning is done up front based on the level of uncertainty in the project, but much of the detailed planning is deferred until later in the project when more and better information will be available to support those decisions. It's similar to the kind of planning you might do for taking a vacation. When you plan a vacation, you don't typically plan what you're going to do down to the last detail for every day that you're on vacation. You probably have at least some goals for the things you want to do that may or may not be rigid and cast in concrete, but there's typically some level of flexibility built into your plans to develop more detailed plans once you're on vacation and have a better idea of what you want to do. The key point is that agile projects are not totally unplanned. They use more of a rolling wave planning process where some level of planning is done upfront based on the level of uncertainty in the project. But much of the detailed planning is deferred until later in the project when more and better information will be available to make those decisions. This value statement indicates a preference for responding to change over following a plan. This statement is in response to many projects that have been oriented towards controlling costs and schedules and made it difficult for the customer to change the requirements in order to control the scope and, hence, the cost and schedule of the project. The problem in applying that approach to environments where the requirements for the project are uncertain and difficult to specify up front is that it forces a user to totally define the requirements for a project up front without even seeing what the final result is going to look like, and that's just not very realistic in many cases. In many situations, it is more effective to recognize that some level of requirements are going to change and evolve as the project progresses and design the project approach around that kind of change. It's important to recognize that this is not an all‑or‑nothing decision to have either completely undefined requirements or highly detailed requirements. There are a lot of alternatives between those two extremes, and you have to choose the right approach based on the nature of the project. All of these statements are somewhat interrelated, and you have to consider all of them in concert to design an approach that is appropriate for a particular project. From a project management perspective, it calls for some skill to be able to do that. Instead of force fitting a project to some kind of canned and well‑defined methodology like Waterfall, the project manager needs to intelligently developed an approach that is well suited to the project. That applies to any project, not just Agile.

Agile Manefesto Principles 1 

In this lesson, we're going to start discussing the Agile Manifesto principles, how to interpret them, and why they make sense. This lesson is part one of four parts on that subject. This slide shows a summary of the first three Agile Manifesto principles. Our highest priority is to satisfy the customer through early and continuous delivery of valuable software. Welcome changing requirements even late in development. Agile processes harness change for the customer's competitive advantage. And deliver working software frequently, from a couple of weeks to a couple of months, with a preference for the shorter timescale. We will discuss each of these in more detail in the following slides. The first principle emphasizes early and continuous delivery of valuable software. In many traditional plan‑driven projects prior to agile, the end‑user customer doesn't see anything until the final user acceptance test phase of the project, and by that time it is very difficult and expensive to make any changes that might be needed. Emphasizing early delivery of software accomplishes two major goals. First, it provides an opportunity for the customer to see the software early in the development cycle and provide feedback and inputs so that corrections can be made quickly and easily. And second, working software is a good measure of progress. It's much more effective to measure progress in terms of incremental software functionality that has actually been completed and delivered to the user's satisfaction, rather than attempting to measure the percentage of completion of a very large development project that is incomplete. It is very difficult to accurately measure progress of a large software development project as a whole without breaking it up into pieces. That can be a very subjective judgment with some amount of guesswork. The key point is that breaking up the effort into well‑defined pieces that each have a clearly defined criteria for being considered done provides a much more factual and objective way of measuring progress. The next principle emphasizes creating an environment where change is expected and welcomed, rather than being rigidly controlled and limited. But of course that doesn't mean that the project is totally uncontrolled. There are lots of ways to manage change effectively, based on a mutual understanding and partnership with the customer. The important thing is that the project team and the customer should have a mutual understanding up front of how change will be managed. The next principle emphasizes using an iterative approach to break up a project into very small increments called sprints, or iterations, which are typically in the range of 2 to 4 weeks. There are a couple of reasons why this makes a lot of sense. First, an agile development process like scrum is based on continuous improvement. Instead of having a rigidly defined process that never changes, the team is expected to take an empirical approach to learn what works and doesn't work as the project progresses and make adjustments as necessary. If the project is broken up into very short increments and learning can take place at the end of each increment, learning and continuous improvement can happen much more rapidly. And second, people work more productively, given short time boxes to get things done. If it is done correctly, the team develops a cadence and a tempo that is very efficient for producing defined increments of work quickly and efficiently, like a manufacturing assembly line. A popular agile mantra is fail early, fail often. In other words, it's better in many cases to try something quickly and learn from it and make adjustments rather than taking all the time that might be needed to design an approach that is going to work flawlessly the first time.

Agile Manefesto Principles 2

In this lesson, we're going to continue discussing the Agile Manifesto principles. This is part two of four parts on that topic. This slide shows a summary of the next three Agile Manifesto principles. Business people and developers must work together daily throughout the project, build projects around motivated individuals, give them the environment and support they need, and trust them to get the job done. And the most efficient and effective method of conveying information to and within a development team is face‑to‑face conversation. We will discuss each of these principles in more detail in the following slides. The next principle emphasizes a partnership approach between the development team and the business sponsors. This is very consistent with the Agile Manifesto value of collaboration over contracts. To implement this principle, both the business sponsors and the project team need to feel jointly responsible for the successful completion of the project. This calls for a much higher level of engagement of the business sponsors than is commonly found in many traditional projects where the implementation of the project might be almost totally delegated to the project team. The degree of engagement, of course, should be appropriate to the nature of the project, and how the business sponsors get engaged might be different depending on the circumstances. For example, Scrum has a role called the product owner that provides a day‑to‑day business direction for the project. But the direction may not be limited to that. In a large enterprise‑level project, there might be a number of other stakeholders that need to provide input and be engaged somehow. Designing an approach that gets the right people engaged at the right time is very important for making the project successful. The next principle emphasizes the importance of properly motivated individuals on a project. Too often in the past, some project managers have used high pressure command and control tactics to pressure project teams into delivering results faster. Many of us have been involved in what I call death march projects where people are given an absolute deadline for getting something done and have to work nights and weekends, if necessary, to get it done. There are also some projects that are run like a sweatshop that are very insensitive to people, and when you're in an environment that requires high levels of creativity and innovation, that approach just doesn't work very well. The philosophy of Agile is based on a high level of empowerment and individual initiative by the people in the project. Instead of being told specifically what to do and being pressured into doing it, Agile teams are given general direction and are expected to figure out how to get it done most effectively and efficiently themselves. Making that kind of approach work requires a very people‑oriented leadership style. However, it doesn't mean that there is no need for leadership whatsoever. As an Agile project manager, you need to adapt your leadership style to fit the situation, and that will typically depend on several factors, including the nature of the project and the level of maturity and experience of the team. The next principle emphasizes face‑to‑face communication. This is another statement that you have to not take as an absolute, but think of it as a relative. It is not always possible with distributed teams to always have face‑to‑face communications, but it is certainly desirable if it is possible. This statement also doesn't mean that the only form of communication is direct face‑to‑face communication. It is a reaction to the history of Waterfall projects that heavily relied on documented requirements as a way of communications. There are many ways to communicate information in various forms, and you need to choose the optimum mix to fit a given situation. The right mix will depend on a number of factors, including the scope and complexity of the project and the distribution of the team working on the project.


Agile Manefesto Principles 3

In this lesson, we're going to continue discussing the Agile Manifesto principles. This is part three of four parts. This slide shows the next three Agile Manifesto principles. Working software is the primary measure of progress. Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely, and continuous attention to technical excellence and good design enhances agility. We will discuss each of these principles in more detail in the following slides. The next principle emphasizes working software as the primary measure of progress. Measuring progress on a software development project can be difficult and problematic. The traditional method is to break a project into tasks and track the percent completion of those tasks as a way of measuring progress. However, that can be very misleading because often the list of tasks is incomplete, and the level of completion often require some subjective judgment, which is difficult to make and often inaccurate. Testing is another factor in this. Very often in the past, the entire development process and testing process might've been sequential. The result is that even though the development of the software might've seemed to be complete, you don't know how complete it really is until it has been tested and validated to be complete. An agile approach emphasizes doing testing much more concurrently as the software is developed. There is a concept in Agile called the definition of done that you will hear quite often that says you have to clearly define what done means, and it generally means that the software has been tested and accepted by the user. In other environments, the definition of done might be a lot more ambiguous and subject to interpretation. If you don't have a clear definition of done, any estimate of percent complete is likely to be suspect. The key point is that a more accurate measure of progress is to break up a software project into chunks of functionality where each chunk of software can be demonstrated to the user for feedback and acceptance, and each chunk of software is not deemed to be complete until it fully meets the definition of done. The next principle emphasizes maintaining a pace that supports sustainable development. Many of the underpinnings of Agile come from lean manufacturing and total quality management, which was discussed in the last module. In a manufacturing environment, companies learned many years ago that running a manufacturing plant like a sweatshop and forcing workers to work an excessive number of hours under poor conditions does not often result in high quality products. A similar thing is especially true in a natural environment because the success of the effort is so critically dependent on the creativity and motivation of the team. In that kind of situation, it is even more important to create an environment where work is sustainable over a long period of time. The next statement is that continuous attention to technical excellence and good design enhances agility. Many people might have the image of an agile software development team as a bunch of cowboys that just get together and hammer out code without much design planning and without any coding standards. That is not the case. Agile recognizes the need for doing things the right way to avoid unnecessary rework later. On the other hand, an agile approach should not result in over designing or gold plating a product either. A comment you will hear often in an agile environment is a concept of just barely good enough. In other words, the work should be done to a sufficient level of completeness and quality to fulfill the purpose it was intended to fill and nothing more. Going beyond that level of just barely good enough is considered waste.

Agile Manefesto Principles 4

In this lesson, we're going to continue discussing the Agile Manifesto principles. This is part four of four parts on that topic. This slide shows the final three Agile Manifesto principles. Simplicity, the art of maximizing the amount of work not done, is essential. The best architectures, requirements, and designs emerge from self‑organizing teams. And at regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly. We will discuss each of these principles in more detail in the following slides. The next statement emphasizes simplicity. How many times have we seen projects go out of control because the requirements get much too complex and very difficult to implement? Some people call that gold plating. This is also related to the concept of just barely good enough. Don't over design something. Keep it as simple as possible. In some cases, it may make sense to start with something really simple, see if it fills a need, and then expand the functionality later only if necessary. Another concept in Agile is called the minimum viable product, which defines a minimum set of functional features a product has to have to be viable at all in the marketplace. It's generally much more effective to take an incremental approach to start with something simple and then expand it as necessary rather than starting with something overly complex that might be overkill for the requirement. The next principle is that the best architectures, requirements, and designs emerge from self‑organizing teams. Agile is heavily based on the idea of self‑organizing teams, but that needs some interpretation. Sometimes developers have used the idea of self‑organizing as an excuse for anarchy, but that's not what was intended. The intent is that if you have the right people on a team and the team is empowered to collectively use all the skills on the team in a collaborative manner, that it will generally deliver a much better result than a single individual could deliver acting alone. The idea is that high performance, cross‑functional teams that work collaboratively can produce superior results. This is especially true for developing more complex products that require integrating multiple perspectives to optimize the design of the product that is being produced. The last principle is that at regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly. As I've previously mentioned, Agile is based heavily on continuous improvement and using short intervals to reflect on what's working and what's not working and taking quick corrective action as necessary. In Scrum, this is called a retrospective, and it happens at the end of each sprint. A focus on continuous improvement is essential to a highly adaptive empirical development process. In Scrum, the process is designed to continuously improve the product that is being produced, as well as a process that is used to produce a product throughout the project. In the next lesson, we're going to start a new module on learning to see the big picture, and the first lesson in that module is on systems thinking.

Learning to see the big picture 

Systems thinking 
Hi. In this lesson, we're going to start a new module on learning to see the big picture. Here's a brief outline of the topics we're going to cover in this section. The first topic is systems thinking, which is critical to see the interrelationship of processes, people, and tools from an overall systems perspective. The next topic is complex adaptive systems, which is based on understanding how systems naturally occur in nature that have adaptive properties similar to Agile. In this section, we're going to discuss systems thinking, which is a very important concept to understand in order to develop an adaptive agile mindset rather than doing Agile mechanically. To do that, it is essential to understand Agile in a larger and much more dynamic context of how it integrates into the overall business system that it supports, and that requires a systems thinking approach. Many traditional plan‑driven project management approaches, like the Waterfall approach, are fairly static and well defined. The process doesn't change significantly from one project to the next. An agile project, by its very nature, has to be much more dynamic and adaptive to fit a given situation. That's much more complex than simply executing a static and well‑defined process. It requires a different way of thinking to consider all the elements that might impact the process and how to dynamically adapt a process to fit a given situation. That's where systems thinking comes in. It's a very different way of thinking that is essential to adopting an agile mindset. Here's the definition of systems thinking. Systems thinking utilizes habits, tools, and concepts to develop an understanding of the interdependent structures of dynamic systems. When individuals have a better understanding of systems, they are better able to identify the leverage points that lead to desired outcomes. The practice of systems thinking can be complex. The phrase systems thinking can refer to a set of tools such as causal loop diagram, stock and flow diagrams, and simulation models to help us map and explore dynamic complexity. For example, systems thinkers often describe the world in terms of reinforcing and balancing processes, limits, delays, patterns of behavior over time, and so forth. However, without adding a lot of complexity, a lot can be gained from simply developing a unique perspective on reality, a perspective that sharpens our awareness of the whole, and how the parts within those wholes interrelate. Here's a couple of definitions that reflect that. Systems thinking is a vantage point from which you see a whole, a web of relationships, rather than focusing on the detail of any particular piece. Events are seen in the larger context of a pattern that is unfolding over time. Systems thinking is also a perspective of seeing and understanding systems as wholes rather than as a collection of parts. A whole is a web of interconnections that creates emerging patterns. That's the way we're going to use the term systems thinking in this course. As it relates to Agile, it means being able to see a complex agile process as a dynamic organism that has many elements with cause and effect relationships rather than seeing it as a more static mechanical process and also seeing it in the context of how it fits within the overall business system that it is part of, rather than seeing it as simply as standalone development process. Systems thinking, in many cases, involves developing a mental model of how a complex system works. Here's an example to illustrate this idea from a business perspective. When someone thinks of a business that they are part of, many times they will think of the business in terms of an organization structure that looks something like this. This is a somewhat static and not very dynamic view of how a business works. As an alternative, this slide shows a simple, but much more dynamic enterprise model of how a business works that I used in the first book I published in 2003 that is designed to provide a framework for applying process improvement initiatives to enterprise‑level business situations. This model is fairly simple, but it does provide a useful model to understand the cause and effect relationships that drive business results. It's very common that when a company's business results don't turn out to be what was expected, that the company becomes very reactive in trying to fix the problem. However, without a clear idea of what factors influence business results, it may not be very clear of what needs to be fixed. Having a model like this allows a much more proactive approach. Instead of reacting to changes in business results, a company can become much more proactive in addressing the factors that influence the business results long before the impact might be visible in terms of actual results. There are several levels in this model that have cause and effect relationships to make the business operate as a dynamic overall system. At the bottom level are such fundamental things as employee knowledge and skills, supporting systems, and other enablers and constraints that all have a significant impact on influencing operational process performance. Operational process performance, if it is designed correctly, in turn, plays an important role in driving customer value. And finally, customer value relative to the competition plays a big role in driving business results. Binary thinking is the antithesis of systems thinking. Many people use what I call binary thinking to simplify what is actually not very simple at all. For example, you will hear a lot of people talk about comparing Agile to Waterfall as if it were a simple binary choice between the two. If you think about it, it is much more complicated than that. There are many variations on what Agile is. Although Scrum is very widely used, it is only one of those variants, and I don't believe very many people use a full‑blown Waterfall approach anymore. People have been using innovative approaches for a number of years prior to Agile. It's not really a simple binary choice between those two alternatives, and there are many ways to blend Agile and traditional project management principles and practices together in the right proportions to fit a given situation. But it requires a deeper understanding to do that. You have to understand a broader range of methodologies and understand the principles behind those methodologies to know how to mix and match them together. That's another example of systems thinking. Many people choose to ignore complexity and oversimplify it. There's an old saying that says it's easier to accept a simple myth in some situations than it is to understand and accept complex reality. Many people oversimplify what isn't so simple at all, and the result can be very misleading. Binary thinking is often based on what is called a false dichotomy. A dichotomy is a set of two mutually exclusive, jointly exhaustive alternatives. A false dichotomy is a dichotomy that is not jointly exhaustive. In other words, there are other alternatives or that is not mutually exclusive. In other words, the alternatives can overlap, or that is possibly neither. Here's an example of a false dichotomy. If you want better schools, you have to raise taxes. If you don't want to raise taxes, you can't have better schools. A third alternative is that you could spend the existing tax money much more efficiently. The typical Agile versus Waterfall argument is a perfect example of a false dichotomy. When you look at Agile versus Waterfall from a systems thinking perspective rather than from a binary thinking perspective, you realize that it is much more complicated than a binary and mutually exclusive choice between two extremes. Here's a diagram that shows the complexity behind the Agile versus Waterfall comparison. Suppose you had to choose between an agile project management approach or a more traditional project management approach. What are the factors that you would consider and how are they interrelated? This diagram shows a few of the things you might consider. A key factor is going to be what planning approach you use for the project. Will it be heavily based on upfront planning, or will it be more based on deferred planning until later in the project? That is a function of a number of things, including the level of uncertainty in the project. But it is also directly related to the approach for managing risks in the project, which is also related to the level of criticality in the project. Another factor is the testing and support considerations, which is also impacted by the project risk and criticality, as well as the scope and complexity of the project. Finally, the organizational culture and organizational capabilities are a very significant factor that would influence whether you would choose an agile or a more traditional project management approach. In summary, systems thinking is an alternative to binary thinking. It involves seeing things in a very different perspective based on the relationships among a system's parts. It is applicable to a very wide range of complex processes and systems. Systems thinking is a way of understanding reality that emphasizes the relationships among the systems parts rather than the parts themselves. Systems thinking doesn't come naturally to many people. The human mind is not naturally programmed to think in systems. It does not naturally think non linearly. From an early age, we learn to think in simple terms of cause and effect and to speak a language rooted in linear cause and effect concepts. In addition, we expect the effect to happen immediately after the cause so that they can be easily associated. Systems thinking is complex. Cause and effect thinking is simple, even a seven‑month old can do it. Systems thinking must be taught rather than learned naturally. Humans are not programmed to easily comprehend effects that are delayed from or several derivatives from the original cause. The key point is that you shouldn't just accept things at their simple face value and mechanically follow a process without questioning why it is done that way and what factors might cause it to work or not work and without determining if there's a better way to accomplish the same result.

Complex adaptive systems 

An empirical process control model is an example of a complex adaptive system, which is the subject we're going to talk about next. An understanding of complex adaptive systems is important because it fundamentally defines a process of how an agile approach works. And, by the way, you're likely to find a question on the PMI‑ACP exam on complex adaptive systems. This slide shows a definition of a complex adaptive system. A complex adaptive system is, "a complex macroscopic collection of relatively similar and partially connected micro‑structures formed in order to adapt to a changing environment and to increase its survivability as a macro‑structure." Examples of complex adaptive systems are found all around us in nature. One example I can think of is an ant hill. There are thousands, perhaps millions, of ants required to build a large ant hill. I'm guessing that they don't start out with a grand plan of what the finished ant hill is going to look like. It just evolves. And somehow those thousands or millions of ants all work collaboratively together as a self‑organizing team without a lot of direction. And when the ant hill is finished, it may not be a perfect‑looking structure, but it satisfies the purpose it was intended for, and it will evolve as needed to fit the overall environment that it is part of. Peter Fryer has developed an excellent explanation of how complex adaptive systems have evolved. "For many years scientists saw the universe as a linear place, one where simple rules of cause and effect apply." They viewed the universe as a big machine and thought if they took the machine apart and understood the parts, then they would understand the whole. They also thought that the universe's components could be viewed as machines, believing that if we worked on the parts of these machines and made each part work better, then the whole world would work better. "Scientists believed that the universe and everything in it could be predicted and controlled. However hard they tried to find the missing components to complete the picture, they failed." Despite using the most powerful computers in the world, the weather remained unpredictable. Despite intensive study and analysis of ecosystems, an immune system did not behave as expected. But it was in the world of quantum physics that the strangest discoveries were being made, and it was apparent that the very smallest subnuclear particles were behaving according to a very different set of rules of cause and effect. Gradually, as scientists of all disciplines began exploring these phenomenon, a new theory emerged, complexity theory, a theory based on relationships, emergence, patterns, and iterations, a theory that maintains that the universe is full of systems, weather systems, immune systems, social systems, etc., and that these systems are complex and constantly adapting to their environment. Hence, complex adaptive systems. The important point is that complex adaptive systems have been found in many natural environments since the beginning of time. What has evolved is not the systems themselves but our understanding of them. We need to adopt a similar thinking approach to project management. Instead of thinking of projects as a linear place where simple rules of cause and effect apply, we need to think of it as a much more dynamic organism. If it is done correctly, a complex adaptive system's approach is a very powerful approach to project management. It's like the ant hill example. I suspect that there aren't designated project managers among the ants building the ant hill to plan and delegate the work to individual ants. The individual ants somehow just work together naturally and collaboratively to build the ant hill without a lot of explicit direction. That's exactly the way an agile project works. If it is done right, an agile team doesn't need a lot of explicit direction to be told exactly what to do and how to do it. With some very general direction, they should be able to figure out what needs to be done to accomplish the goals that have been established. Peter Fryer has also provided a simple diagram that explains how a complex adaptive system works. The agents in the system are all the components of that system. In the ant hill example, the agents are the ants. Here's a few other examples of complex adaptive systems, the air and water molecules in a weather system and the flora and fauna in an ecosystem. These agents interact and connect with each other in unpredictable and unplanned ways. From this massive interaction, regularities emerge and start to form a pattern, which feeds back on the system and informs the interactions of the agents. For example, in an ecosystem, if a virus starts to deplete one species, this results in a greater or lesser food supply for the others in the system, which affects their behavior and their numbers. A period of flux occurs in all the populations in the system until a new balance is established. Here are some properties of complex adaptive systems that Peter Fryer has identified. The first is emergence. Rather than being planned or controlled, the agents in the system interact in apparently random ways. From all these interactions, patterns emerge, which informs the behavior of the agents within the system and the behavior of the system itself. The ant hill is a great analogy for how a pure, agile project might evolve. The next is co‑evolution. All systems exist within their own environment, and they are also part of that environment. Therefore, as their environment changes, they need to change to ensure best fit. But because they are part of the environment, when they change, they change their environment, and as it has changed, they need to change again. And so it goes on as a constant process. This characteristic is also applicable to an agile process. An agile project management process is part of a much larger business ecosystem and needs to adapt and evolve as that business system evolves. The key point is that we need to recognize the dynamic nature of complex adaptive systems in our projects and also recognise that those systems need to evolve as needed to fit the environments that they're part of. Many traditional plan‑driven projects fail to recognize and take advantage of the dynamic nature of project teams that are part of the project, and as a result, fail to utilize the full power of the teams. Many agile teams focus on optimizing the performance of a team from a development perspective only and fail to recognize the need to adapt a system to the overall business ecosystem that the development project is part of. Complex systems are also suboptimal. A complex system does not have to be perfect in order for it to thrive within its environment. It only has to be slightly better than its competitors, and any energy used on being better than that is wasted energy. A complex adaptive system, once it has reached the state of being good enough, will trade off increased efficiency every time in favor of greater effectiveness. This is also true of an agile process. An agile process is based on the understanding that absolute perfection is not necessarily the most appropriate goal. Ideally, an agile process seeks to meet the requirements for a product in terms of minimal, marketable features, and only going beyond that level if it provides business value. Ideally, complex adaptive systems also have what is called requisite variety. The greater the variety within the system, the stronger it is. In fact, ambiguity and paradox abound in complex adaptive systems, which use contradictions to create new possibilities to co‑evolve with their environment. Democracy is a good example in that its strength is derived from its tolerance and even insistence in a variety of political perspectives. Here's how this applies to agile. A strong agile team is able to successfully integrate people of different perspectives into a very cohesive, cross‑functional team. A team of yes‑men that all think alike is typically not that strong. Another important property of a complex adaptive system is connectivity. The ways in which the agents in a system connect and relate to one another is critical to the survival of the system because it is from these connections that the patterns are formed and that the feedback is disseminated. In relationship to an agile process, this is extremely important. The agents in an agile process are the people on the team, as well as the outside stakeholders who interact with the team. The success of the process is critically dependent on the relationships and interactions among all these people. Ideally, those interrelationships are very dynamic and based on simple rules without being overly controlled or structured. The next property is that complex systems have simple rules. Complex adaptive systems are really not that complicated. The emerging patterns may have a rich variety, but like a kaleidoscope, the rules governing the function of the system are quite simple. A classic example is that all water systems in the world, all the streams, rivers, lakes, oceans, waterfalls, etc., with their infinite beauty, power, and variety are governed by the simple principle that water finds its own level. Complex adaptive systems are also based on an iterative process. Small changes in the initial conditions of the system can have significant effects after they have passed through the emergence and feedback loop a few times, often referred to as the butterfly effect. A rolling snowball, for example, gains on each roll much more snow than it did on the previous roll. And very soon, a fist‑sized snowball becomes a giant one. Complex adaptive systems are also self‑organizing. There is no hierarchy of command and control in a complex, adaptive system. There is not much planning or managing. But there is a constant reorganizing to find the best fit with the environment. A classic example is that if one were to take any typical town and add up all the food in the shops and divide by the number of people in the town, there will likely be only about two weeks of food supply. But there is no food plan, food manager, or any other formal controlling process. The system is continually self‑organizing through the process of emergence and feedback. Both an iterative development approach, as well as an emphasis on self‑organizing teams, are extremely critical for successful agile projects. Complexity theory is not the same as chaos theory, which is derived from mathematics. But chaos does have a place in complexity theory in that systems exist on a spectrum ranging from equilibrium to chaos. A system in equilibrium does not have the internal dynamics to enable it to respond to its environment and will slowly or quickly die. A system in chaos ceases to function as a system. The most productive state to be in is at the edge of chaos, where there is maximum variety and creativity leading to new possibilities. This slide shows how that concept relates to the Stacey complexity model that was previously discussed. A process that operates near the edge of chaos might be ideal, but there's a risk of it straying over the line into chaos and losing its effectiveness altogether. On the other hand, a system that is in equilibrium may not be responsive at all to changing conditions and for that reason may not be effective. This is a delicate balancing act to choose the appropriate balance. The key point is that in a highly uncertain environment, an adaptive approach that encourages individual creativity is essential, and over‑emphasis on control and predictability can stifle the adaptivity and creativity that might be needed to deal with a highly uncertain environment. The final characteristic of complex adaptive systems is that most systems are nested within other systems, and many systems are systems of smaller systems. If we take the example of a food shop, the food shop itself is a system with its staff, customers, suppliers, and neighbors. It also belongs to the food system of that town and the larger food system of that country. It belongs to the retail system locally and nationally and the economic system locally and nationally and probably many more. Therefore, it is part of many different systems, most of which are themselves part of other systems. A key point is that "it is important to optimize the performance of a project team within the broader context of the business ecosystem that it is part of rather than within a narrow project development context." In the next lesson, we're going to start a new module on design thinking. And the first lesson in that module is What is Design Thinking?

Using design thinking 

What is design thinking 

Hi, and welcome to the next module in this course .In this module, we're going to discuss design thinking. Design thinking is a relatively new concept that puts an emphasis on user‑oriented design, which has a high priority on developing products that are very well designed to satisfy user needs. It is very consistent with an agile approach. Here's a brief summary of the topics we will talk about in this module. We're going to start with a discussion of what is design thinking? Then we're going to talk about the stages of the design thinking model. Next, we're going to discuss how does design thinking relate to agile. Then we're going to discuss some successful examples of design thinking. And finally, we're going to do a summary of design thinking. The first lesson in this section is on what is design thinking? There are many different definitions of design thinking. I've developed a simplified definition of my own that is shown here. Design thinking is a way of thinking that emphasizes a disciplined and methodical approach for problem solving, combined with creativity and innovation throughout the product life cycle to develop highly competitive, leading‑edge products. The general problem that design thinking addresses is that many people have a tendency to jump too quickly into a technical design solution before thoroughly understanding all aspects of the problem to be solved and before evaluating alternative solutions to determine an optimum design. Design thinking can be critical for companies to develop a competitive advantage, where product leadership is an important factor. It is especially useful for very difficult design problems, which are ill‑defined or tricky. One of the most critical aspects of design thinking is an emphasis on human engineering. "Design thinking has a human‑centered core. It encourages organizations to focus on the people they're creating for, which leads to better products, services, and internal processes." When you sit down to create a solution for a business need, the first question should always be, what's the human need behind it? "In employing design thinking, you're pulling together what's desirable from a human point of view with what is technologically feasible and what is economically viable." It also allows those who aren't trained as designers to use creative tools to address a vast range of challenges. The process starts with taking action and understanding the right questions. It involves embracing simple mindset shifts and tackling problems from a new direction. Design thinking means paying attention to customer needs and preferences, rather than simply optimizing a design from a technical or manufacturing process perspective. Here's an example. PepsiCo has used design thinking to improve many of its products. Sun Chips is an example. The original size was 1 inch by 1 inch. When you'd bite into a chip, it would break into pieces. In focus groups, consumers told PepsiCo they went to another product because it was bite sized. PepsiCo had to conclude that Sun Chips were too big. It doesn't matter that the molds can only cut 1 inch by 1 inch. PepsiCo does not sell products based on the manufacturing processes they have, but on how target customers can fall in love with them. Design thinking seeks to maximize innovation. However, innovation is multi‑dimensional. Many times a design is limited to looking at technological feasibility and business viability. However, human values are often overlooked. Design thinking seeks to maximize overall innovation in all of these areas.

Stages of design thinking 

In this lesson, we're going to talk more about the Stages of Design Thinking. This slide shows the five generally accepted stages associated with Design Thinking. We will talk about each of these in more detail in the following slides. However, I want to emphasize that this is just a conceptual model and it is not intended that Design Thinking always rigidly follows these stages. The first stage of the Design Thinking process is to gain an empathetic understanding of the problem you are trying to solve. This involves consulting experts to find out more about the area of concern through observing, engaging, and empathizing with people to understand their experiences and motivation, as well as immersing yourself in the physical environment so that you can gain a deeper understanding of the issues involved. Empathy is crucial to a human‑centered design process such as Design Thinking, and empathy allows design thinkers to set aside their own assumptions about the world in order to gain insight into users and their needs. The next stage is to synthesize the information created during the empathize stage and develop a concise statement of the problem. The problem should be defined in a human‑centered manner from the perspective of the user need. The Define stage will start the process to the next stage, the Ideate stage, to look for potential solutions. During the third stage of the Design Thinking process, designers are ready to start generating ideas. You've developed an understanding of your users and their needs in the Empathize stage, and you synthesize their observations in the Define stage and created a human‑centered problem statement. With this solid background, we can start to identify new solutions to the problem statement that you've created and start to look for alternative ways of viewing the problem. During the Prototype stage, the design team will produce some inexpensive prototypes to further investigate the potential solutions. The aim is to identify the best solution to the problem that satisfies the user need most effectively and most economically. During the Test stage, the design team will test the solution and gain user feedback. The results will often be used as an iterative process to either further define the problem or to better understand user needs. I want to emphasize once again that these stages don't necessarily take place sequentially, and the process might require going through these stages more than once to find an optimum solution.

How does design thinking relate to agile 

In this lesson, I want to talk about how does design thinking relate to agile? On the surface, it might look like design thinking is in conflict with agile. Design thinking encourages you to slow down and pay attention to all aspects of the design prior to jumping too quickly into a solution. If you tried to do it literally, it might mean following a set of stages sequentially, like waterfall. However, that's not necessarily really the intent. As I previously mentioned, the stages are meant to be conceptual. They are not meant to be followed exactly in a sequence, as it might look. The relationship between agile and design thinking is very similar to the relationship between agile and lean. Some people might see lean and agile as contradictory to each other. Lean tends to emphasize standardizing and streamlining processes to improve efficiency, while agile emphasizes flexibility and adaptivity to meet customer needs in a very uncertain environment. If each of those goals were pursued in isolation and to an extreme, they might in fact be somewhat in opposition to each other. However, if they are treated correctly and blended together in the right proportions, they can actually be very complementary to each other. The relationship to design thinking is similar. Agile emphasizes, reducing time to market and getting the development done as quickly as possible. Design thinking emphasizes considering all aspects of the design, particularly the human engineering aspects, and evaluating alternative solutions before you jump too quickly into a design. This is a slide I created originally for a discussion of agile and lean. It shows the balance that needs to be established between an agile approach and a lean approach. I've expanded it to include a balance between agile and design thinking. All of these things need to be considered in the right proportions in an agile development approach. An important point that you should get from this is that agile may require balancing conflicting forces. The common focus should be on maximizing customer value. The overall agile approach is not entirely inconsistent with design thinking. Agile is a flexible and adaptive approach to project management and development that emphasizes relying heavily on user feedback and inputs to optimize the value of the solution that you develop. While that aspect of an agile approach is very consistent with design thinking, it doesn't go as far as to emphasize the structured approach that design thinking advocates to understand the human needs behind project requirements. So that leaves open the question of how do you go about integrating agile and design thinking? Here are some general recommendations First, design thinking is not a methodology. It is a way of thinking, like lean and agile. It does not invalidate the basic agile/scrum methodology, and it isn't inconsistent if it's done in the right context. The most important aspect of design thinking is the philosophy behind it. Be sensitive to the human aspect of design. Don't just rush into a design from a technical development perceptive, and don't create a technically elegant design that may or may not be well designed from a human engineering perspective. Here are some more specific recommendations. Exactly how you might go about integrating design thinking with agile might vary from one project to the next. Agile is based on delivering value, and human engineering is only one component of value. There are different approaches that might be used, depending on the level of value placed on human engineering in the project. There are several different approaches that could be considered, depending on how critical design thinking is to the overall success of the project. In some projects, if the focus on human engineering is very critical, it may justify doing more up‑front design prior to the start of the project to develop and test the overall design approach. Alternatively, a special sprint or a spike could be dedicated to finding an optimum design from a human engineering perspective. Another approach could be to incorporate a focus on design thinking directly into the design process for each sprint. Roman Pichler has suggested an approach for doing that that is shown in the diagram on this slide. If the focus on human engineering is less critical, another approach would be to just integrate the focus on human engineering into the design as a constraint that has to be met. It is essentially a risk of not providing an acceptable level of human engineering in the design

Successful design thinking examples 

In this lesson, I want to talk about successful examples of design thinking. The first example is Apple Computer. Apple is one of the leading companies that is renowned for its unique products and brand. A short talk with an Apple user will reveal that there is an emotional relationship between consumers and Apple products, including every i product that was created in the past two decades. Why are Apple products different from their competitors' products? How does Apple manage to achieve innovation in its products families? Apple had a rough history for a while. In 1985 Steve Jobs was forced to leave the company. This marked the start of a very chaotic era in the company's strategy and product development. This slide shows some of the problems that Apple had during the period of 1985 through 1997. The company had a very unstable strategy due to the change of executive teams. There was unclear vision about Apple's competitive strategy. There was also unclear vision about selling operating system licences, which would put the company in competition with Windows operating systems. There was a large number of failed products, such as the Newton PDA, and few successful ones, such as the PowerBook. There were many products that were not unique in the market. And finally, there was lots of confusion and uncertainty among Apple consumers resulting from this strategy. When Steve Jobs returned to Apple after being fired, Apple's stock was selling for only $5 per share. One of the most important aspects of design thinking is an emphasis on human engineering. Here's a quote from Steve Jobs on that subject. "Most people make the mistake of thinking design is what it looks like. People think it's this veneer‑‑that the designers are handed this box and told, 'Make it look good.' That's not what we think design is. It's not just what it looks like and feels like. Design is how it works." Steve Jobs applied design thinking by focusing on people's needs and desires rather than only the needs of the business, building empathy by helping people to love Apple products. The design rather than the engineering work, designers considered both the form and the function of the product and building simple yet user‑friendly products rather than complex, hard‑to‑use products. The key difference is that although other competitors focus on the features and product capabilities, Apple focuses on a holistic user experience. This slide shows a summary of what was needed to be done to achieve this vision. The first area was excellence in execution. Steve improved the execution process by closing 2 divisions, eliminating 70% of the new products, and focusing only on the higher‑potential props, reducing the product lines from 15 to just 3, and shutting facilities to move manufacturing outside the company. Apple also launched a website for direct sale of its products and started to take an interest in materials and how products are manufactured with a consumer‑driven culture. The next area was the Apple platform strategy. Apple streamlined their product portfolio strategy to a family of products that can be produced much more quickly while keeping the existing design elements. Also, the company targeted products that required less repair and maintenance. A critical element of making this successful was iterative customer involvement. The consumer experience should be integrated into the design and development stages through participating in usability testing. Also, the design for interfaces should focus on the user experience. And finally, Apple wanted to create beautiful products. In addition to the function of the product, the form should be beautiful, which can be achieved through continuous innovation and development. Apple also focused on the materials and manufacturing process and took a bold approach to trying new ideas rather than sticking with the ordinary design forms. Can you imagine trying to do all of this with a traditional, plan‑driven approach to project management? It's clear to me that it would have been impossible. An agile approach was essential. Before we finish this lesson, I want to go over a few more examples of how design thinking is applied in other industries. The first additional company that I want to talk about is PepsiCo. PepsiCo is one of the world's leading food and beverage companies, serving more than 200 countries and territories around the world. They're well known for selling Pepsi canned soda and snacks such as Ruffles potato chips. You might not think that a company like this that sells such commodity products would benefit from design thinking. For example, a bag of potato chips is a bag of potato chips, isn't it? However, that's the very reason why design thinking can make a big difference in an industry like this, where there may be so little natural product differentiation. Pepsi is an example of a company that did a major corporate transformation based on applying design thinking to products as mundane as a can of soda pop. Just a few years ago, it wasn't clear whether Pepsi's CEO would survive. Many investors saw Pepsi as a bloated giant whose top brands were losing market share. The shelves seemed to be just more and more cluttered with competitive products, so Pepsi had to rethink their innovation process and design experiences for their customers, from conception to what's on the shelf. Now Pepsi teams are pushing design through the entire system, from product creation, to packaging and labeling, to how a product looks on the shelf, to how customers interact with it. A well‑designed product is one that you can fall in love with, or you hate. It may be polarizing, but it has to provide a real reaction. Ideally, it's a product that you want to engage with in the future, rather than just say I bought it and I consumed it. The lesson that we can learn from Pepsi is that commodity products as mundane as cans of soda pop can benefit from a lot of design thinking. Design thinking might need to permeate every aspect of the product, including how it is marketed and presented to customers in order to shift how people think about a product. The next example I want to talk about is GE Healthcare, which is known for MRI scanners, which can be terrifying experiences for children. If you've never had an MRI, I can tell you that it is a very frightening and difficult thing to do, even for an adult. You're typically strapped down to a sliding table in a very cold and unfriendly room and then pushed inside this relatively narrow tube for as long as 45 minutes while the machine makes all kinds of strange buzzing, clicking, and clacking noises while it tries to scan your body. And during that entire time you must be absolutely still. You can imagine what that experience might be like for a small child. Doug Dietz, who was a primary designer of the GE Healthcare MRI system, described the experience. "Everything in the room was kind of like, beige," he said, describing what he calls "crime scene" stickers (which tell patients where to go), and the exclamation mark warning sign on the door. "The room itself is kind of dark and has those flickering fluorescent lights." And he adds "that the machine that I had designed basically looked like a brick with a hole in it." The plan was simple: offer an environment that was so welcoming that children would feel like being scanned is an adventure and not a trial. To do so, they asked customers and children about what they would like during ideation sessions. This slide shows the results of that work. The next example I want to talk about is a very simple process change that was implemented by a chain of university hospitals to improve the way they handle patients in the emergency room. A trauma room in a major hospital can be a very chaotic place. When a patient with a gunshot wound or a motor vehicle accident arrives, a bed is prepped, the right supplies are on hand, and up to 20 nurses, respiratory therapists, and physicians are ready to spring into action. "In a typical situation, a huddle of highly stressed emergency staff room members spoke over one another and there were no clear roles. In particular, no one knew who was leading the trauma code. One very simple innovation made a very big difference. The leader of the trauma team now wears an orange vest. The easy‑to‑spot garment, called the trauma team leader identification vest, clearly identifies who's in charge. It's a simple yet effective innovation created by a nurse after a very hectic gunshot trauma simulation." This situation illustrates the importance of supporting and utilizing innovative and novel ideas from all members of the health care team. Traditionally, hospitals were designed with input from administrators. With design thinking, the innovations come from those who actually work there, providing feedback to designers to improve the final product or service.

Design thinking summary 

In this lesson, we're going to summarize some of the material that we've discussed related to design thinking. First, here's a concise definition of what design thinking is. Design thinking is both an ideology and a process. It is concerned with solving complex problems in a highly user‑centric way. As you can see from some of the examples we've discussed, design thinking can be critical for companies to develop a competitive advantage where product leadership is an important factor. It is especially useful for very difficult design problems, which are ill‑defined or tricky. An important point is that design thinking requires out‑of‑the‑box thinking to create innovative solutions. Humans naturally develop patterns of thinking modeled on repetitive activities and commonly accessed knowledge. These assist us in quickly applying the same actions and knowledge in similar or familiar situations, but they also have the potential to prevent us from quickly and easily accessing or developing new ways of seeing, understanding and solving problems. We discussed the potential stages in a design thinking process. It's important to recognize that these stages are meant to be a conceptual model, and the design thinking process is not necessarily intended to rigidly follow these stages. This slide shows a comparison of agile and design thinking. As you can see from this slide, they have many of the same characteristics. Both are an ideology and a general framework. Both emphasize creativity and innovation. Both require an incremental and iterative approach to find an optimum solution. Both are heavily focused on meeting user needs and requirements. Both might require an enterprise‑level transformation. And both require empowerment of employees to encourage creativity and innovation. The key point is that design thinking and agile are very complementary to each other. Both have some of the very same requirements. In most cases, it is only a difference in emphasis. It requires some level of judgement to determine how important design thinking is in the overall solution and how to integrate the right level of design thinking into the process. In the next lesson, we're going to start a new module on managing flow in agile projects, and the first lesson in that module is Kanban overview and examples.

Managing flow in agile projects

Kanban overview and examples

Hi, and welcome to the next module in this course. In this module, we're going to discuss managing flow in Agile projects. One of the major differences between a traditional plan‑driven project and an Agile project is that a traditional plan‑driven project puts a lot of emphasis on managing the structure of the project with such things as work breakdown structures, PERT charts, Gantt charts, and so forth, while Agile projects put an emphasis on managing flow and efficiency rather than structure. Here's a brief summary of the topics in this module. First, we're going to provide an overview of a kanban process and how it works, as well as an example of a kanban application. Next, we're going to discuss kanban boards, which are very widely used in many Agile methodologies such as Scrum. Then we're going to discuss flow and the theory of constraints, which are important concepts from improving the performance of any agile process. And finally, we're going to discuss cumulative flow diagrams, which is a great tool for tracking and forecasting projects. In this session, we're going to talk about kanban, which is another widely used agile process in addition to Scrum. Where Scrum is used heavily for project and product development efforts that require some level of planning, kanban is generally more suited for continuous flow kinds of operations. However, the principles with flow and kanban are essential to Scrum. The origin of the word kanban is Japanese. It originated in the Toyota production system. Roughly translated, it means a card that you can see. Here's a simple example. Picture yourself on a Toyota production line. You put doors on Priuses. You have a stack of 10 or so doors. As you keep bolting them on, your stack of doors get shorter. When you get down to five doors, sitting on top of the fifth door in the stack is a card, a kanban card that says build 10 doors. Well, it may not say exactly that, but it is a request to build exactly 10 more Prius doors. You pick up the kanban card and you run it over to the guy who builds doors. He's been waiting for you. He's been doing other things to keep busy while waiting. The important thing here is that he's not been building Prius doors. He takes your kanban card and begins to build the doors. You go back to your workstation and just a bit before your stack of doors is gone, the door guy comes back with a stack of 10 doors. You know that kanban card is slid between doors five and six. You got the doors just in time. The kanban card is a tool to signal demand between stages in a process; however, the word kanban has taken on a much broader meaning than that in actual practice. To understand kanban, it is first necessary to understand the difference between a push system and a pull system. A push system is totally planned in advance. An example would be a traditional manufacturing process. It starts with a manufacturing plan based on a forecast of demand that someone thinks will take place. Raw materials are ordered and held in inventory waiting to go into production, and production resources are planned in advance and allocated to meet the forecasted demand in the manufacturing plant. In essence, raw materials are planned, and queued, and pushed through the manufacturing process. A traditional waterfall development process works on the same push principle. Requirements are forecast based on what someone thinks the customer needs, work is planned in advance, and resources are scheduled against the plan. A pull process, on the other hand, is totally demand driven and not planned. An example would be a manufacturing process where furniture is custom built to satisfy customer orders. In an ideal pull system, there is no plan and no work is scheduled until a customer order is received. Raw materials would be ordered only as needed to satisfy customer orders, and production resources would be allocated to work only as needed to satisfy customer orders. This is the inverse of a push process. In a pull process, customer orders are queued and raw materials are all pulled through the process to meet actual demand. All Agile processes are adaptive to customer demand to some extent, rather than being totally planned; however, a kanban process is based on pull and is totally unplanned and adaptive. Kanban is sometimes used as a software development process, but it is more often used in processes that are totally reactive and unplanned such as a customer service response system. In a customer service response system, some amount of capacity is planned to meet anticipated demand for support from customers, but those resources may be mostly idle until a customer calls with a problem. Scrum is far more widely used than kanban as a software development process. Scrum is actually a hybrid process involving both push and pull. In a Scrum process, work is planned to some extent in the product backlog, and it is broken up into sprints and releases. Within a sprint, the work is planned and allocated to the team; however, the whole sprint is demand driven based on priorities set by the product owner, and the product owner has considerable ability to influence what is built. A kanban process is more of a totally adaptive continuous flow model. No attempt is made to plan requirements in advance. The development process operates continuously and isn't broken up into sprints at all, and work is taken into the process immediately as soon as resources are available to work on it. An example would be a customer service response queue where you have some number of customer service agents who are trained to respond to customer support, telephone calls, and provide support. In that situation, it is totally demand driven by customers calling in, and it is also not heavily planned because the nature of the calls can vary significantly from one call to the next. There's a general level of planning to provide a reasonable level of capacity to handle an expected volume of support calls and at least some general level of training to the agents to handle typical support calls, but it is impossible to completely plan all aspects of the process. By its very nature, the process has to be highly reactive and very adaptive. A critical concept in kanban systems is the idea of work in process, or WIP. Kanban processes many times have stages that work goes through. An example is the customer service system designed to solve customer problems. A kanban system is designed to optimize the overall flow through the system, and each stage in the process might have capacity limitations that limit the overall work in process. For example, in a typical customer service response system, there's many times an initial level that you go through that is designed to screen calls and weed out problems that can be easily resolved without requiring more sophisticated resources. And problems that can't be resolved in that stage are routed onto a second or third level of support for further resolution, depending on the complexity and difficulty of the problem. The overall process is designed to optimize flow, and any individual stage in the process might have capacity limitations to limit the overall work and process or flow. When that happens, work waiting to enter the process simply piles up waiting to be done. In this situation, the size of the screening staff would typically be much larger than the more sophisticated problem resolution staff to optimize the flow and efficiency of the overall process. Each stage has a work in process limitation based on the capacity of the resources in that stage, and the flow through the process will be determined by which stage in the process is more limiting on the overall flow. For example, if the size of the level 1 staff is inadequate to handle incoming support calls, customers might wind up waiting in a phone queue for support. If the size of the level 2 staff is limited, more difficult problems might take much longer to resolve while customers wait for level 2 resources. Next, we're going to discuss a few examples of how a kanban process is used in the real world. There are some people who advocate using kanban rather than Scrum as an Agile product development process. And there are some situations where that makes sense. Here are a few examples. The first is a development process that is totally demand driven. An example I have seen is that a group is assigned to produce business reports on demand for users. In that situation, user requests would go into a queue of requests to create reports based on priority, and people would pull items to work on out of that queue whenever resources are available to work on them. The second example is a situation where product has reached a mature stage in its lifecycle that calls for mostly maintenance of adding some additional features and doing bug fixes. Those are the kinds of things that don't necessarily have to be planned. Some amount of capacity is planned to work on those things, and the items to be worked on are prioritized in a queue similar to the previous situation. The last situation is less typical. You could have a product development effort where planning is not essential, and it is desirable to get the work done as quickly as possible. In this kind of situation, a kanban process might be more streamlined and more efficient, but, of course, the caveat is that it is totally unplanned. That might be appropriate for small development projects, but would be difficult for it to apply to large critical projects. An example would be the situation I previously mentioned of producing reports. Producing an individual report is not typically a very complex effort and normally does not require a lot of planning. And finally, it's important to note that kanban is actually a foundation for Scrum. Within a sprint in Scrum, a kanban process is used to manage the flow of the work in process. For example, as this diagram shows, the work in progress in a sprint flows through stages, and a kanban board is often used to track the flow of work through the different stages.

Kanban Boards

The next subject I want to talk about is Kanban boards. Kanban boards are a very widely used tool in many agile methodologies, including scrum. Kanban boards are widely used in scrum to visually show the flow of items through a Kanban process in a sprint. The Kanban board can be as simple as a whiteboard with stickies on it, or 3 x 5 cards on it, that are manually moved around the board to show progress through the flow. The example shown here shows a simple Kanban board as it might be used in scrum. The columns in a Kanban board represent the stages in the Kanban process. For example, in this particular instance, the first column titled Next represents work items waiting to enter the process. The next column titled Analysis represents work items requiring further analysis before they go into development. The middle column titled Development represents work items that are currently in the development process. And finally, the last two items represent work items that are in acceptance testing and work items that have been released to production. Using stickies or 3 x 5 cards on Kanban boards has been widely used by small, individual scrum teams. In this approach, a whiteboard is dedicated to keeping track of work in progress, and individual work items are represented by colored sticky notes that are manually moved around the board as their status changes. The colors of the notes might signify different types of work or different priorities. Using stickies or 3 x 5 cards has its limitations. First, the data on the board is not updated automatically as progress is made. Updates to the status of items on the board are generally made once a day during a daily stand‑up meeting. And second, it doesn't work well for distributed teams that are not colocated, because all the people on the team do not have direct access to the board. It also doesn't provide a management reporting tool. Anyone who wants to know the status of work in progress has to go and look at the Kanban board. Most online tools that are designed to support agile projects provide a capability for Kanban boards. The version shown here was generated by an online electronic tool. This approach solves many of the problems of the informal whiteboard approach using stickies or 3 x 5 cards. The online tool has a number of advantages. The data can be automatically updated in real time as the work is being done, anyone can have direct access to either view or update the information locally or remotely, and the information on the board can automatically feed reporting tools such as burn down charts, and could be summarized and rolled up at any level for management reporting.

Principles of product development flow 

In this session, we're going to talk about the principles of product development flow, based on Don Reinertsen's book, which are very important considerations that are needed to optimize the flow of any agile project. One of the major differences between an agile project and a traditional plan‑driven project is that an agile project focuses much more heavily on optimizing flow, where a traditional plan‑driven project focuses very heavily on the structure of the project, with models such as Gantt charts, PERT charts, work breakdown structures, etc. That's why it's so important to understand how to optimize flow in an agile project. In an agile project, there's normally not enough information available up front to develop elaborate Gantt charts or other structural project tools, and by organizing a project around high‑performance, self‑organizing teams, the structure of the project is considerably simplified. As a result, the structure of the project is not as important. In simple terms, you can think of an agile project as a big pipe, and the goal is to try to push as much stuff through the pipe as quickly as you can. That's why the concept of flow is so important. The most well‑known book on this subject is The Principles of Product Development Flow, by Don Reinertsen. In his book, he identifies seven principles of product development flow. Here's a brief explanation of how to apply these principles to optimize the flow in an agile project. The first principle is take an economic view. An agile project is based on maximizing value, and as a result, a key goal should be to deliver the highest‑value items as quickly as possible. This is done by prioritizing the items in the product backlog by highest business value first and deferring items of lower value. Another aspect of this principle is a recognition of the point of diminishing returns. Produce the simplest product you can, or what is called the minimum viable product, and then enhance it only as needed. Many traditional projects have gotten bogged down because users tend to ask for everything that they think they could possibly want, because they think that if they don't get it into the requirements they'll never get it at all. That's one of the big advantages of an agile project. By prioritizing requirements and working in a collaborative partnership, the users can say at some point that the project has gone far enough and needs to go no further. The next principle is actively manage queues. One of the important principles of optimizing the overall flow through a process is minimizing waste that occurs by having too many items in a queue waiting to be worked on. In a traditional plan‑driven project, you might have a large number of detailed requirements waiting to be worked on up front. Developing requirements that sit in a queue far in advance waiting for development has a number of disadvantages. The requirements might change prior to going into development, and much of the effort involved in developing the requirements might have been wasted and/or speculation in the requirements that is done too far into the future can result in erroneous assumptions that make their way into development without being questioned. The next two principles are very closely related in regard to batch size and variability. Large batch sizes tend to cause bottlenecks and inhibit flow. Imagine trying to push a large number of balls through a pipe. As you can imagine, that process would work better if you had a large number of small balls like ping‑pong balls rather than a smaller number of larger balls like basketballs. The basketballs would tend to get jammed up in the pipe, while the small ping‑pong balls would be much more likely to flow smoothly through the pipe. A related principle with regard to variability is that breaking up requirements into smaller ones that are of more uniform size reduces variability and can improve flow. For that reason, agile processes typically break up requirements into the smallest possible increment, such as a user story, in attempt to keep the requirements to a similar size to improve the ability to estimate and manage the flow of requirements through the process. Another important principle with kanban processes is the idea of work‑in‑process constraints, or WIP. As we discussed in any kanban process having multiple stages, there will always be some amount of imbalance between the stages, and one stage may have more capacity than a previous stage. For example, in a development process, there may be more capacity for doing development than there is for testing, and the output of the development effort could easily overload the testing capacity if it isn't controlled. Another example could be the use of shared, specialized resources in a project that are outside of the project team, such as a high‑level database architect. These resources should be used wisely to maximize their impact on the overall flow. The next principle is control flow under uncertainty through cadence and synchronization. Having a repeatable cadence improves the efficiency of the product development process and allows synchronizing a predictable development process with a much more unpredictable flow of requirements. In an agile project, a cadence is established by using fixed‑length sprints, which are timeboxed to a certain length. Instead of expanding the length of the sprint to accommodate a larger amount of work, keeping the sprints at a uniform length and limiting the work to be included in the sprint to the capacity of the sprint makes a flow of work much more uniform and predictable. The next principle is related to fast feedback. Taking corrective action quickly to adjust for problems and inefficiencies is absolutely essential to optimize the flow in a process. That's a key reason why sprints are kept a short as possible, typically two weeks. It enables rapid learning to make adjustments in the process to optimize the flow very quickly instead of letting problems compound themselves. And the final principle is related to decentralized control. Decentralizing control can enable much more rapid and more efficient decision making. In an agile project, this is done by empowering the people on the team as much as possible to plan and manage their own work and to make important decisions quickly as an empowered team.

The theory of constraints 

In this section of the course, we're going to talk about the theory of constraints. The theory of constraints is a systematic way of analyzing flow problems and making improvements in a process flow. The theory of constraints was originally developed by Elihu Goldratt and was originally published in his book, The Goal. His book is a classic in this area. Here's an example. In a company I worked in, we had a task of taking over 100 legacy applications that needed to be upgraded to a new version of Microsoft Visual Studio .NET, and we designed a Kanban flow with a number of different stages that each application would go through. The first stage was a senior level technical lead would check out the application and do a preliminary survey of the application to identify any risks in the conversion process and flagged those items to the development team. He would also establish coordination with the owners of the application and any other related applications to begin the conversion process. Second, an offshore team was assigned to do the conversion process and to perform initial unit testing. The application was then turned over to QA for formal QA testing, and finally, the application was staged for release and released. We had about a year to do over 100 applications, and the plan was to do about 6 to 7 applications per month. We explored a number of options to try to accelerate this process, and this is a good example of how the theory of constraints can be used to accelerate a process. This slide shows their recommended steps in doing an analysis for the theory of constraints. The first step is to identify the stage or portion of the process that is the most critical constraint or bottleneck. For example, in the customer service system, it may be that there are not enough people to handle calls and to do the initial screening, and as a result, people are waiting for a long time to get through. The second step is to do whatever you can to exploit or optimize that portion of the process to make it more efficient. For example, in the customer service response system, you might add an improved interactive voice response system that would provide some level of initial screening to relieve the load on the people answering the calls. The third step is that after you've done whatever you can to optimize the performance of that constraint, you should subordinate everything else in the process to work within that limitation. For example, if the number of people available to answer the phone is limitation, there is no point in having an excessive number of people in the second stage to resolve problems if those people are idle, waiting for calls to come through. If the process flow through the overall system is still not sufficient, the next step is to do whatever is necessary to elevate that constraint. For example, in the customer service response system, you might add additional resources to answer the phone until that portion of the process is no longer the limiting constraint. Finally, once you've received a bottleneck, another part of the process, then, will typically become the bottleneck. For example, in the customer service response system, if you have relieved the constraint of the front‑end people answering the calls, the people on the back end resolving problems may then become the constraint or the bottleneck, and you need to repeat the above steps with that as the new primary constraint. Here's how the theory of constraints can be used in this situation. The first step in the process is to identify the system constraint or bottleneck that is most critical for limiting the flow. In this situation, the system constraint was the resources available to perform step three in the process, which was QA testing. There are a limited number of QA resources to do the testing that was required, and they would become quickly saturated with testing because a conversion effort wasn't that difficult for many of the applications. The next step is to exploit the constraint. Given that the QA resources were the bottleneck, what can be done to optimize the use of those resources to improve the overall flow? We were able to do several things to optimize those resources, including planning the conversion process to spread the load among the QA resources we had as evenly as possible. Once we had done everything we could to optimize the QA resources, it made no sense for the conversion process to go faster than QA could handle the applications. As a result, the application conversion effort needed to be slowed down to match the speed of the QA process, otherwise, a very large queue of applications waiting to be tested would have piled up in QA. The next step to improve the flow would be to add more QA resources to elevate the constraint and to reduce the bottleneck. In this particular situation, that was not an option. Of course, if you add enough QA resources at some point, that will no longer be the most critical constraint, and the bottleneck will move somewhere else in the process such as the number of developers to do the conversion process. We never really got to that point in this particular example.

Cumulative Flow Diagrams 

n this section of the course, we're going to talk about cumulative flow diagrams, which is a very useful tool for visualizing process flow problems. Before we get into what a cumulative flow diagram is, we first need to understand a couple of other commonly used agile measurement tools. The first is a burn down chart. A burn down chart shows the overall rate of completing work over a period of time. It has two axes. The Y‑axis is the planned work to be completed in a given period of time, which typically would be measured in story points or hours. The X‑axis shows the planned amount of time allotted to complete the work, and that could be a sprint, a release, or an entire project. For example, a burn down chart is widely used in scrum to track the work completed in a scrum sprint. The burn down chart assumes that on the average work is completed linearly over the planned period of time. So the planned work to be completed is simply a diagonal line running from the total amount of work to be completed at the beginning of the time period on the Y‑axis to 0 at the end of the time period on the X‑axis. That, of course, is an ideal model, and work doesn't always get completed exactly as planned. The red line in this diagram shows the actual work remaining to be completed. When the actual work remaining to be completed, shown in the red line, is above the planned work to be completed, shown in the blue line, progress is slower than planned. And the opposite is true if the work remaining to be completed is below the blue line. The next item to understand is a burn up chart. A burn up chart is very similar to a burn down chart, except that it is the inverse. The Y‑axis is still the planned work to be completed in a given period of time, which typically would be measured in story points or hours. However, instead of starting with the planned amount of work to be completed and working down to 0 as the work is completed, it starts with 0 and works up to the total amount of work completed at the end of the planned interval. The X‑axis still shows the planned amount of time allotted to complete the work, and that could be a sprint, a release, or an entire project. The burn up chart also assumes that on the average work is completed linearly over the planned period of time, so that the planned work to be completed is simply a diagonal line running from 0 at the beginning of the time period to the planned total amount of work to be completed at the end of the time period. One interesting aspect of a burn up chart is that it makes it easier to show a change in scope as shown here. That shouldn't happen in the middle of a sprint, but it can easily happen in the middle of a release or a project. Another difference is, instead of showing the work remaining to be completed as a red line, the burn up chart shows the actual work completed as a red line. The relationship is also the inverse of the burn down chart. If the actual work completed shown in the red line is above the planned work to be completed shown in the blue Line, progress is faster than planned. And the opposite is true if the work remaining to be completed is below the blue line. A burn down chart or a burn up chart is a great tool, but they both have a major limitation in that they only show the total amount of work completed or remaining to be completed, and they don't provide sufficient visibility into how the work is progressing in each stage of the process or where the bottlenecks might be. If you want to improve the process flow, you probably need more information to know what the problem or bottleneck is and what could be done to improve it. A cumulative flow diagram is similar to a burn up chart, but it provides much more detail. If we look at this diagram as an example, the bottom portion of the chart showing the items that have been deployed is similar to a burn up chart. It shows the completed work. However, the cumulative flow diagram goes beyond that and also shows the items of work that are in each stage of work at each given point in time. For example, in this diagram, there's a nice, smooth flow of work through all of the stages of work in the process. At each point in time, there are two items of work in Acceptance and two items of work in Development. This is what you would consider a normal cumulative flow diagram. The important point is that cumulative flow diagrams enable us to measure how efficiently we're delivering valuable working product to the customer and indicate where we need to focus our process improvement efforts. The value of cumulative flow diagrams really comes into play when there's a need for process improvement. Here's an example. Suppose your burn up chart looks something like this. This definitely indicates a somewhat abnormal situation, and it's fairly typical in the real world. What this burn up chart shows is that the work completed lagged way behind schedule for most of the time period, but somehow the team made a crash effort at the very end to meet the goal and came out on time. Although this effort completed successfully, it probably isn't a very efficient mode of operation to crash this amount of work at the very end of the time period. In addition to not being very efficient, teams that operate in this mode are not very predictable and reliable to produce consistent results. However, this burn up chart doesn't give you enough information for diagnosing where the bottlenecks are in this process and what can be done to improve the flow to make it more efficient and predictable. This diagram shows what an abnormal cumulative flow diagram might look like. What we see in this diagram is, instead of a nice, even and consistent flow of work through all stages of the process, there's a big spike in the middle of the process where there's a peak of 10 items in Development at one time, and the spike then gets transferred into Testing and Acceptance. It's easy to see how this mode of operation is less efficient than one in which there's a nice, even flow of work through the process. It is also prone to lower levels of quality because what happens in this process is that a large number of items are completed in Development at one time and then are transferred into Testing. Testing is then under pressure to completely test a very large number of items in a very short amount of time in order to complete the sprint successfully. With this information, we can then dig further into possible reasons why this problem is occurring and take corrective action to try to improve the process. For example, the root cause of this particular problem might be that development is delaying taking on new work and then taking on too much work at once in order to try to complete the work required for the sprint. A cumulative flow diagram provides a lot more information about what is going on in a process. Two of the most significant pieces of information are work in process, or WIP. The vertical extent of any area at any particular point in time shows the work in process in that stage of the process at that point in time. And next is cycle time. The horizontal extent of any area over a period of time shows the average cycle time required to process a given number of work items in that particular stage of the process. Cumulative flow diagrams require a lot of data. For that reason, they would be almost impossible to create manually. However, they can be created very easily if the team is using an online tool for agile project management. In the next and final module of the course, we're going to do an overall course summary.

Overall Summary 

In this final module, we're going to do a brief overall summary of the course. There are four key points that I want you to take away from this course. The first is that in order to be excellent in agile and develop truly high‑performance teams, you have to focus on the principles and values and not just the mechanics. The second is to recognize that there are stages of learning in agile, and no one ever becomes fully proficient. It is an ongoing journey of continuous learning. The third key point is that it's important to fit the approach to the project, rather than just force fitting a project to a canned, predefined approach, and it takes a lot more skill to do that. And finally, use systems thinking to better understand agile at a deeper level. We're going to discuss each of those points individually in this summary. The first point is to focus on the principles and values and not just the mechanics of how to do agile and scrum. It should be understood that the effective execution of an agile process requires highly trained people. If it is done correctly, it is not just a matter of mechanically following a process by the book. Scrum is meant to be an adaptive process, and that requires a fair amount of skill and judgement to know how to adapt it to fit a given project and business environment. It is also not meant to be a static process that never changes. Continuous improvement is essential as a project is in progress, and that also requires knowledge and skill to determine how to optimize the process to make it more effective. It is well known that the training given to most agile teams is very limited. You can become a fully qualified Certified Scrum Master, or CSM, by taking a short course that is only 2 days long. The result is often that many times people learn the mechanics of how to do scrum and wind up following the process rigidly by the book. It is ironic that that is exactly the opposite of what was intended in the Agile Manifesto. Achieving excellence in developing truly high‑performance agile teams requires a significant commitment to learning that goes well beyond the basic scrum practices and focuses on a deeper understanding of the principles and values behind both agile and scrum, as well as more broad‑based principles and values based on Lean and TQM. Second, it's important to recognize that there are multiple stages of learning that are needed to reach the level of a truly high‑performance team. There's a model from the martial arts that I think fits very well that shows three major stages of learning, shu, ha, and ri. In the shu stage, the student learns to do things more or less mechanically by the book, without significantly deviating from accepted rules and practices and without improvising any new techniques. This stage is equivalent to a new, inexperienced project manager following PMBOK or other accepted practices by the book without necessarily adapting those processes to fit the situation, or a freshly trained scrum master following basic agile practices by the book. In the ha stage, the student learns to understand the principles at a deeper level and learns how to improvise and break free from rigidly accepted practices. But it's important to go through the shu stage and gain mastery of the foundational principles before you start improvising. Finally, in the ri stage, the student gets to the highest level of mastery and is able to develop his or her own principles and practices as necessary. There's a logical progression of learning through these stages, and it takes time to make this progression. A very important point is that you have to learn the foundational principles before you can improvise. Improvisation without knowledge is just amateurish experimentation. The next key point is to fit the approach to the project. As we discussed in the agile versus waterfall course, many people make the mistake of attempting to force fit their projects and business environment to some kind of canned, predefined approach, and that can cause a lot of confusion and consternation because it doesn't often result in a good fit. A better solution is to fit the approach, or combination of approaches, to the project and to the business environment. That requires a lot more skill. It requires knowledge of a broader range of methodologies, both agile and plan driven, as well as a deeper knowledge of the principles behind those methodologies, and the systems thinking approach to visualize how it all fits together in a well‑integrated approach that is well aligned with the business environment that it is part of. Finally, it is critical to understand and develop a systems thinking approach in order to see agile and the business environment that it is part of at a deeper level. Systems thinking is a perspective of seeing and understand systems as wholes rather than as a collection of parts. A whole is a web of interconnections that creates emerging patterns. The key message is, don't just accept things at face value, and mechanically following a process without questioning why it is done that way and without determining if there's a better way to accomplish the same result. Thank you very much for taking the time to take this course. I hope that it has been helpful to you. I am very interested in all of my students, so please keep me informed of your progress in implementing these ideas, and let me know if I can help in any way. Thank you.

Cybersecurity/ Infotech

Introduction 

In the early years of computing, security often wasn't much more than having a lock on the data center, asking people to change their passwords once a year. So, yes, things have changed quite a bit. Today the word Information Security and some people use other terms, like InfoSec or Cybersecurity, but for our purposes, we're talking about the same thing. To think about all aspects of computer systems, from the physical hardware, computers, networks, all the devices connected to those networks, not just laptops and phones. But these days, we're also talking about alarm systems and cameras and door locks and thermostats, the different software applications that all of these devices run and, of course, huge amounts of data, intellectual property, customer information, documents, databases, emails. Securing all of this is a critical and ongoing task for every organization with a direct impact on your bottom line and your ability to be agile in the market. So, to be clear, what we're about to cover will not just be a few typical personal security recommendations, like you should have strong passwords or use two‑factor authentication. Yes, that's important, but our focus here is security at the organization level. The different ways this impacts the companies that you work for, the projects that you're a part of, and the teams that you work with. We will begin by taking this vague idea of security and breaking it into more useful, specific areas. Several functions that work together mitigating different risks to your organization and combining to form the suit of armor that protects it. Any gaps in these business functions represent different types of vulnerabilities that open you up to various threats. As we talk about how to close these gaps, we'll go over some security terminology and jargon, and there's a few terms that you've probably heard of already. But there's others that might be new to you, because yes, like everything else in technology, the security landscape is always changing. There are always new kinds of attacks, new vulnerabilities that malicious actors will try to discover and exploit. But whatever your role is, you can be actively involved with this without needing to become a security expert, because over the last few years a lot of work has been done to develop principles and best practices. We have structured repeatable, cost‑effective approaches to plan and think about security. They'll bring our attention to different aspects, make sure that we avoid any blind spots, and in some cases, even just allow us to have meaningful conversations about it. To make us, and the organizations that we work with, better protected and less vulnerable. Welcome to the Executive Briefing on security.

The Threat Landscape 

Okay, nobody will disagree with the idea that security is important, but they will disagree about what that means. They'll even argue about what the most significant security threat actually is. And it's easy to get the wrong idea about this. When you hear about businesses being hacked, those high‑profile attacks that make the evening news, often there's a common theme. ‑The personal data of over 50 million customers was leaked onto the internet after one of the worst hacks in corporate history. ‑Hotel company revealed that this recent attack on its computer systems has compromised the data of 400 million users, including credit card and passport information. ‑The latest cyberattack has exposed the confidential information of over 100 million customers. Were you affected? Tell us how you feel. Call in at 1‑800‑555. ‑It's very easy to see these and think to yourself, clearly, the main focus of security is to stop hackers from stealing our customer data. I have a full understanding of this. But being over focused on one potential threat, or what we call on attack vector, can distract us from the many others that are often far less dramatic, less newsworthy but just as significant. Many security risks are simple, they're mundane, and honestly, kind of boring. For example, a disgruntled employee on their last day accesses an internal server and deletes or corrupts a bunch of internal project planning documents and data. You would not see this making the news. There was no hack, there was no leak, your customers' data was unaffected, but it can be just as impactful to your business. Some vulnerabilities are external, but many are internal. And even if attacks do come from the outside, they won't have the singular focus on getting your customer data. Today's attackers have a wide range of motives that go well beyond that. Some try to make money through ransomware to install software that will effectively lock down a computer and everything on it and only unlocked when you pay them. If this software gets onto your laptop or your desktop, it's for sure a major inconvenience, but if it gets onto the server that hosts your website or holds your corporate database, that's a whole other level of pain. But other attackers aren't looking for money, but instead will take a business offline to make a political point, sometimes called hacktivism. Others don't care about obtaining your customer data. They're looking for your intellectual property to corrupt the data that you use to make decisions or just to gain a back door to your systems so that they can quietly hang around and spy on your operations to gain an advantage. And some attackers don't have any goal at all other than to just generally disrupt a business, any business, because they can. Or worst of all, other malicious actors are looking for ways to leverage cyberattacks to cause physical real‑world harm, destruction of property, or loss of life. And no one is too large or too small to be a target. It's not unusual for an organization to be complacent and assume that if they don't have a big public profile or if they are a smaller business that no one will be interested in targeting their computer systems. But that's not the right perspective because many attacks are basically random and opportunistic. They're the equivalent of a thief walking along the street, quietly testing the door knobs. They didn't set out on a target of one specific house. They're just looking for any vulnerable house to exploit. So we need a way to bring our attention to all of the different areas of focus, the different types of security risks and vulnerabilities. If we're talking about security, what does that mean, and what is it that we're trying to secure? And luckily, there is a simple shortcut, one small word that's been around since the 1970s to help us do this.

The CIA Triad 

For several decades, we've used a simple abbreviation to explain three overall goals of security, CIA. And no, not the US Central Intelligence Agency. Here, CIA stands for confidentiality, integrity, and availability. It sounds almost too simple, doesn't it? Just three words. But we use this to help us evaluate the data and IT functions most critical to the operation and success of an organization or even one specific project within that organization. Let me walk you through what that means. Confidentiality is making sure business systems and data are only accessible to the right entities. And I use the word entities because this can mean people, employees, contractors, business partners. But it can also mean other technology systems. Our HR system needs to talk to their payroll system. Their website needs access to our inventory database. But still, we need to keep our secrets, well, secret and share information we want to share only with those that we intend to share it with. And when an organization is unable to keep personal information confidential, it can cost millions, in some cases hundreds of millions. Integrity is about making sure your systems and data are what they're supposed to be, that we can trust the information is accurate. We can trust everything is current, up to the moment, and that there's no corruption. Nothing's been added or changed or deleted. And in some situations, manipulation of trusted data can cost not just dollars, but lives. Think of computer systems that run power plants or emergency service dispatch or air traffic control. That data has to be accurate all the time and trusted or people's lives are at stake. And availability is about making sure your systems and data are online whenever they're needed, accessible from the right location. Not just ensuring that internal systems are up and running, but also that there's nothing outside interfering with anyone's ability to reach us. As a simple example, if your organization has a customer‑facing website selling products or services, every second that site is not available is money lost. So all three goals are important, and I'll say that again. All three of these goals are important. But they're not equal, and you can find them prioritized in slightly different ways from organization to organization and even project to project because all of them have a financial impact on organizational cost. And improving one aspect does not always help the others. As just one example, if you decide confidentiality is the single most important factor, you might make decisions that downgrade the availability by only allowing access at certain times or restricting connections from some locations. But with these three ideas in mind, we can drive a little deeper into the different aspects of security that make up your overall risk mitigation strategy. But a quick sidebar. While security today is an immense, multifaceted topic, I will say again, you do not have to become a security expert to engage with this. And here's what I mean. If I'm a homeowner and talking with my next door neighbor, we can have a conversation about the importance of a good lock on the front door, and neither of us need to be a locksmith. If we see cracked tiles on the roof, we can understand how it can make it vulnerable. We can recognize drainage issues or see exposed wood as a risk factor for certain pests. And sure, sometimes we may bring in experts, but it's completely possible to understand and recognize common issues, plan for, and even implement solutions without requiring deep technical knowledge. But let's keep this homeowner analogy going a few more seconds. If you've ever done any modest home renovation, you know how quickly your to do list will become overwhelming if you don't start to categorize all the different things that need to be done. You might group these tasks room by room. Here is the list of things that we need to do in the kitchen, and here's the list of things that we need to do in the bathroom. Or you might not go room by room, but instead break it into different aspects. Here's all the electrical stuff that we need to do. Here's the plumbing work that we need, the cosmetic list versus the structural alterations. Here's the ongoing maintenance tasks and so on and so on. And however you choose to categorize tasks, there's always some crossover in dependencies. But still, breaking them into different areas of focus make it easier to think about, plan for, and achieve. And it's the same here. So while there are many different ways that you can organize the elements of security, we're going to break it into six areas, and we'll go over each one to explore what value it brings to the business or, in many cases, what value it protects. We'll begin by looking at the security of various products we build and use, and this includes software applications and infrastructure. We'll then shift our focus to access management, controlling who can get to our systems and what they're allowed to do. After that, we'll cover specific issues for securing our data, an ever‑increasing focus for almost all organizations. That will lead us to governance where we'll also touch on risk management and compliance. Understanding those first four will help us in exploring security operation, which includes secure administration. Finally, is cyber intelligence and testing, a key component of a mature cyber defense and a distinct important element for your overall security plan. So let's get started.

SideBar- Hackers and Actors 

A quick sidebar. In movies and TV, if you have a character who attacks or compromises a computer system, they're usually called a hacker. Nowadays it's universally been morphed into a term to represent a villain, and they're surprisingly easy to recognize. They always seem to wear a hoody. They work alone, sitting in a dark room, furiously typing green text on a black background. The reality, of course, is not so simplistic. Compromising a computer system doesn't have a dress code, and it's rarely about how fast someone can type. Even the word hacker is problematic. It has a long history, and not all of it's negative. Describing something as a hack can mean just a quick and dirty fix, a clever solution, not necessarily an attack. So in the security community, while a hacker is, of course, understood these days, it's more common to use threat actor or a malicious actor instead. It's more clear and more intentional usage. And threat actor or malicious actor can mean an individual, but it can also mean an organized group, whether that's a criminal group or even a state‑sponsored organization called nation‑state actors, so we'll mostly use those terms moving forward. End sidebar.

Product Security 

We'll begin by talking about the area that a lot of people have in mind when the word security comes up. Product security is about making sure any products that you build and/or use, including software, are designed to be as secure as possible. And this focus doesn't just look at products we develop in house, but also the commercial software that you have paid for and implemented and infrastructure and platforms that your software runs on. If you've heard the term application security, which is identifying and fixing security vulnerabilities in your own software applications, that's part of this larger product security story. As you purchase, install, or develop any new system, whether it's part of your own infrastructure, a new software application, or a product that you are prepping for market, it should be securely designed from the outset, following current industry best practices and including security specialists in the process. This is the time that you want to bring in the experts. They're the ones who know what you're up against, and they can help you make sure your new systems have the best possible chance of being secure from the start. And even if your developers have secure coding policies and best practices, we recognize that they aren't static and must continually evolve as we learn of new and emerging trends, vulnerabilities, and exploits. And this means your development teams need to be continually learning and growing their skills as well. This includes your infrastructure teams too. Their skills need to continually evolve to maintain a secure infrastructure over time. So what are these security best practices when it comes to building and using applications and infrastructure? Well, and this is a very common answer to questions about security, it really depends. For example, the best practices to build a more secure on‑premises infrastructure where you own and control all of the hardware are different from those that you'd use when outsourcing parts of it to a cloud provider. And security best practices differ based on the kind of software, web applications, mobile apps and desktop apps that all have their own specific issues and between programming languages as well because there are technical implications of how different languages are implemented on different infrastructure. The point is to be looped into the industry best practices that apply to the specific technology that you are using and then expand on those within your teams to meet your specific needs and to mitigate the specific threats that your organization may face. But that same dedication needs to go to your existing systems. Conduct security reviews and decide what security concerns might exist and how you can mitigate them. If you've had a system running for 5 years, it may have been secure back when it was first implemented, but you need to continually review infrastructure, code, and more to make sure that is prepared to deal with today's threats. You also need to implement policies and practices that help mitigate the risk to older systems, such as controlling who can access them, from where they can be accessed, as well as implementing monitoring capabilities for vulnerabilities that you can't patch because some of these can't be patched in these older systems or they will break the application. And I'll keep coming back to one idea. Security is a continually moving target. It's something that has to be part of the day‑to‑day routine for every part of your technology estate. Whenever you decide to build or implement a new system of some kind, just be aware that you're making a long‑term, permanent commitment to keeping that system secure as the threat landscape evolves and changes. Your organization likely uses enterprise software platforms that you buy or lease from someone else, SAP, Oracle, Salesforce, Workday, you name it. You need to understand how those systems are secured. More importantly, you need to understand what elements of security will be covered by the vendor and what elements of security will not. These days, it's common to see a security responsibility matrix for cloud providers to make it very explicit what they take responsibility for, like the physical data center and the machines themselves, versus what they expect the client to handle, like users and account management. Your team needs to understand how to harden those systems against attack, how to monitor them, just like you would monitor your own assets. And when it comes to products, software, and infrastructure, the thing to remember is that nothing is secure forever. Systems are updated, configured, and developed in real time. And with each change, you are creating the potential for a new vulnerability. There are teams of people, both benevolent and malicious, poking and prodding at hardware and software systems alike, looking for vulnerabilities and creating exploits. I myself am occasionally one of those people, depending on the hat that I'm wearing. So even when nothing is being changed, old systems become vulnerable as new vulnerabilities are found. Part of any organization's core competency needs to be keeping their systems, both the ones that they build and the ones that they buy, capable of dealing with the modern threat environment.

Identity and Access Management 

Identity and access management, often shortened to IAM, is part of security that deals with two simple, but incredibly important questions. First, who are you? And second, what are you allowed to do? We begin with identity. These are the mechanisms that you have to have to define and validate who's who in your organization. These might be a single directory users log into to prove who they are, or it might be multiple different directories that authenticate users for specific purposes. But there's more to identity management than just creating user accounts and making people change their passwords. We recognize that there are different types of identity, ways to represent employees or devices, partners, contractors, or other software systems. But nothing in the business world is static, so beyond the process to create accounts, you also need processes to regularly review all accounts and make sure that they're still needed because a neglected or orphaned account that still has access is a really great way for a malicious actor to gain entry to your systems. Some of these processes can be automated, like automatically removing a contractor whose contract has ended, or they might be part of a larger process, like when an employee leaves the organization, or maybe even something that's manual from doing a periodic review. But there are important aspects beyond just, is this account active or is it not? For example, if we have an employee based in Iowa, but their credentials are suddenly being used to log in at 3 a.m. from halfway across the world, that might be legitimate, but it could indicate that their account has been compromised and someone is attempting to impersonate them. This kind of situation is something that your systems should automatically be raising as an alert and where you would have someone to receive that alert and quickly look into that situation. And that's exactly what security operation centers are for, which is something that we'll get into in a few minutes. But even when you can confidently match real‑world entities like employees to an account and validate their identity, there's the separate issue of maintaining what they should have access to. If we create an account for a new employee and they've successfully logged on, that does not mean that they should instantly be able to change the home web page of our website and delete all of our databases and then access the payroll system and make themselves the CEO. There are multiple levels of permissions and privileges, even when someone's proven their identity. And just like identity has a lifecycle, what the identity has access to also has a lifecycle. If someone needs to use a system, does that always mean forever or until the job role changes? Or do they just need it for a month, or just a day, or maybe just an hour? There's a security guideline called the principle of least privilege that wherever possible, any user account should only have the absolute minimum permissions necessary to complete the current task. And this doesn't just apply to entry‑level employees, in fact, it's more important for advanced administrator roles. Many organizations have a separate set of elevated administrative privileges with access to the most critical assets in the organization, and one bad apple can subvert your entire system, so it's important to closely limit the scope of what an administrator can do at any moment. So smart organizations have processes that remove most capabilities from their administrator's users accounts, and when an admin needs to do something advanced, they use special tools to temporarily request elevated permissions, and when they do that, it will trigger an alert to log that request. In normal use, the security systems will ignore that alert because the action is part of the admin's normal job, it's part of their behavioral baseline. However, if it occurs too frequently over a given period of time, it might indicate a problem that someone should look into. But most of the time, the admin can finish the task they are performing, and their elevated privileges will be automatically downgraded. They're able to do their job, but in a controlled and monitored fashion that keeps everybody safe. If that seems like a lot of work, well, it really is. And that's why many organizations use access management tools to help automate that work. With the right tools and skilled people to implement them, you can create an actively secure environment without creating a lot of manual repetitive work for your teams. Identity and access management are the foundation for everything else that your security plans are built on, and so it's important to take the time to do them right.

Data Security

Today's businesses accumulate an incredible amount of data and not just the usual suspects like customer addresses and order information. We have data gathered from advertising campaigns, customer behavior, and these days, they've been generated by machine learning artificial intelligence systems. Beyond that, there is a company, financial, and operational data, legal data, proprietary engineering designs, and the list goes on and on and on. Whatever you have, it's your most valuable intellectual property. Your competitive edge, hence the security of your data that's paramount. Companies need to have a plan for securing data, and you can break that plan down into three basic elements. We start by securing data at rest wherever it's actually stored and this might be on your own on‑premises servers, but it also might be in a cloud provider, it might be data in an electronic document, or it might be in a database, any of your data, no matter where it is stored and no matter what form that it's in. We think about how this at rest data is protected from accidental or unauthorized disclosure, even protected from physical theft. So first, we need that well‑defined identity and access management that we just talked about earlier, but on top of that, we can also implement encryption so that even if other layers of security fail and the stored data is stolen, the data is encrypted and not usable, and we have effectively mitigated that security threat. This concept of multiple layers of fail safes is described as defense in depth. Next, we think about how the data is protected when it's in transit being communicated from one place to another. Again, various times of encryptions are used to make sure that data in transit isn't seen or modified by anyone else. And finally, one thing people often forget about is the security of data that we've generated or received from someone else or someplace else. Can that data be trusted? Did we receive it in a way that's secure so that we can verify it wasn't tampered with? We are likely making business decisions with that data, so we want to know where it came from and where it has been. Another way to think about these three elements is where is our data? Where did our data come from, and where is our data going? Making sure that you have a plan to provide the right level of security in each case is the key to an effective data security program. Data really is the lifeblood of your company. You can imagine how important it is to make sure that your data is not only protected, but also trustworthy. Smart businesses make all of their big decisions based on their data and information, and you want to ensure that you're operating from a safe, secure place when you do so.


Governance 

Most organizations use the words governance, risk management, and compliance to refer to distinctive activities that each have specific goals and outcomes. In practice, though, they're quite interrelated, and some businesses use terms like GRC or integrated risk management to cover all of them. First, let's talk about governance. This is one of those words that you see used in politics and corporations, land management, healthcare, and it can seem a bit ambiguous and vague. In the context of security, governance describes how we formalize those strategic high‑level responsibilities, policies, and procedures around security efforts. And while there are specific low‑level security techniques and practices your developers and administrators need to focus on, governance describes the higher level decisions to drive your security goals. How do you ensure your overall security approach stays intact and relevant in the hustle and bustle of day‑to‑day business? How do you decide on security priorities and share information across teams? What processes and procedures are in place to ensure that security won't be forgotten in a moment of pressure? Governance also includes understanding the involving threats that your organization faces and developing plans to mitigate them. It even includes the human side of security, such as establishing policies that direct employees, contractors, and others to take the best approaches to staying safe. So now let's look at risk management. This is a big part of any effective security plan, and it's not just about identifying the risks that your company faces, it's about weighing those risks against the cost of mitigating them. Think about it this way. It is easy to achieve complete security, just shut off all your computers, unplug them, and go home. That will keep everything secure, but it won't really make for a very effective business model, will it? So risk management is not about preventing every possible risk. We can't do that. What we can do is evaluate the risk from plausible threats and assess the potential business impact. The result is either a qualitative, or even more detailed quantitative value that serves as the basis for a level of security applied to mitigate the risk, or in some cases, to offset the risk by transferring it to a third‑party like a cyber insurance company or a managed security provider. It's about understanding the risk and managing it to whatever degree the business requires. And for many companies, that takes us into the realm of compliance, which typically means the need to demonstrably conform to some external security requirements, like laws or regulations. These vary significantly by region and business function, but examples include the European Union's General Data Privacy Regulation, or GDPR, or in the U.S., the HIPPA security rules for healthcare information. Your risk management activities will be based in part on the compliance that you're required to meet, and your governance activities will need to take those into account on an everyday basis. That includes not just securing your data, but maintaining that security and being able to report adherence on an ongoing basis as well. And how do you know that you're doing all the right things when it comes to risk management and governance and compliance? One way is to partner with a respected consulting firm. Most of them have created assessment models that let them review an organization and determine if they're seeing the right kinds of activities and processes happening. It's like a second set of eyes filling out a scorecard. They let you know when you're doing the right things, and they let you know when you have room to improve.

Secure Ops 

In this section, what we're calling Security Operations, we'll cover two topics. First, there's secure administration. This is really just your ordinary IT operations team doing their job every day with a firm focus on security at all times. They're managing your servers, administrating your network and other infrastructure, deploying applications, backing up the email server, and everything else that they do every day, all done with the firm awareness on the importance of security in every action. You can't underestimate the role that secure administration plays in your total security picture. Everything from making sure that only the right systems and the right devices can attach to your network to ensuring that those fancy smart light bulbs are updated and secure all falls under secure administration and operations. It's a lot of work, and it goes on every single day. Then there's the dedicated set of activities often handled by a SOC, or security operations center. These individuals are the moment‑to‑moment operators of your dedicated security activities. They're the ones watching your entire environment moment to moment and responding to incidents. They are the...hold on a second. Excuse me. Speaking of incidents, yeah, okay, I think we're good. Looks like our security operation center is already on it. But that's exactly what I'm talking about. We registered some unusual activity, a potential security problem, and our systems are configured to send alerts. But someone has to see those alerts and respond to them, which is what they're doing right now. It could be anything from an administrator suddenly resetting a lot of account passwords or an application seeing an unusual number of logins from an unusual location or, well, really, just anything. And, you know, this is a really good example of how some of the major pieces of security fit together. For a lot of organizations, it starts with activities like software development, one of the places where new technologies and systems come to life. A software dev team focused on security might make sure their app does things like run security self‑tests before it starts and making sure the application is instrumented to send activity events to a central location. The operations team works to ensure the infrastructure is secure, and that might include making sure a central location is available for all those events and alerts to be sent to. And the security operation center receives and responds to those events and alerts, deciding if something is a security problem, and if it is, handling it. You've probably heard of this term, DevOps, which, to use a really short definition, means the developers and operations individuals in the organization working closely together to put applications into production. But that trend is now actually going a step further to DevSecOps, which pulls in the security teams in the loop from the outset. The idea is to create and deploy technologies that are easily secured right from the start, and to make sure both operations and security operations teams have what they need to do their jobs effectively. It's a great way to make sure that you're not only building secure systems but that you're building systems that are designed to be monitored and kept secure over the long haul. But, as well as the day‑to‑day operational procedures, we should also have procedures in place for our response to any security incident. How do you conduct postmortems after an incident? How do you measure the impact of an incident on the business, make sure any fixes applied will work, and ensure the same thing can't happen again? And this leads us into cyber defense.

Cyber Intelligence 

Finally, is cyber intelligence and testing. It's all about how your organization prepares to combat specific threats and what processes you have in place to deploy those defenses when the need arises. It's about your resiliency in the face of attack. Some organizations roll their cyber defense activities into an existing area of their org chart, and that's fine. Others build a special team on the org chart to handle these tasks, and that's fine, too. What's important is to recognize that these activities provide specific and unique value to the businesses and that they need to be things that you do all the time, continuously in conjunction with the other security functions. Let's start with threat intelligence. This is how your business knows on a day‑to‑day basis what you're up against, what's going on in the world, and how it might affect you. Was there something in the news that might cause your specific business to become a target for hackers or for people looking to make a public statement? What new attacks like ransomware or malware are suddenly in circulation? This means keeping an eye on the news on industry reports and more all on the ongoing basis. Penetration testing, or pen testing, is another important activity. This is where people deliberately try to gain access to your systems and data without authorization. They try to find the holes in your security so that you can patch them before a real threat actor gets in. These people might be working for you, or they might be consultants that you hire. They're generally interested in testing your actual technology systems, which is an important part of a strong defense. A step up from pen testing is the idea of red teams and red team operations against blue teams, which tends to be a more holistic activity. The red team role, often performed by trusted outside consultants, pretends to be an actual threat actor. Like a pen tester, they're trying to gain access to your systems, but unlike a pen tester, they'll try to use every dirty trick that the actual adversary would and proceed past your external boundary to have persistent post exploitation operations. This allows for your defense to be tested over a longer period of time and focus on achieving specific outcomes or actions on their objectives, following the full lifecycle of an attack. The idea is to expose the holes wherever they may exist in your organization so that they can be fixed. They may try to use social engineering against the people in your company. Let me show you how that works. Social engineering isn't a technology, quite the opposite. It's low tech, even what some would call no tech. It's using psychological ways to get information from people or to get them to do something. This includes phishing emails or text messages. Even something as basic as just phoning some random employee at the company and saying, ‑Hello? hey, hey, it's Bob from tech support. Yeah, yeah, we're running a security audit. Oh, I know. It's always something from us, right? Hey, look, could you tell me what you're currently using as a password? Uh huh, yeah, uppercase P? Oh, is that your cat's name? Oh, that's adorable. Yeah, yeah, that great. Thank you so much. You're good to go, and thanks again. Have a great day. Yeah, yeah. You, too. You might be surprised at just how often that can work, and a red team will do those kinds of things. Testing not only your technical defenses, but also your processes, and procedures, and people. A red team can work entirely on their own just like an organized adversary would. Sometimes, though, you might want to run scheduled red team versus blue team exercises where your own security staff, they're the blue team, actively respond to the red team, cut off their access, mitigate the red team's threat, and so on. It really becomes kind of a cyber knife fight. It's a great way to test not only your technology, but also your processes, and procedures, and, most importantly, your people. An organization's approach to security, that is, the specific tools that they use, the actions they take, and the processes that they use will always be changing. After a red team test, for example, you may modify your processes or engage in different kinds of security training for your teams based on the results. After an actual incident, you will conduct a postmortem to consider permanent mitigations for whatever permitted the instant to occur, also known as the root cause. You'll engage in threat modeling to test your implementations and create confidence that they're actually effective. It's always an evolving process, anticipating and responding to whatever the world throws at you. Many organizations will appoint a dedicated chief information security officer, or CISO, to oversee these specialized cyber defense efforts, along with all of their other security‑related efforts as well. Some organizations will rely on a trusted consulting partner to provide these services and still others will put these efforts directly under the CEO or another executive. But wherever these functions fit in the org chart, you hopefully can see the importance of cyber defense as part of a complete security plan. And let's be clear, a security plan is a mandatory part of any modern organization. It's a way to provide executive‑level direction that embeds security practices throughout the organization, making security everyone's job.

Summary

We've covered a lot at a high level. We've looked at product security, identity access management, data security, governance, security operations, and cyber defense. As you've seen, there's a lot going on with security in our modern world. Organizations need to understand the risks and implement not only appropriate safeguards, but also implement ongoing processes and organizational investments, as well as executive‑level sponsorship to mitigate those risks. And remember, it's all about CIA, confidentiality, integrity, and availability. Security is a space every bit as complex and robust as finance, legal, human resources, or any other part of the organization. And at this point, you should have a better idea of what security looks like in an organization, how to think about it, and how it supports the business over the long haul. Thank you for joining me.

Introduction to Information Security 

Course overview 

Hello everyone. My name is Keith Watson, and welcome to Introduction to Information Security. This is the first course in Pluralsight's Information Security Big Picture series. I'm an information security professional at Optiv Security, and I've been involved in information security for more than 20 years. If you've paid attention to the news lately, it seems that security has not improved. If anything, it seems to be getting worse. Between the many organizations that have suffered security breaches and the vulnerabilities that continue to be uncovered, we seem to have a significant problem. And as we continue to rely on technology and online services in our daily lives, these security issues threaten our private information, our personal devices, our businesses, the national economy, and our lives. Government and private industry have responded, albeit slowly and reluctantly, to the threats through increased security budgets and hiring. As a result, the demand for information security professionals has increased significantly. Unfortunately, current projections show that open positions for information security professionals will far exceed the number of professionals available. Through this course and the Big Picture series, we hope to prepare you to join this exciting field and help us face these information security challenges. In this course, we're going to introduce most of the key concepts, methods, and management techniques in information security. Some of the major topics that we will cover include an introduction to key security concepts and principles, governance, risk management, and compliance, the protection of assets through security controls, auditing information systems for compliance, monitoring networks for anomalous and malicious activity, managing security incidents and security operations. By the end of this course, you'll know the basics of information security and should be ready to dive in to other courses in the Big Picture series, as well as courses in the Pluralsight Information Security library. I hope you'll join me on this adventure into information security with the Introduction to Information Security, the Big Picture, here, at Pluralsight.

Security Principles, Governance, Risk, and Compliance 

Course Introduction 

Welcome to Pluralsight. My name is Keith Watson, and this is Introduction to Information Security. This course is your introduction to information security, as well as to a series of Pluralsight courses on information security. We like to call these Big Picture courses because they paint a detailed picture of specific information security topics. While they are not in depth technology-specific courses, they provide a high-level overview and a discussion of key concepts that you need to know. Together, these courses will get you started in learning more about information security and improving your career options as an information security professional. In the last 30 years as more organizations moved online, they have been forced to consider security more than they did in the past. They've had to develop an information security program to manage how they collect, store, and share information with employees, customers, and partners. These programs are developed around understanding what the business does and how it does it, which includes the overall business objectives and its willingness to take on some risk to meet those objectives. To move the business forward and manage those risks, protecting the business becomes paramount, which will include the business information and physical assets, the current financial assets and ability to bring in revenue, the reputation of the business, the ability of the business to sustain operations during business or operational challenges, and the regulatory environment in which the business operates. While I use the word business here, these ideas apply to nonprofit and governmental organizations as well. All organizations have assets. These could be anything of value. Organizational assets can include intellectual property, which includes valuable intangible creations. These creations include intellectual goods such as written works, business processes, designs, software, games, music, videos, and many other types. Most of these are legally protected by trademarks, copyrights, patents, or are defined as trade secrets. In today's business environment, there's a greater need for and reliance on information about clients and customers. Organizations have collected more data and analyzed and developed context around it to build a valuable store of information and insights into their customers. Organizations carefully construct recognizable brands, and strive to establish a positive reputation in the marketplace. These are investments that allow organizations to distinguish themselves with their customers and drive sales. Without access to these assets, an organization could struggle to exist. If competitors had access to this information, or could impugn or damage the organization's brand or reputation, an organization could struggle to succeed or even exist. Therefore, protecting these assets should be of paramount importance to any organization. In my own time in the information security field, I've heard a lot of interesting statements about security. There are a particular few fallacies that I want to address here. You may hear something similar in the future, so hopefully this will help you address them. I want this to be 100% secure. This is one of my favorites. There's a belief that security is absolute, that if we do the right things, we can get complete security forever. However, security is not guaranteed. Threats change over time. Controls can be bypassed. We need to think in terms of strong and weak security, not absolute or perfect. We are compliant, so we are secure. I've heard this one after a department or an organization passes an audit and goes through a compliance assessment. Being compliant does not mean that you're secure. Compliance frameworks define a desired state, typically with a prescriptive set of controls. There can be a focus on tackling only the minimum number of controls with the goal of checking the box on a compliance form. Compliance is only part of a complete security story though. With the right security technology in place, we will be secure. This is an attitude that believes that security equals security technologies. However, information security is more about the appropriate balance of administrative, technical, and physical controls than it is a single piece of security technology. Our awesome IR team detected and stopped the bad guy. So it's all good. Hey, congratulations to you and your team. What are you and your team doing about the areas of the network where there is no monitoring in place? How are you improving visibility through the environment to catch other bad actors roaming your network? Our attitude here shouldn't be on the single victory, but the iterative improvement in capabilities. Securities hard to get right, so we shouldn't worry about it. Or, put another way, security is inconvenient or complicated or gets in our way. Sure, it's hard. You'll get no arguments for me on that. Whether it's hard or not, there are bad actors that will take advantage of your lack of security. We need to understand the risks, apply the right level of security, and refine it over time. As an industry, our attitudes towards information security must change. We need to see information security as an investment, and like other investments putting time and attention, training, and money into information security now pays dividends later. This can lead to fewer significant security incidents, lower compliance costs, lower remediation costs, and a more agile organization that can adapt to changes in the business and the threat landscape. Information security is also a business enabler. An adaptive information security program can quickly adjust for new opportunities that the organization needs to pursue to remain competitive. It understands risk and can implement controls quickly to enable those opportunities for success with a manageable amount of risk. Information security is also a responsibility of everyone in the organization, not just the information security staff members. The guy working the loading dock can impact security as much as the executive on the 20th floor. It's critical that we train everyone on that responsibility and equip them with tools to protect the organization's assets. One of the common analogies I've come across in the information security field frequently is this one. We put the brakes on cars so we can go fast. The similarities lie in how the use of security controls can allow organizations to operate successfully in risky environments. It's not a great analogy because security controls are not the beginning or end of security programs. However, I think it sets the stage for what is to come in this course. Our objectives in this course are focused on getting you started on your way to learning more about information security. Again, this is a Big Picture course and that's our approach. We are looking at the Big Picture on information security. We have two related objectives. First, we will explain important principles, domains, and concepts. Our objective here is to increase your understanding of the foundations of information security. Second, we will describe how those foundational concepts apply within organizations. Our objective is to help you understand how organizations build an information security management program. So even if you're new to information security or have been a practitioner for many years, this course has something for you. This course is organized into four modules. The current module will cover information security principles, governance, risk management, and compliance. We start with the core principles of security and then discuss the high-level organizational concepts for managing information security. The module titled protecting and defending assets looks at the use of security controls to safeguard the organization's assets. Auditing, monitoring, and testing focuses on auditing those security controls, monitoring systems, and conducting security testing. Managing incidents and operations discusses the management of security incidents, preparing for disruptions, and operations related to security management. In this module, we'll be discussing some of the foundational topics of information security. We will start with a discussion on information security principles. These are fundamental concepts that are applied to information security. Understanding how to apply these concepts is important in creating effective information security programs, implementing security controls, and designing more secure systems. Governance is a way of defining and distributing the responsibilities, roles, and procedures for making decisions on information security for an organization. With this in place, an organization can define specific security objectives, enable the attainment of those objectives, and evaluate performance. An organization needs to understand the various risks that it will face. Risk management is not only about identifying and understanding those risks, but enabling the organization to control those risks to a reasonable level. Compliance is about adhering to a set of defined guidelines, requirements, or objectives that seek to improve an organization's information security posture. These guidelines may be defined internally by the organization or mandated by government, trade groups, or standards bodies as a way of applying common practices across a variety of organizations. Here we talk about some of the more common compliance frameworks such as HIPAA, PCI DSS, the ISO 27000 series, and the CIS critical controls.

Security Principles 

Security principles are the foundational concepts in information security. There are three core principles and other fundamental ones. You need to understand each of them and how they can be applied. Let's talk more about those principles. The three core security principles, often referred to as the CIA triad or just CIA, are confidentiality, integrity, and availability. Confidentiality is the prevention of unauthorized disclosure of information. Basically, we want to make sure that only specific and intended individuals can view information that they are authorized to see. Confidentiality can be mandated in policy and implemented through access controls, system design, and encryption. Integrity is the prevention of unauthorized modification or destruction of information. Essentially, we want to make sure that information cannot be deleted or modified accidentally or intentionally by individuals that should not be able to do so. Integrity can be found in the implementation of file systems, digital signatures, and file provenance. Availability guarantees access to information and information systems to authorized individuals when they need it. When needed is the key phrase with availability. To ensure available systems, redundancy is often used in the design of the primary systems and the supporting infrastructure. Other key principles include the identification of unique individuals requesting access to information or information systems and the means by which they prove their identity in a verifiable way, the process of granting rights to authenticated users to access specific information or to execute specific functions, the methods used to ensure that the sender and recipient of information cannot later deny that the exchange occurred, and the ability to record the activities of individual users and the periodic review of those activities to ensure compliance with policies or for investigations. There are other principles that you may encounter during your studies in information security. So this should not be considered an exhaustive list. In designing computer systems and applications, there are other core principles that should be considered and utilized to protect information. We will explore more of these principles in a later portion of the course on security architecture and engineering. I want to mention the sources of those here. In 1975 Salter and Schroeder wrote The Protection of Information in Computer Systems, which included a list of security design principles. Starting in 1992, and with several updates later, the Organization for Economic Cooperation and Development, the OECD, published nine high-level guidelines. The National Institute for Standards and Technology, or NIST, has published several documents that include security principles. The special publication 800 series includes 800-14, Generally Accepted Principles and Practices for Securing Information Technology Systems, and 800--27, Engineering Principles for Information Technology Security. The Open Web Application Security Project, or OWASP, has created its own list of Security by Design Principles on its wiki. Most of these principles come from Howard and Le Blanc's book, Writing Secure Code.

Governance 

Organizations are led through governance, which consists of the leadership roles and their assigned responsibilities and the rules and procedures for making organizational decisions. Corporate governance is often defined by a structure consisting of a board of directors, shareholders, managers, committees, and policies. Governance also applies to IT and information security. Let's start with the structure of corporate governance as an example. Corporate governance typically starts with the shareholders. They are the owners of the business, and they seek some amount of transparency in how the business operates for their investment. A board of directors represents the shareholders of the business and provides oversight. Each director is often one of the largest shareholders or someone with a particular specialty that can guide the business. The board exists to set the direction for the organization, hire the appropriate managers, and review the performance of the organization. The board hires or is influential in hiring the senior managers of the organization. This typically includes the CEO, CFO, President, and other senior positions. With the board setting the direction for the organization, it is the managers that lead the way and manage the daily operations of the business. Governance can and should be applied to information technology because when appropriately applied, IT is a business enabler. It's also a significant financial investment for the organization. So IT has to deliver value to the business. IT can optimize manual processes, reduce the amount of staff dedicated to specific tasks, automate key business functions, and improve internal communications. The advantages to using IT the right way is significantly valuable to the growth and stability of the organization. If the wrong strategy or no strategy is set for the organization to take advantage of technology, then the business may suffer with outdated, inefficient, or the completely wrong technologies. If IT resources are not managed well, then the business may lose out on the possible efficiency gains and availability of data and systems. IT risks also have to be managed. In addition to the vulnerabilities, threats, and attacks that you may be familiar with, there are other IT-related risks that also affect the organization. Here are a few examples. An expensive IT upgrade project that is not completed, the lack of qualified system administrators for the number of deployed systems, and the repeated failure of the phone system during peak call times. The organization has to define the right strategy, create appropriate policies and procedures, and then apply security controls in the right way to manage IT risks. Governance of information security involves establishing the program that will protect the business and its information and has the following expected outcomes. The strategy for information security must align with the objectives in the strategy of the business. Information security should be an enabler. To enable the business to reach particular objectives, the security strategy should be crafted specifically to support those business objectives. Just like IT, information security is a significant investment for organizations, and must be guided with the right strategy and managed well to deliver the expected value to the organization. The information security program has to understand and manage risks. We'll talk more about risk management in an upcoming section. Just like other parts of the business, an information security program has attached financial, technological, and human resources that have to be managed well and in accordance with the strategy. Information security programs also need metrics and data that can be used to determine how well the organization performs the information security functions. This data must be analyzed and reported. There are also opportunities to identify deficiencies to correct and optimizations needed. Guidance is a general term used to describe the various documents an organization has that define what needs to be done and how it will be accomplished. Policies are the guiding documents of the organization. They are high-level summaries of corporate philosophy and strategic thinking of management. Policies are generally developed by the board and senior managers and apply to all parts of the organization to ensure consistency. Division and department level policies can be created that are unique to those groups, but they must be in alignment with the top-level policies to avoid conflicting objectives. Procedures are derived from policies and contain more details on implementation. These are typically step-by-step instructions written by middle management in alignment with the top-level policies. While a policy may state that complex passwords must be used for user accounts, a procedure would describe how to implement the password complexity requirements that the system should require. Standards provide the technical requirements for implementation. Organizations create standards to group and simplify the security controls that are needed. Standards can specify the types of security controls needed for a variety of technologies that an organization supports. There can be standards that detail how password complexity controls are applied in the various directory services, network devices, Microsoft Windows operating systems, Linux, and Mac OS systems. Baselines provide descriptions of the security controls that must be applied to entire systems. These are used to ensure consistency throughout the organization. A baseline ensures that a desktop operating system will have the same security controls in place regardless of what IT support group deployed it or manages it. It would describe the required password complexity controls for each operating system. Guidelines are optional controls that allow a user or administrator to make a decision on whether to use those recommended security controls based on the circumstances. Guidelines provide best practices and recommended approaches. A guideline could provide details on enabling password features to prevent password reuse, which may not be specified in a policy but is generally considered a best practice. All of these types of guidance documents should be reviewed periodically. If the policies are written well, they will require the least amount of change. However, the other documents will need more frequent updates to stay current with the organization as it changes over time and to adapt to changes in technology and threats Organizations use oversight as a means to set the direction of specific initiatives and to monitor performance over time. One common way of providing oversight is through committees. These offer a way to provide diversity of thought and input from a variety of business units and stakeholders within the organization. Strategy committees are typically board-level advisory committees that provide strategic direction to the organization. Some organizations have general IT strategy committees with responsibilities for security. Other organizations may use a security-specific strategy committee. These committees can consist of board members, senior management, and specialists that provide advice to the board on a variety of IT and security topics related to current and future needs and issues. Steering committees are executive-level committees that assist the senior management with the implementation of a strategy. These types of committees are more concerned with the initiation and management of IT and security projects, allocating money, defining project goals and objectives, monitoring project progress, and reviewing performance. Audit committees are usually board-level committees that are responsible for financial reporting. For publicly traded companies in the United States, they are required by law and have specific responsibilities. One area of focus for an audit committee is on risk management. Audit committees set the practices used to identify and respond to organizational risks, as well as providing reporting on those risks. Change control boards are typically IT management-level committees consisting of IT and business unit members that oversee changes to information systems. Over time, these systems will likely need to be changed in some way, such as software and hardware upgrades, configuration changes, security control changes, and vendor-driven changes. Any change to a complex information system can introduce instability and downtime to users of the system. Change control boards try to manage those changes to prevent significant unintended downtime that can negatively impact the business units that rely on these systems. There are some specific roles and functions in information security that we need to identify. The Chief Information Security Officer, or CISO, or Chief Security Officer, CSO, is the senior executive in charge of information security for the entire organization. The CISO is responsible for establishing and managing the organization's information security program. Security architecture and engineering are technical individuals involved in developing and managing security controls. Some organizations have security architects that have a broad view of security and work with various business units to design and plan appropriate security controls. Security engineers are typically technical individuals that are responsible for implementing and managing security controls. Security operations are typically teams of technical analysts and investigative staff members that oversee the daily operation of the security program. They monitor threats, vulnerabilities, and security controls, investigate incidents, and may develop intelligence on various threat actors to improve their response. Compliance is responsible for ensuring that the organization meets its various industry mandated or legal requirements. Members of a compliance team may include technicians with a deep understanding of the various compliance frameworks and possibly individuals with a legal background. Internal audit typically consists of auditors focused on evaluating and reporting on the organization's management of resources. In addition to the traditional role of reviewing financial records and business processes, internal audit teams look at an organization's approach to risk management, implementation of controls, and governance processes.

Risk Management 

Organizations need to understand risks to the business and reduce them to manageable levels. Let's talk a little more about risk management. Risk management for organizations involves operating with the understanding that there is the possibility that future events may cause harm to the business. The key message of risk management is that organizations can never eliminate risk entirely, but they can reduce it through appropriate management. To do this, organizations have to establish a risk context or an environment in which decisions are made based on risk. The goal is to create a risk management strategy that defines the organization's approach to understanding risk and making decisions about how to manage it. Assessing risk is needed to know what the risks are and how the organization has planned for them. The assessment process looks at weaknesses, threats, and any security controls in place to mitigate those threats, as well as the potential impact to the organization. The result is an understanding of risk. Once the risks are known, the organization has to respond. These can be actions that include evaluating, developing, and implementing options for response to reduce or limit the risk. The organization will change over time. Those changes to the business environment and the potential threat landscape mean that the approach taken to address specific risks will have to change. Monitoring risk over time includes periodic review of the approaches taken to address risk and determining the effectiveness of those approaches. There are some basic principles in risk management. Organizations are guided by these principles in their risk management programs. We will be specifically talking about IT and information security here, but these principles apply to all aspects of business and risk. Risk avoidance is avoiding activities that carry some risk. For example, if an organization knows that placing an outdated and potentially vulnerable information system on their perimeter network increases the likelihood of a breach, then the organization is better off if it decides not to use that system, or at least not place it on the perimeter network. Risk transference or sharing risk is often linked with insurance. But that's only part of the picture. The legal responsibility is not transferred because insurance only provides compensation for losses. Outsourcing your IT infrastructure to a cloud provider is one method of sharing risk in that your organization shares the responsibility for the security of the technology stack with the cloud provider. Risk mitigation or risk reduction involves minimizing risks through the use of countermeasures and security controls. Organizations can deploy security and monitoring tools, configure systems using security guidance, and train staff as ways to reduce their exposure to different risks. Risk acceptance or risk retention means that the organization is willing to accept the loss, should it occur. There may be some risks where the cost of reducing or transferring the risk is high and the likelihood that it would occur is low. The organization may have an old static website with non-sensitive information. They may decide to continue to operate the site knowing that it may be breached in the future from a currently unknown vulnerability. These principles describe the steps needed for organizations to properly manage their risks. In order to apply the appropriate treatments for risk, an organization has to know what their risk is. This is accomplished through process of risk assessment. First, they have to identify vulnerabilities. These are weaknesses that a threat might exploit. Then identify threats, which are events in which there is an adverse or negative impact to an organization. Determine the likelihood of occurrence that a threat can take advantage of a vulnerability, which can be based on intent and capability of that threat. And the negative impact to the organization has to be determined should the threat event occur. Then using likelihood and impact information, risk can be determined, which must be reported to the organization so that decisions on addressing the risk can be explored through the selection of controls. For a good overview of the risk assessment process, take a look at the NIST Special Publication 800-30.

Compliance 

Organizations need to manage their security programs against specific guidelines. Which guidelines they use, how they manage to those guidelines, and how they identify and correct gaps in the coverage of those guidelines are all part of an organization's compliance program. Let's look at how organizations manage compliance. Control frameworks are the guidelines that organizations use for compliance purposes. These frameworks help organizations by ordering and grouping various security controls. Each framework may have a different focus and group controls differently toward that purpose. However, most are based on a common set of security controls. Depending on an organization's current security program, control frameworks can be used to design a new security program or refine an existing one. For organizations that have not invested in security or haven't had a specific strategy around security, control frameworks can be helpful in designing a security program. Control frameworks can also be used to audit an organization's security program. Since frameworks define a series of required security controls, they can be used as a basis for determining if an organization has implemented those controls. An organization may need to follow specific control frameworks for several reasons. These reasons might include legislative or regulatory purposes. Some organizations are required by law or through government regulation to have specific security controls in place to protect the public in general or some specific groups such as investors or healthcare patients. Industry-specific controls might be required. These are typically mandated by industry, governing partners, or trade groups. Industries that are at a higher risk for loss or hold key portions of a nation's infrastructure are often required to have security controls in place. When doing business with other business partners or even government agencies, there may be a contractual requirement for some specific security controls or control frameworks that are required due to the critical nature of the business conducted or sensitive nature of the information exchanged between the two entities. In order to build or maintain a competitive advantage, organizations may choose to implement and follow specific security control frameworks. Compliance with a specific control framework demonstrates commitment to information security and may assure clients concerned about how the organization will protect their information. Other organizations recognize the advantages to having a standard or consistent application of security controls across the entire organization. They may choose a control framework that best matches their business objectives and apply that as a set of best practices. They may choose to implement key parts of their chosen framework or a majority of the controls as needed. Once an organization has a controls framework to follow, they need to periodically review how well they comply with that framework. While it's one thing for an organization to state that they follow and are in compliance with the framework and at specified controls, it's another to be able to prove that internally and to other entities. This is where auditing for compliance with the controls framework becomes necessary. There are several outcomes from auditing that can assist an organization in achieving that compliance. First, they need to determine the level of compliance to the framework that they have reached. Auditing can identify the overall compliance against the frameworks controls. Next, organizations need to identify gaps in their control implementation. There may be missing controls that were never implemented in the first place. There could be controls that are partially implemented. Controls can also be implemented incorrectly. Finally, organizations need to know whether the controls they have in place are effective. An audit opinion may provide some level of assurance on the effectiveness of those controls and whether the controls aid the organization in achieving specific business and security objectives. Auditing can be an internal exercise or one provided by an external auditor. In some cases, external auditors are required depending on the regulations, industry requirements, or contractual requirements. External auditors also provide the organizational independence needed for an objective audit opinion not unduly influenced by the organization as internal auditors can be. There are several key regulations that specify security controls for organizations. Here are a few key ones to know. The Health Insurance Portability and Accountability Act of 1996, or HIPAA, mandates protection for patient health information in physical and electronic forms. There are two major parts to this law--the privacy rule and the security rule. Most security professionals are concerned with the security rule because it deals with electronic patient health information, and it specifies administrative, technical, physical, and documentation controls that must be implemented. The Sarbanes-Oxley act of 2002, or SOX, was enacted to improve financial reporting and disclosures for publicly traded companies in the United States after several large companies were involved in fraudulent accounting and reporting. The key portions of this law for security professionals are sections 302, Disclosure Controls, and 404, Assessment of Internal Controls. Controls are required to be in place to ensure the integrity of financial data, and the internal controls have to be reviewed to determine their effectiveness. The General Data Privacy Regulation, or GDPR, is a European law to protect the private data of EU citizens. It empowers citizens to have more control over their own private data with a series of defined privacy rights. This law applies to any organization that collects the personal data of EU citizens, regardless of the company's location. Internal security controls are needed to protect the personal data, as well as enable the rights of access, erasure, and portability. There are industry-specific compliance frameworks as well. These apply to specific types of business and their transactions. Some of the more well-known ones include the Payment Card Industry Data Security Standard, or PCI DSS, or just PCI. Service providers and merchants have to protect cardholder data and demonstrate PCI compliance. In addition to the implementation of specific security controls, PCI entities are required to review their security controls and report their compliance to their acquiring banks. The Federal Financial Institutions Examination Council, or FFIEC, has many standards for supervision of the banking industry. The council has created standards for information security and conducting examinations and audits. For IT systems, there is the Information Technology Examination Handbook known as the IT handbook and the Information Security Booklet, which is part of the IT handbook. The North American Electric Reliability Corporation Critical Infrastructure Protection Standards, commonly known as NERC CIP, provides standards for the reliability and security of the bulk power system in Canada, the US, and parts of Mexico. The CIP standards include cybersecurity and physical controls for key portions of the information systems that manage the power system. There are several standards for building or auditing information security programs. The International Standards Organization and the International Electrotechnical Commission have the 27000 series. These are a series of standards for best practices around creating an information security management system. There are several standards documents in the series to define the management system and various implementation techniques. The National Institute for Standards and Technology, or NIST, is a US Federal Government agency that defines various standards for the Federal Government. The Special Publication 800 series is focused on information security. While the standards are required for federal agencies, these standards may be required for contracts with the Federal Government. Most of the standards are generic enough for implementation by private organizations too. The Center for Internet Security has a list of 20 basic controls currently known as the CIS controls. The controls are designed to provide the basic actions needed to counter the most common threats and increase the use of automation to limit human error. A consensus process with a significant number of contributors is used to collect ideas and develop the best set of controls.

Module Summary

So that was a quick introduction to some of the foundational concepts in information security. Now let's do a quick review and look at some related courses. In this module, we focused on those core principles and high-level concepts that define organizational approaches to information security. We started with the core concepts of confidentiality, integrity, and availability, the CIA triad. Then we spent some time on other security principles of identity, authentication, authorization, non-repudiation, accountability, and auditing. All information security programs should be built on these guiding principles. In our discussion on governance, we talked about the high-level organizational stakeholders and purposes of corporate IT and information security governance. We also explored some of the key components and guidance, as well as the methods of oversight. Organizations that establish the appropriate guidance structures are more likely to have effective security programs. For risk management, we focused on how organizations assess, respond to, and monitor risk. In responding to risk, organizations can choose to avoid, transfer, mitigate, and accept risks. We followed that up with a discussion on the risk assessment process. Organizations that understand their own risks make better decisions in managing those risks. In compliance, we spoke about the needs that organizations have in selection, implementation, and assessing their compliance with specific control frameworks as required by law, industry, contract, or best practice. We highlighted some specific controls frameworks such as HIPAA, PCI, and the ISO 27000 series. Organizations have to meet their compliance obligations, whether those were internally chosen or mandated in this. In this module, we introduced some of the concepts that are explored in more detail in other Big Picture courses. Here are the other courses that you may want to tackle next. Security Management looks at the security of organizations and their strategies, principles, and risk management approaches. ISO/IEC 27001 Information Security: The Big Picture covers the international standard on information security management systems. PCI DSS: The Big Picture dives into the details of the payment card industry's Data Security Standard. In the next module, we will discuss what assets are and how organizations protect them using security controls.

Protecting and defending assets 

Welcome back. Organizations are basically a collection of resources from which value is derived. When properly applied, those resources or assets can be used to promote the growth of the organization in its pursuit of goals. So it's crucial that the organization protect and defend those assets. In this module, we will focus on three key ideas. First, the protection of assets through the application of controls. We will also define assets more clearly. Second, we will look at how organizations design and build various controls through security architecture and engineering processes. Finally, will look at the use of security operations centers and how they can be used to coordinate people and technology along with define processes as part of an active and effective information security program. There are two types of organizational assets at a high level. Both types have value to an organization. Tangible assets are those that are physical in nature or exist in a physical form. Intangible assets do not have physical form. Let's look at some examples of both and how organizations apply them. As I said, tangible assets are physical forms. Buildings are a great example and perhaps the most obvious tangible asset an organization has. Buildings are where human assets are assembled to conduct the business of the organization. Manufacturing facilities are tangible assets where an organization brings together human resources, machinery, automation, raw materials, and components to assemble the products. Data centers are spaces that contain centralized computing, storage, and network equipment-- tangible IT systems. Distribution systems are vehicles, equipment, and workers used to transfer goods from one location to another. They are often used to move completed goods from the manufacturing facility to the stores that sell them or to other distribution centers. Inventory can be the physical goods that the organization has created for sale, or it can be the components and/or raw materials used to create those goods. Transportation includes the physical vehicles used to transfer people or goods from one location to another. Depending on the type of vehicle, think airplanes versus bicycles, the overall value and technology integration can be significantly different. In each of these examples of tangible assets, the organization has an investment of significant financial resources in order to purchase, maintain, and operate the assets. The loss or disruption to one of these assets can result in difficulty to continued business operations. Intangible assets as opposed to the physical assets we discussed do not exist as physical objects. They are the ideas, concepts, designs, written works, and other forms of human ingenuity and creativity. Some of these works are or can be protected by legal protections such as patents, trademarks, and copyrights. Here are some examples of intangible assets. Intellectual property is typically a generic term applied to intangible assets from which an organization can derive value through the creation of products and services. These are typically protected legally with patents and copyrights. Strategy is anything related to how the organization plans to conduct future business. The specific details of strategy is information that is typically not shared publicly. Financial data is detailed information about the company's performance in the marketplace and the metrics related to those results. Business processes are the step-by-step procedures to conduct internal business operations. These might include processing orders, managing inventory levels, chemical processes, developing software, or any number of internally developed methods. Some of the processes may be considered trade secrets. Customer data is any data gathered about an organization's customers or potential customers. As organizations have taken advantage of the internet and built relationships directly with their customers, they have collected more data about them. Internal data are any other types and forms of data that an organization uses to operate. These can be employee data files, human resource information, facilities information, or anything that may not have a direct impact on the company's business but are needed to keep the organization moving forward. All of these intangible assets are essential to the success of the organization. If any of these assets were to be disclosed publicly, the organization could suffer some harm. There are threats to assets that can cause operational issues and challenges for organizations. Consider for a moment how well an organization might conduct normal operations if it were to suffer the loss of information, inventories, distribution, or transportation systems, buildings, or some other damage or destruction that prevented the continued use of an asset. That would cause significant challenges to an organization, especially if the assets cannot be restored or recovered. Disruptions could prevent an organization from normal operations. Datacenters with no network connectivity, manufacturing facilities with no access to inventory or distribution systems, building without working environmental controls, workers with no access to their electronic files, or any other type of disruption could limit an organization's ability to operate normally. Exposure of sensitive or business-critical information assets can limit an organization's ability to compete in the marketplace. The disclosure of information might lead to legal action, loss of legal protections, or inability to offer viable products and services. These types of threats can prevent organizations from conducting normal operations, limit business success, or bring about the end of the organization. In order to prevent threats from affecting assets, an organization can protect those assets. There are three basic objectives in that protection. First, an organization can prevent the threat action. It can detect when the threat action occurs. It can respond once the threat action has occurred. You can think of these objectives as acting before, during, and after the threat action. The asset protection objectives are put into effect through the design and implementation of security controls. You may hear the term internal controls. They are similar in purpose and may be applied to financial information or internal processes. Security controls are safeguards or countermeasures, which can be a procedural, logical, or physical means of minimizing, detecting, avoiding, or reacting to identified security risks. We defined a risk earlier as the negative impact of a threat and its likelihood of exercising a vulnerability. When we look at where on a timeline a security control can effect a threat event, it can be before the event occurs in that it prevents specific actions or activities from occurring. It could be during an event when specific activities are noticed or side effects are recognized. And it can be after the event has occurred, and the results of the event are observed or actions are taken to correct the effects from the event. In formal terms, there are three specific security control categories. Detective controls identify key activities and distinguish unique characteristics about the event as it happens. Preventive controls disrupt or stop specific actions before the event would have occurred. Corrective controls limit the effect specific actions have once the event has occurred. There are three security control types. Administrative controls are procedural or process-oriented security controls. These types are focused on the human either as an instrument of the control for oversight review or for following a defined set of steps, or as means to determine the next actions required. Physical controls are tangible objects or mechanisms that serve as barriers, deterrence, or devices used during a threat event. Technical controls are logical tools that can limit access, prevent specific actions, detect and analyze activities, or correct or restore normal operations. Now that we know more details about security controls, let's look at some examples that should help you identify them by type and category. And in some cases, a security control is contained in multiple categories. Remember, we started this discussion around assets, so think about the asset and how it is protected in these examples. Background checks are used to review a person's past and to make decisions about them that can affect their future. Sounds ominous, right? Organizations are trying to decide if a particular individual can be trusted with access to sensitive information or placed in a position of authority. One technique that helps in that determination is to find out if the person actually has the education or certifications that they claim to have, or that they haven't been convicted of a crime. Think about this security control. It's administrative because it involves investigation and review of a human. It's preventive when used in the hiring process because the organization is denying employment to individuals that cannot pass the review. It can also be detective when used for current employees. Antivirus software is used on computer systems to find and remove malware. It's a technical security control since it operates on the computer systems. It is preventive in that it detects known malware before it can be installed or executed. It is also corrective in that it can remove some malware after it's been installed or while it's executing. A security guard can provide a physical presence in a location to prevent specific actions and serve as a deterrent. A guard can also detect anomalous or malicious activity or respond to an event that has happened. Here are a few more examples of security controls. User roles in software provided separation of duties between different users and limit the actions a specific role can take. For example, you may have a standard user role that provides most of the functionality of the software, except for the ability to create new accounts. That functionality is limited to an administrator role, which can create and assign those new accounts. This is a technical control since it is integrated into the software. It is preventive in that it limits functionality to the role capabilities assigned to the specific user preventing access to key higher-level functions. Door access controls are entry controls that can restrict access to the interior space of the building or to specific areas inside the building. There are variety of these controls from pin pads and badge readers to hand geometry scanners to man traps. They all provide the same function even though they require different methods for valid access. These controls are physical and preventive in that they limit access to authorized individuals. Mandatory training is required training for all or specific members of an organization's staff. For example, training on security policy might be mandatory for all staff members, whereas PCI or HIPAA training might be mandatory for only the staff members that deal with those types of information. These are administrative controls that attempt to discourage bad behavior and encourage desirable actions for the staff. Hopefully these examples of security controls demonstrate how they can be grouped and categorized as part of an information security program. When we talk about intangible assets and an organization's sensitive and critical information, we need to talk about information management. While this can be a wide ranging discussion, I will only introduce it in this course. Organizations have different types of information, and each type may have a different value to the organization. For example, financial performance information may have a higher value than shipping invoice data. Both are important, but the integrity, availability, and confidentiality of the financial data is valued more than the integrity and availability of the shipping invoice data. Organizations need to create a classification system to identify the various types of data, assign a level of sensitivity to each, and define the required security controls needed to protect each level. You may have heard the terms top-secret or secret used to describe sensitive government information. These are the levels of information sensitivity. Organizations may use restricted, internal use only, or public to describe their sensitivity levels, to label specific sets of information, and to apply security controls. Once the classification scheme is developed, organizations may need to explore their collections of data, looking for those different types of information, and ensuring that it is labeled and stored on the right systems with the appropriate security controls in place. This may include manual system reviews or the use of data discovery tools to locate the information on large file systems or databases. After the information is more clearly defined and labeled, organizations can use preventive and detective security controls on computer systems and networks to avoid or limit the loss of sensitive information. Users can be blocked from sending sensitive data in email or file transfers to people outside the organization. Automated file system searches can locate sensitive data that is not located on the appropriate server or network. Border network devices can examine network traffic looking for sensitive data and blocking it before it leaves the organization's network. Information management deals with information in all its form, electronic and physical, as well as its use in business processes across the organization. The security controls needed can involve administrative, technical, and physical.

Security Architecture and Engineering 

Now that we understand more about security controls, we will discuss the overall design and implementation of those controls through security architecture and engineering. Here is my definition of security architecture. It is a process of selecting security controls that meet both objectives of the business and the security objectives of the organization. Security engineering can be defined as taking the security control plans, implementing the security controls, and monitoring their effectiveness. There are some specific design principles that should be considered in designing systems, software, and security controls. In 1975 Saltzer and Schroeder described design principles in a paper titled, The Protection of Information in Computer Systems. It included the principles which I'll describe briefly here. I would encourage you to read the whole paper for more details. Economy of mechanism means design protections that are small and simple. The challenge with complex protection systems is that there are likely implementation errors and vulnerabilities. Remember, complexity is the enemy of security. Fail-safe defaults is the idea of creating a base configuration in which the default action is to deny access. The assumption is that security protections somewhere along the line may fail. Having the system access restricted by default limits the impact of protection failures. Complete mediation is the verification of access and authority for all objects and actions. This is a system-wide approach for checking the source of all requests. Open design is the idea that the design of any protection system should be open for review and scrutiny. Systems built on secrets lack the critical outsider perspective that can identify flaws and vulnerabilities. We should always be cautious of crypto algorithms, security products, and blackbox systems in which the designers do not share the details of the design. Separation of privilege is the division of responsibilities among the authorized users of the system. It prevents a single user from executing a critical action without the oversight or review of someone else. Whether the action is accidental or intentional, there is another user that can prevent it from proceeding. Least privilege is limiting a user to the smallest set of privileges needed for their job. This prevents users from having too many privileges on the system and limits the damage from errors, accidents, and intentional actions. Least common mechanism is related to shared functions that are depended upon by all users. The better design choice is to avoid supervisory functions in favor of common library functions. Psychological acceptability is related to the design with consideration for the humans that use the system. There is a need for ease-of-use so that protections are applied routinely, consistently, and automatically without hesitation. The design must also consider how to handle human error. Defense in depth is actually not part of the principles from Saltzer and Schroeder, but it's a common one to which most professionals are aware. It's the idea of building multiple layers of controls and defenses. If one layer of protection fails or is bypassed, there are additional layers that can prevent attacker success or raise the likelihood of detection. Here are some resources for design principles that you should explore. There is the Saltzer and Schroeder design principles paper, the US CERT site has a modern list of design principles, some of which are based on the Saltzer and Schroeder ones. The Open Web Application Security Project, or OWASP, has a list of design principles as well. The process of designing security controls should ideally occur early in the IT service design stage. It's at this stage where the security controls can be determined and integrated into the architecture of the service. However, you will often find that security is an afterthought, and that the security team is engaged late in the process. It's still possible to protect IT assets and information with security control design late in the process. There may be issues with the completeness of the control coverage or implementation and integration issues though. The first consideration should be on understanding the business objectives of the IT service. This helps ensure that any conflicts with IT or security strategy are addressed and that the security controls do not add any significant limitations to the service provided. At this point, alternative controls can be considered if the standard set of controls are not suitable. The security policy and relevant procedures and standards must be considered to ensure that the security controls provide the required protection and that the implementation is consistent. The security control selected should be cost-effective. Using the existing security infrastructure and controls should be the preferred approach whenever possible. Adding in new, untried security controls or building new security infrastructure adds cost perhaps unnecessarily. Some of the cost could include new product licensing, hardware, training, integration with other security systems, and internal chargebacks. Choosing security controls that are simple is also preferred. By reducing the complexity of the protection provided, the security controls in place will be easier to understand and explain. There are fewer opportunities for errors in implementation and management. Simpler controls are also easier to test an audit. While they should be simple, the protection provided should be adequate for the identified risks. Cryptography is a large area of information security practice that provides secure communications in the presence of adversaries. It is a much larger topic than we will cover here since it can involve a discussion on mathematical theory, computer science, cryptographic algorithms and protocols, key links, and implementation challenges. We have another Big Picture course that covers that level of detail. Let's focus on the high-level uses in security controls. The first purpose is the exchange of sensitive information. Cryptography can allow two or more parties to share information. Adversaries that capture the data exchanged cannot read it without knowledge of the private keys. Organizations rely on cryptography to exchange information with their employees, clients, and customers through tools like secure TLS enabled websites and the SSH protocol. Cryptography can be used to prevent the unauthorized modification of information. This use is helpful in information exchange to ensure the message sent matches the one received. It can also be useful in detecting data that had been modified after it was created. Cryptography can help improving the identities of various entities. Public key cryptosystems are one tool in proving identities. Each user creates or is issued a unique crypto key, which, through its use in key exchanges and digital signatures, proves their identity. TLS protected websites have public key certificates that have been digitally signed by trusted third parties to prove their origin and identity. Security controls can be applied across the network based on the security objectives of the organization. Network security controls need to continually change to adapt to new threats and to keep up with changes in network infrastructure. As a key metric for consideration, we can expect that bandwidth needs will continue to grow based on the reliance of modern organizations on networked devices, cloud services, and Software as a Service. Network security devices haven't always kept pace with the expanding bandwidth needs of the organization. One key component in network infrastructure is the use of an isolated network segment for external internet connections to reach the organization's public network services such as their website, email server, DNS servers, and other public-facing network services. DMZs are designed to limit external connections to carefully configured and closely monitored services and servers. If one or more of the DMZ servers are compromised, the attackers are limited in their access to organizational data and internal servers. The monitoring in the DMZ should be sufficient to detect attacks and attacker progress quickly. Network intrusion detection and intrusion prevention systems provide network awareness of malicious and potentially anomalous activity on the networks. Intrusion detection systems are passive in that they monitor the network and raise alerts when suspicious or malicious activities occur. Intrusion prevention systems are active in that they affect network traffic when malicious activities occur, in addition to raising alerts. Firewalls and originally routers with access control lists in place specifically block bad network traffic based on its source, destination, and network port. Modern firewall technology is advanced in capabilities for stateful inspection and deep packet inspection. Next-generation firewalls are more application aware and have features of intrusion prevention systems. As more business is conducted using external web resources through Software as a Service, social media, and external web APIs, attackers have been using more of those web resources as an attack vector. Web proxy servers and more modern cloud access security brokers, or CASBs, can inspect web traffic and block some services and untrustworthy sites. With the various attack vectors on the network and the multiple devices in place, organizations need a centralized way of managing events for review and action. A security information and event management system, or SIEM, collects log and event data from multiple sources and presents that information to analysts and can initiate response actions automatically. Along with the SIEM is the need to manage log data from a variety of devices. The log data can be fed directly or selectively into the SIEM, or the log data can be collected in mass through a log management system and then fed into the SIEM for analysis. Log data is also helpful for reviewing events during an investigation and for the creation of new rules for the various network security devices we discussed here. An endpoint is often the term used to describe various computing devices. Endpoints can include desktops, laptops, mobile devices, and servers. Security controls for endpoint devices are generally the same across the different devices but may have varying levels of configurability. These controls should be based on the intended use of the device and the risks associated with it. Antivirus and anti-malware are common tools with which even the most non-technical users are familiar. The first antivirus products for PCs became available in the late 1980s. The technology in antivirus has changed very little over the decades, but the number of virus and malware variance has exploded. In enterprise environments, centralized management and the deployment of special malware signatures on demand are key features. Storage encryption is one control used to protect data at rest on hard drives, solid-state drives, and other forms of storage media. The primary threat here is the loss of a portable device. Encryption protects the contents from disclosure. Application white listing is a security control with two primary purposes. One is limiting the user to authorized software. The other is to prevent the introduction of malicious software to a system. When you contrast this with antivirus controls, which allow all activities except those known to be bad, white listing allows only specific activities and blocks everything else. Data loss prevention controls prevent the accidental or intentional disclosure of sensitive and classified data. As we discussed previously, organizations with information classification schemes can enforce the required protections on the endpoints with data loss prevention tools that can cover actions in email, file transfers to removable media, and posting information online. Intrusion detection and response tools are security controls that aid in both detecting intrusions on the endpoints and responding to intrusions in real time. All of these security controls can be used to protect server, desktop, and mobile endpoints and should be used in conjunction with appropriate network security controls based on the security objectives of the organization. Configuration guides are documents that describe various configuration options for hardware, operating systems, and applications that improve the operational and security readiness of systems deployed in a hostile environment. Hardening guides provide specific instructions on improving the security of systems over their out-of-the-box configuration. NIST has the National Checklist Program, which is a repository of hardening guides. Some are published by government agencies such as the Defense Information Systems Agency and the National Security Agency. Some hardening guides are published by the vendors themselves and are typically supported configurations. The Center for Internet Security publishes security benchmarks, which are hardening guides created by the consensus of independent security professionals. The traditional systems or software development lifecycle was focused on repeatable and consistent practices in design, creation, and deployment of a complex system. Unfortunately, the security of the overall system may not be considered until the system is deployed or about to be deployed. Security issues discovered later in the lifecycle are generally more costly to correct, and some issues may require the redesign of significant parts of the system. Due to schedule pressure, some systems may be deployed with known security flaws. Hopefully it's obvious that the risks of delaying security practices until the end of the system lifecycle are significant. When security practices are built into the SDLC processes, then security is considered during the requirements and design phases. The system has the right foundation to operate in hostile environments with security constraints identified, threat models developed, and security controls integrated into the system design. Practices during the development phase include secure coding practices to avoid common programming errors that lead to security flaws, and manual and automated code review to catch errors in the entire code base. In the testing phase, manual application penetration testing can be used to find vulnerabilities, and prototype environments can be documented for secure deployment of systems. Of course, there are several ways that security can be built into an organization's SDLC. The Building Security in Maturity Model, or BSIMM, and the Open Software Assurance Maturity Model, or OpenSAMM, are two great resources. In addition to the IT-related security controls we've discussed, an organization's human and physical assets need protection. There are physical security controls that can be used in an integrated manner that involve the people employed by the organization, including all the staff members, as well as those tasks with physical protection. Procedures designed to ensure that equipment, material, and visitors entering and leaving the facility are verified, logged, and monitored, and that specific actions are taken during an incident. And use of specific kinds of equipment and construction techniques to provide facility protection. The integrated approach to physical security provides what is known as a physical protection system. A properly designed and integrated physical protection system can detect malicious and suspicious activity by an attacker, delay the attacker's progress, and allowing the response to stop or disrupt the attacker. Here are some types of physical security controls that can be used to protect physical access. Entry controls allow authorized individuals access to a facility or spaces inside a facility. For example, entry doors can be unlocked when a user enters a pin, has their badge read, or provides some biometric. Sensors and alarms provide monitoring and alerting. Sensors can detect a variety of conditions or system states or provide visibility. Alarms use data from the sensors to alert personnel when there is a violation of a defined baseline or activity. Reception areas are locations in a facility where visitors enter and are greeted and processed by staff to determine the validity of their presence there, and to contact the appropriate staff to manage the visitors. These places are typically building lobbies with trained staff members to follow a set of procedures. Reception areas can also be used to protect internal spaces such as datacenters where contractors, vendors, and some staff members need to enter that space. Physical barriers serve to deter, delay, or trap attackers. Fences, lighting, walls, berms, locked doors, checkpoints, vehicle bollards, and other physical mechanisms can be used to prevent attackers from approaching and entering facilities. Guards provide some deterrence for attackers in an active response during incidents. Visitors that are escorted while on the premises are logged and monitored during their entire time at the facility. In our discussion so far, we've touched on two security control types--technical and physical. Administrative controls also serve a purpose in protecting an organization's assets. Just as technical and physical controls have to be designed and implemented effectively, the same applies to administrative controls. This is a human factor type of information security control. Some examples of administrative controls include policies, procedures, guidelines, and standards, which are the documented security guidance that start with the high-level requirements, provide step-by-step instructions for specific actions that must occur, and describe technical implementations. Hiring and termination procedures are needed to ensure that the appropriate people are brought into the organization with the position of trust and that should they leave or be asked to leave, their departure does not add risk. Training and awareness is needed to educate and periodically remind staff members of their responsibilities for information protection. This can include onboarding procedures and new hire training to inform staff members of their information security responsibilities and job-specific training on security techniques and practices. Oversight and review are methods to verify that specific actions are executed as defined by written procedures. Oversight is used before an action can proceed further. Review can be used after the action has been completed to check compliance with those procedures. Audits are often used as a way to determine compliance with policies and procedures and their effectiveness.

Security Operation Center 

Now that we've looked at the architecture and engineering involved in security controls, let's look at how some of those controls are used in a centralized approach known as the Security Operations Center, or SOC. My definition of the Security Operations Center is the centralized application of people, process, and technology used to manage information security operations. Similar to a lot of what we talk about in information security, there is that pivotal role that people, process, and technology play in any successful program. A SOC is no exception. In fact, it is probably even more critical to an organization's ability to adequately manage its security defenses than any other aspects in information security program. Let's explore a little more. Here are some key pieces that a SOC should have. The SOC needs a defined and declared mission. This helps define its purpose for the organization instead of a place where all the security magic happens. Mission focus is also helpful in honing the attention of SOC staff members and for teambuilding. Executive level support is a requirement for a SOC to be successful in terms of budget, headcounts, and physical space. Having the right people and the right processes in place is more valuable than having the right technology. A SOC needs to have smart people doing the right things. The technology used in the SOC will change over time. The organization needs to realize that protections will fail. The SOC should prioritize detection capabilities over protection. Additionally, knowing about an incident but not being able to do anything isn't helpful. So a SOC needs to have the right response capabilities and support. While the mission of the SOC can be more specifically and formally defined by the organization, there are two basic objectives for a Security Operations Center. The SOC protects the information assets of the organization. The policy of the organization defines the needs for information protection, and the SOC is an instrument of that protection. The SOC also ensures the continued operations of the business. Security incidents and disruptions can negatively impact the organization's ability to conduct business. The SOC monitors for and works quickly to resolve these issues preventing or minimizing the impacts to business operations. The SOC has a specific set of functions to achieve those objectives. The specific activities for each SOC have to be defined. Some SOCs may prioritize some activities above others, or they may add activities as their capabilities are developed over time. Monitoring involves the direct and indirect observation of security devices and systems through their activities, events, and log data. Any anomalous, suspicious, or malicious activity is recognized and an investigation is conducted. The SOC can also monitor the performance and effectiveness of defenses. Baselines are the configuration standards that are applied to a variety of systems based on the organization's threat environment, known attacker methods, and best practices. A SOC can define those baselines and test configuration effectiveness against attacks. A response is needed once events are detected that indicate an incident or attack. The SOC can gather and analyze information related to the incident and take the appropriate steps towards containment, eradication, and recovery. Vulnerability detection is a proactive approach to defending assets. The SOC can actively manage the automated scans and manual testing needed to identify and report vulnerabilities within the organization. Intelligence is an active approach to preparing for attacks and developing defenses and responses for known and new threat actors. The SOC can develop internal intel and work with outside organizations to research, collect, and develop attacker indicators of compromise and tactics, techniques, and procedures. Coordination during security incidents may be needed internally to the SOC within the organization and with external parties. The SOC is the central hub of the organization's security operations and must develop and maintain cooperation with multiple entities. A Security Operations Center can exist in several forms. A dedicated or in-house SOC is one that is located on-premises at the organization's facility and is typically staffed with full-time employees or contractors. This type of SOC is typically centralized within the organization security program and serves the organization across all lines of business and all points of presence. The distributed SOC is one in which there may be functions spread across several locations. If the organization is a multinational, then there may be a need to distribute security functions and operations in several localities or regions. There may be a global SOC with several regional ones. Some specialized functions or activities such as forensics or threat intelligence could be centralized. An outsourced SOC can include all or some security operations that are handled by a service provider. A completely outsourced SOC might include a third-party provider handling the entire SOC. A partially outsourced SOC might have a managed security service provider, an MSSP, providing some management of security devices such as firewalls or IPS or the SIEM or after-hours support. While outsourcing may save initial costs of rebuilding a dedicated SOC, there are challenges in getting value from outsourcing a SOC. A virtual SOC is an approach that does not employ a dedicated facility. It may offer a portion of the normal SOC functions or operate only during an incident. The employees of a virtual SOC may have normal office workspaces and may only work part time on SOC duties. The physical space of a Security Operations Center may vary based on the needs and mission of the SOC. Not every organization has the resources to build a sophisticated SOC with dedicated space, or the SOC may start as a virtual SOC and then grow to include a dedicated space later. Here are some of the features that organizations utilize in the design of their SOCs. Having isolated rooms with limited access provides the SOC employees with a reasonably quiet area that is focused on the mission. Keeping other non-SOC employees away allows for concentration and collaboration during security incidents and avoids the spread of rumors and the interruptions that can occur when other people can enter the SOC. Well-designed status displays and dashboards allow the SOC employees to quickly assess the status of the SOC, the current workload, and the state of various defenses deployed. Some SOCs have large video walls displaying information or various monitors throughout the SOC. SOC employees can change these displays as needed, or share their screens on the large display for the team working an incident. Each employee will have a workstation to conduct their work. Some SOCs offer desks assigned to an individual employee. SOCs that operate around the clock in shifts may have shared workstations for the staff members of each shift. SOC employees may need to step away from the workstations for collaboration, discussion, forensics work, and training. Having these spaces nearby allows the SOC employees to focus on other aspects of their work. These spaces may have specialized equipment and the additional workspace needed. While we spent a little time talking about what SOCs are and what they look like, let's turn the discussion towards the key components of a SOC. First, the people that make up the SOC are critical to its operation. There are specific roles for personnel. And to make sure that everyone knows what to do, training is needed. Process provides the steps and guidance needed to ensure the right resolution of events and consistency in the approaches used. Process can include the step-by-step procedures, the methods for escalating incidents for assistance and visibility, and then reporting and metrics related to incidents, as well as the overall operations of a SOC. Technology aids the SOC personnel along with the processes in place. The right tools can simplify and bring the scale needed to handle the large volume of data associated with today's network environments. These tools can help the SOC function in monitoring, assessing, and defending the organization's information assets. I would argue that the combination of people and process outweighs the technology component. If you have the right staff with the right training and the right processes, then the technology employees won't matter. In fact, it'll change. A lot of SOCs fail when they're built around a specific tool or a suite of products from a vendor. There are some specific roles and responsibilities for SOC staff members. The SOC manager role is generally the leader of the SOC and has to manage the human resources, the processes, and technologies used. Some organizations include a security architect within the SOC. Their responsibility is generally around the organization security controls and the technology used within the SOC. If there's a security architect attached to the SOC, then security engineers may be as well. Their responsibility is to install, configure, and maintain the technologies used by the SOC. Lead analysts are senior members of the SOC team. They provide technical leadership, mentorship, and training to the other analysts and may have responsibility for multiple SOC functions. They may work collaboratively with other analysts on more complex incidents. Security analysts handle a majority of the SOC functions. Their responsibilities can include triaging incidents based on available information, managing the incident response process, gathering and disseminating threat intelligence, and participating in threat hunts to identify adversaries already inside the organization's network and systems. In addition to having the right staff members in the SOC, they need to be trained. While some staff members may come to the SOC with good experience and knowledge of incident response and the technologies in place, no one will have all the knowledge needed on their first day. Training is essential for SOC staff members so they can grow as employees, take on additional responsibilities, and to practice collaboratively with their team members. Processes and procedures are key to the effectiveness of the SOC. Each staff member should be trained on them. Education and training can include university courses, certification courses, technical training, vendor training, and other formal or even informal methods of learning. Tabletop exercises are scenario-based exercises in which members of the SOC team gather to walk through the scenario while discussing the next steps and reviewing procedures. A facilitator guides the team through the exercises and solicits input from the team on their approach and procedures Simulations and games are learning opportunities with a hint of fun. These can include digital forensics challenges in which the team collaboratively examines gathered evidence from a recent incident or through one of the online forensics challenges. A SOC may have several processes and procedures for their work. Most of them may center around managing incidents. While each incident will vary, there are some basic processes that occur. Triage is the process of reviewing the known details of incoming alarms, alerts, warnings, indicators, user reports, and other notifications to determine the next actions to take. Some may warrant further analysis to determine whether an incident has occurred. Escalation is the process of bringing in additional analysts or other resources to assist in working an incident or the process of informing management about an incident. If the incident requires technical skills beyond the current analyst or the effort requires additional resources due to the volume of work, the incident can be escalated. The investigation itself will have processes to follow for consistency and documentation. Each organization may define some basic processes to follow and documentation elements that are needed. As the analysis and investigation proceeds, the analysts must have some flexibility in their approach that can adapt to the direction the investigation takes. The response to an incident may involve containment, eradication, and recovery. Containment is that set of actions taken to limit the incident either from causing more damage or consuming more resources. Eradication is elimination of the threat from the environment. Recovery is the restoration of systems and services to their normal operations. There are some key tools that SOCs can use to aid in managing their processes and procedures. Playbooks or run books are scenario-based tactical guides for incident handling. They can be specific to the types of attacks anticipated and have detailed steps listed for the investigation and response. These are typically organization specific as they may have details about the specific roles and deployed security tools. Communication may need to occur with teams outside of the SOC either externally to the organization or internally. External contacts might include upstream network providers, related organizations with shared resources, and even information sharing and analysis centers, or ISACs. Internally, the SOC may need to coordinate analysis or response activities with other technology teams. Having an updated list of contacts is needed. Post-incident reviews are opportunities to evaluate the SOC team's actions and to make improvements to the process. These reviews are essential to iteratively improving the performance of the SOC. A knowledge base is a tool for collecting, grouping, and storing information about the organization's environment, threats seen, techniques, tools, documentation, and any other piece of information that might be useful to the SOC team. Individual team members should be encouraged to store information they have acquired over time in a common repository for the benefit of the whole team. Another process for the SOC is metrics and reporting. Organizations want to know if the investment made in building and maintaining a SOC is paying off. The value of the stock is measured through a variety of metrics. Each organization may want to know specific details about their SOCs, but the types of metrics can be categorized. Volume metrics are related to the number of alerts, threats, incidents, cases, or tickets. As the organization makes changes to the security program or introduces new security controls, there may be an increased scrutiny of volume metrics to determine the effectiveness of these changes. Time metrics show the time to detect, manage, and resolve incidents. Some of the metrics could include the time spent on incidents. The elapsed time from the beginning of the incident to various stages of the process and the time to respond to the initial report. Impact metrics show the business units impacted by the incident in the downtime and outages attributed to the incident. Cost metrics can include the costs of an incident, costs avoided through attack remediation, and manpower costs. Now let's talk about some of the technology used by SOCs. In addition to the various security controls in place, there are tools to collect and present the event data and to manage data used in the SOC. Log management tools collect raw log data from security controls and systems in the organization. These tools can store, filter, and archive log data. Most log management tools can provide advanced search options. Log data can be forwarded into a security, information, and event management system, or SIEM. It's a platform for aggregating and parsing log data, correlating log data from different sources into events, and presenting that enhanced information to analysts. SIEMs offer data enrichment through external threat feeds, integrating with internal data sources, and geolocation data. Automation with the SIEM can be enabled through the use of rule sets, external process execution, and integration with other security systems. Governance, risk, and compliance, or GRC, systems can be integrated with SOC operations as well. These tools can aid in presenting overall risk data and managing the incident workflow processes. Ticket systems allow analysts to manage incidents through a centralized information management tool where each incident has its own ticket. Tickets can be dynamically created by the SIEM when rules or alarms are triggered. As we discussed in the security architecture and engineering section, there are a variety of technical security controls that an organization can deploy. Some of the technology areas that the SOC may use as part of its mission focus include perimeter, network, endpoint, application, user, and data. The specific technologies can include the following controls-- intrusion and prevention or intrusion detection systems, which monitor network traffic looking for known malicious patterns that can indicate attack. An IPS can block attacks in real time, though too many rules enabled may affect the network performance. An IDS can generally look for more patterns in the traffic without impacting network performance. The SOC can monitor log data and alerts from these systems and actively change the rules to look for new patterns of attack. Distributed Denial of Service or, DDoS, mitigation systems and services monitor inbound network traffic looking for indicators of a denial-of-service attack. The SOC and monitor these systems and enable attack mitigation when needed or configure them to start automatically once certain thresholds are met. In addition to allowing only permitted traffic, firewalls can generate log data for packets and sessions that were dropped. The SOC can monitor the log data for attacks and alter firewall rule sets as needed to contain attacks. Endpoint management can aid in managing security baseline configurations and the recovery of systems from incidents. It can also help threat hunters looking for attackers or malware already inside systems. Proxy servers are generally used to monitor web traffic for known threats, block specific types of websites, and monitor traffic inside encrypted sessions. The SOC can set up website blocking, adjust the rules, and monitor alerts for threats identified in web traffic. Malware management systems include antivirus and anti-malware software, and the central management tools needed to operate in a large organization, as well as malware sandbox systems. The SOC can review alerts from these systems, collect malware samples from quarantine for further analysis, add in new signatures for new malware variance, and detonate malware in a sandbox to see what it would do.

Module Summary

It's important to understand the assets an organization has and the unique threats it faces in order to design and implement an appropriate set of security controls to protect them. An organization also needs to consider how to maintain visibility into those controls and react quickly to threat events, attacks, and incidents to contain and limit damage to those assets. In this module, we took a look at assets, security controls, and security operations. First, we defined what assets are to an organization. These are the tangible and intangible assets of value. Of course, where there is value, there are threats that must be considered. The design and implementation of security solutions requires an understanding of the appropriate security controls needed to protect information assets. Security architecture is an approach for understanding the business and security objectives and choosing appropriate security controls. Security engineering is the implementation and monitoring of those security controls. Managing and monitoring the security of the organization can fall to a Security Operations Center, which is a centralized approach to watching over the security controls of the organization. The security analysts that work in the Security Operations Center are involved in monitoring and testing the security of assets and managing incidents. As we did in the previous module, we discussed some of the concepts that are explored in more detail in other Pluralsight Big Picture courses on information security. Here are the other courses that you may want to explore next. Cryptography: The Big Picture is all about crypto. Security Management: The Big Picture covers the basics of an information security program. Threat Modeling: The Big Picture looks specifically at how to understand threats. Security Architecture and Design: The Big Picture explains the design and use of security controls for an organization. In the next module, we will look at how organizations use auditing, monitoring, and testing to evaluate and assess security controls and actively identify threats and vulnerabilities.


Auditing and Monitoring 

Auditing

Welcome. Organizations conduct their business using a variety of valuable assets. They may employ internal and security controls to protect those assets. They need to ensure that those assets have appropriate controls in place, monitor those assets and controls, and validate that the controls are operating effectively. In this module, we'll look at three specific areas, auditing, focused on information systems, monitoring of information assets and the security controls that protect them, and testing techniques to determine the effectiveness of security controls. Let's start with a definition of auditing. It is a process conducted by a competent and independent auditor that collects evidence associated with assertions made about a system and evaluates those assertions and forms an opinion and reporting on whether the assertion conforms to a standard. That's a lot to consider, so let's address each piece. Auditing is a systematic process in that there are specific and repeatable methods used to conduct the audit and develop consistent results. Audit independence applies to both the auditor and the organization. Both need to maintain independence from the area or activity under review to provide an objective assessment. In some cases, an organization may need to employ an outside audit firm to maintain the needed independence and objectivity. Evidence is the information and observations needed to determine that the business and security objectives are being met by the internal and security controls in place to protect the organization's assets. Assertions are the claims made by an asset and its set of controls. The auditors evaluate the assertions and render an opinion on the adequacy and effectiveness of controls in place. This opinion is provided through reporting. There are a variety of internal and external standards or frameworks with which an organization must demonstrate compliance. There are many types of audits. Some are related to or specifically for information systems or information security. Integrated audits are a combination of financial, operational, and information system audits. This type of audit is needed based on the use of and dependence on information technology in business. An information systems audit, or IS audit, evaluates the protection of information assets and various factors in how the systems meet business and control objectives. A specialized audit is one that is typically defined by an independent audit organization for a specific audit target or objective. In information security, there is this Statement on Standards for Attestation Engagements #18 or SSAE 18, which is used by certified public accounts to evaluate controls at service organizations. Forensic audits are audits focused on discovering and disclosing possible fraud and criminal activity. These can be conducted to support lay enforcement or corporate investigations. One thing that has bugged me over the years as an information security professional is the inappropriate use of the term audit. Those not familiar with the audit process typically make assumptions and inaccurately use the term to describe some aspect of testing or review as an audit. Here are some examples I've heard. Technical testing can be an important component of understanding an organization's risk and control effectiveness, but by themselves they are not audits. Vulnerability scanning is not an audit, but could be used as evidence of testing in a vulnerability management program audit. Penetration testing is also not an audit because it looks at the controls in place and not at the decisions made to select those controls or the remediation steps taken and documented as an outcome of testing. There are also some administrative activities that by themselves are not audits. A risk assessment is not an audit by itself. That type of assessment can be used as a data source for auditing and it can be evidence of key activities in an information security program. Tabletop exercises are not audits either. They are valuable in that they can be a training and process improvement tool. An audit may look for evidence that they're being conducted and that the results are used to make improvements. Be wary when someone asks for an audit or claims that some testing was an audit. They may be mistaken in their understanding of auditing. Organizations that understand their risks through a risk analysis process can take steps to mitigate those risks. The risk analysis information is also helpful to auditors as well. They identify unique risks to the business and identify their threats and threat actors, the negative impacts that may be experienced and the probability that those threats may occur. To an auditor, this is key information because it identifies the risks that the organization needs to address through controls. Risk analysis information can aid in the determination of high-level audit objectives. The information is also helpful in evaluating the controls to determine their effectiveness. And, with risk information about the organization, audit decisions can be made in a risk-based fashion. Audit and auditors use some key terms. Let's explore a few of them here. An audit charter is similar to high-level policy that is specific to audit. It defines the purpose, authority, responsibility, and accountability of the audit function for an organization. The audit subject is an area or activity that is the target of the audit. The audit objective is the purpose of the audit, for example, the audit may verify that systems deployed in a DMZ follow defined procedures for configuration and testing. The audit scope defines the specific systems and activities that are part of the audit. Audits themselves have risk in that there's a possibility that the audit opinions and reporting could be wrong based on errors in the collection of evidence. Some audit findings are more material or significant than others depending on the audience of the audit report. Some levels of management may not view all findings with the same concern. Evidence is the information gathered during the audit and used to determine if the audit subject is following the audit objectives. Auditors must use evidence that is sufficient and relevant. Auditors can start with the documentation associated with the target of the audit. This can include policies, procedures, standards, installation, configuration, or any other documentation associated with the activity or systems being audited. Auditors may conduct interviews with personnel involved in the creation, operation, or management of the target of the audit. The notes from the interviews may be used as evidence. Auditors may observe the activities of personnel involved in the audit target to determine if appropriate steps are taken. They may also ask for the current configuration files or log entries for review. Testing is used to determine if the controls are in place and operating as expected. A compliance test determines whether the controls are in place. A substantive test determines whether the controls are effective. There are time and cost considerations that must be addressed when an auditor conducts testing. It isn't reasonable to inspect or test every system, transaction, or activity that is under review. The auditor must select a smaller number. This is referred to as a sample. A statistical sampling approach may be used to objectively select the size and evaluate the results to make an inference on the entire population. This approach is based on statistics and probability. A nonstatistical sampling approach is a subjective method that uses auditor judgement to select the sample size. This method may select items that have a higher risk or those that are more relevant to the audit. Once the auditor collects information and reviews the audit objectives, she has to create an audit opinion. This opinion is an evaluation of all of the evidence gathered along with the original objectives established at the start of the audit. The opinion is a judgement that is arrived at through experience and not based on a specific method or process. It is not always obvious which controls are effective and which meet the control objective. This requires serious thought and consideration of the controls and the control objectives. There are strengths and weaknesses of any set of controls. These need to be identified. Recommendations need to be provided to correct or improve the controls in place or to suggest controls that are missing or are incomplete. One tool that can aid in determining the effectiveness of controls is to use a controls matrix. It's a simple approach that identifies the known threats on one axis and the known controls for those threats on the other. The matrix is then filled with information gathered from testing that shows the effectiveness of the controls and areas of weakness or missing controls. Compensating controls are situations in which a weak control is compensated for by a strong one. You may also have to consider that having only one control, even if it is a strong one, may not be adequate for the control objective. Other controls, such as compensating or overlapping controls, must be considered before an opinion is given. The audit opinion has to take a variety of factors into consideration before a conclusion can be determined. Once the audit opinion is determined, the audit details and findings need to be reported to the appropriate members of the organization and the documentation for the audit needs to be collected and organized. And executive summary is a concise report for executive management. This type of report avoids the technical terms and sticks to a business focus for the audience. An audit report contains most of the details that need to be provided. It includes information about the objectives, scope, and methodology used, the opinion on the effectiveness of the controls reviewed, and a detailed list of audit findings along with recommendations. Visual aids can be helpful in simplifying how the information is presented to the audience of the audit report. Graphs and charts showing the results of testing and the controls matrix we discussed earlier are good methods of visually representing more detailed data. We mentioned that findings and recommendations are part of the audit report. Depending on the audience and the materiality of the findings, the findings and recommendations details may need to be customized. For example, the technical and operational personnel may need a sufficient level of details to respond and implement changes to the systems affected. Follow-up activities are post-audit work needed to verify that the corrective actions to address the audit findings are completed. Audit documentation is the collective set of information and data created and collected during an audit. It may need to be archived by legal or contractual requirements. It includes all of the documentation from the planning phase all the way to the audit report that was issued. Audit programs should be designed to meet the needs of the business while simultaneously using limited audit resources effectively. The first approach is to ensure that the audit process is in line with where the business is heading. By knowing the business strategy, the audit program can ensure that audit activities are valuable to help the business achieve those critical business and security objectives. Most audits are periodic in nature. Continuous auditing uses an automated approach to audit transactions and activities in real time. This approach allows auditors to mark and review activities that deviate from the norm as they occur. A controls self-assessment is an approach that collaborates with the stakeholders in gathering data. There are a variety of methods that can be used, but the main idea is to include those stakeholders in reviewing the risks to the business and internal controls used to manage those risks. These can be as simple as questionnaires about controls, all the way to facilitated workshops.

Monitoring 

Organizations that monitor their assets and the activities within their systems are more likely to detect issues and react quickly to reduce risk. Let's look at monitoring in detail. There are a variety of monitoring activities and programs that an organization can build to improve their security posture in the face of adversaries. Monitoring activities have some shared objectives which we explore here and then dive into some specific monitoring programs shortly. Any monitoring program from information security should contribute to the reduction of risk for the organization. Ideally, these programs should be deployed enterprise wide so that the coverage for the organization is complete and consistent. The purpose of monitoring is for the observation, detection, and evaluation of issues. You have to find the problems, events, and activities that are known to be or suspected to be detrimental. Monitoring capabilities and the extent of that monitoring is critical to ensure detection. In most monitoring programs, the effectiveness of security controls is key to the success of the program. Either the program itself evaluates those controls or is dependent on the controls to provide data. Having effective controls in place ensures that monitoring processes operate as intended. The evaluation of those controls can lead to improvements. The output of monitoring activities initiates and drives the remediation and resolution work. Monitoring provides data that can start the initial evaluation of a finding or enable the correlation of other data sources to provide context around an event. There are two important monitoring programs that we will discuss. Network security monitoring is a program for monitoring network resources to identify malicious, suspicious, and anomalous activity. The tools used in monitoring alert incident responders and they drive the investigation and response. This type of program assumes that prevention will fail at some point and a breach is inevitable. Vulnerability management is a program for monitoring an organization's assets to detect vulnerabilities and to remediate them. By reducing the number of vulnerabilities available to attackers, the organization can reduce its risk. Another monitoring program that you may encounter is continuous monitoring, which is an overarching program for maintaining an accurate and real-time view of risks across the organization. Some of the elements in continuous monitoring can include vulnerability management and network security monitoring as sources of information. Network security monitoring, or NSM, is network-based in that the primary source of data used in this type of monitoring comes from the network. NSM is focused on threats and not vulnerabilities. It operates under the assumption that we talked about earlier that prevention will fail at some point. NSM works to foil attackers within the network through quick detection and remediation efforts to limit damage. NSM works with both the asset owners and the incident handlers to build processes to prepare for adversaries, protect assets, detect activities, and respond appropriately. NSM is primarily tool driven, since there is a significant reliance on tools to enable the program. The tools are part of the security control suite for the collection, delivery, and presentation of event and activity information. Establishing the network security monitoring program needs some processes in place to ensure success in detecting and mitigating threats within the environment. Some of these processes include planning and preparation for the monitoring, evaluation, escalation, and remediation of threats. These planning activities can include determining areas of the network that need monitoring, determining the steps needed during investigation, and reporting requirements. Network monitoring equipment needs to be deployed and managed. Choosing the right location to monitor on the network is critically important. Network visibility is an important consideration. Depending on where this sensitive data and critical business operations occur, the placement of sensors is important in capturing the right network traffic. Once the sensors and sensor platforms are deployed, periodic system maintenance and updates are likely needed. Once the sensor platforms are in operations, they will be collecting network traffic and evaluating rules. Sufficient storage space is needed for the network packet captures and a mechanism for transferring those captures to the analyst for analysis is also needed. If an active threat or attacker is detected, then a response is needed to contain and limit adversarial progress inside the network. Once the threat is contained, then the cleanup and restoration of normal services are needed. There are several types of tools used in network security monitoring. These include collection tools, such as network taps, span ports, sensors, and packet capture devices. These tools copy, route, and filter traffic of interest and apply a set of rules looking for suspicious or malicious activity in network traffic. Events, activities, and packet captures are then collected by delivery tools. These tools take the data generated by the collection tools and transports it or stores it in databases or files used by the presentation tools. These tools are used by security analysts to dive deeper into the information about events and activities of interest. The presentation tools simplify and enhance the job of the analyst by adding capabilities and efficiency to the analysis process. The collection, delivery, and presentation tools are bundled into software distributions to allow quicker setup and use. The collection tools can be distributed quickly to systems and networks that need monitoring. The presentation tools can be up and running in short order to get a network security monitoring program operating quickly. Vulnerability management is a program to identify and reduce the number of vulnerabilities within the organization's information systems. This type of program is focused on vulnerabilities and not specific threats or threat actors. The need to reduce vulnerabilities applies across the organization. If one area is not covered by the program, then the vulnerabilities that go untreated may be used to establish a foothold by attackers and leverage that access to attack the rest of the organization. The purpose of the vulnerability management program is to reduce risk for the organization. Risk management is a key piece of the vulnerability management program and provides useful information to determine the prioritization and approach to treating discovered vulnerabilities. Most organizations view vulnerability management as a simple scan and patch approach and lack any formalized process to manage it. The challenge for organizations is the abundance of systems with vulnerabilities and the lack of resources to address them. The result can be critical business systems with exploitable vulnerabilities, fragile systems that require significant effort to maintain, or even security breaches. Here are the key processes needed for a successful vulnerability management program. Organizations should have a risk management program in place already in order to understand and manage risks. Information from risk management is needed to identify areas and systems of higher risk so the treatment of vulnerabilities can be prioritized. An asset inventory can include the hardware and various software components used within the organization. That inventory should include details on the systems and their criticality. Vulnerability identification uses technical tools to identify vulnerabilities in the deployed systems and rank those vulnerabilities by severity. There are tools that can scan for known vulnerabilities through the network. This also includes gathering information from external sources on recently identified vulnerabilities. Organizations with critical business systems need to maintain stability and reliability to prevent issues that can negatively impact business operations. Change control is a formalized process of identifying, ranking, reviewing, and deploying changes, patches, and upgrades to systems. It is a key component in treating vulnerabilities quickly and lowering the likelihood of system issues from the changes applied. Configuration management is used to maintain the configuration of system across the organization. It includes knowledge of the configuration of systems through a configuration management database. The other part is being able to affect that configuration is needed without having to manage each individual system. Finally, we get to the actual patch and upgrade process. All of the previous processes are key drivers in managing this part of the process. I hinted at a few of the tools needed to implement the vulnerability management program. Keep in mind that the formal processes need to be in place first and supported by the tools. Organizations that prioritize tools over process may find that the prioritization and system impact issues will eventually become challenges that have to be addressed. Here are some specific tools. Asset management and configuration management tools are often overlooked as key components in vulnerability management. As I mentioned previously, organizations that use only scan and patch processes may not identify all of the critical vulnerabilities within the organization because they fail to keep pace with changes to the organization's infrastructure and assets. Tools are needed to identify new assets, both hardware and software, maintain an inventory of known assets and their configuration, and to change the configuration of systems when needed. Vulnerability scanners are used to periodically scan the network looking for known and new assets and to test for known vulnerabilities. These tools use a continually updated database of vulnerabilities and tests to discover vulnerabilities. They also build various reports and dashboards using current and past scanned data to show the vulnerabilities discovered and treated. As the primary purpose of vulnerability management is to reduce risk for the organization, it is essential to understand how well the program is working. Various metrics can be created from the status of the program over time. There are some challenges to reporting in that new vulnerabilities are being discovered all the time, and sometimes a single vulnerability may appear suddenly that affects a majority of the assets within the organization.

Testing

Testing in a security context involves confirmation or validation that security controls are working as intended. The testing process exercises the controls and evaluates their response. Testing can be applied at several points in the development and deployment of a system. So let's explore testing further. The purpose of security testing can be specified in some specific objectives. Similar to other programs we've talked about, security testing should reduce risk for the organization. Testing results provide the organization with details on their current security posture which can be used to determine the next actions needed to further reduce risk. The testing process evaluates whether a security control is in place and whether that control is effective given a specific set of threats and threat actions. If the control is not present, then the test may show a lack of protection in place. Some of the tests may show the presence of a security control, but identify weaknesses in the control that allow it to be disabled, bypassed, or delayed in ways that reduce its effectiveness. Some testing can be used as an approach to evaluate the response capabilities of the organization to attack. This type of testing can be a secondary factor to technical testing, or it can be a well-designed exercise to simulate specific types of situations that the organization may face. The testing can be a training opportunity or a measurement of an organization's current procedures and staff knowledge and capabilities. For technical security testing, there are several resources to consider. The National Institute of Standards and Technology, or NIST, publish special publication 800-115, the technical guide to information security testing and assessment. This guide provides an overview of the process and procedures used for security testing without the technical details. It could be used to develop a formal program for security testing within an organization. The Penetration Testing Execution Standard, or PTES, is a living document in the form of a wiki that provides the key details for penetration testing from the beginning of a testing engagement to the reporting of results. It has a companion technical guide with specific details on the technical testing aspects. The Open Source Security Testing Methodology Manual, or OSSTMM, provides a process for measuring the operational security of an organization through consistent testing and evaluation. The last formal release for version 3 was in 2010. The Open Information Systems Security Group published the Information Systems Security Assessment Framework, which is a thorough reference for testing, though a bit old. The NIST guide and the PTES are probably the more appropriate references to study. The others offer some good detail, but seem to be stagnant in terms of adapting to the current technology landscape. Before we talk more specifically about the various techniques, let's define terms. A security assessment determines the effectiveness of an assessment target in meeting a specific set of security objectives. The objectives are sometimes loosely defined in policy or more specifically in a security plan or system definition. The assessment process can include direct testing, interviews, and observations. Security testing is the process of exercising the various security controls. In testing, the conditions are simulated, but similar to real attacks. Through the testing process, the state of the controls can be determined and then compared to the intended behavior. One of the techniques used in assessments and tests is observation. The assessor reviews the artifacts of the system and evaluates the controls. These artifacts can include the documentation of the system. Some of the documents can include the security plan, configuration, management, and response documents. A well-documented system addresses the human side of the security management of systems. Logs, alerts, and event data are generated during the operation of a system. Testing that simulates attack conditions should result in the generation of unique and specific messaging, which can then trigger a system or human-driven response. Packet captures contain network packet data recorded in flight on the wire and stored for analysis. Packet data can show the presence of network packets and their header and payload details. Assessors may use packet captures as evidence of the effectiveness of network level security controls. Configuration reviews are a comparison of actual system configuration details against a known configuration standard, baseline, or best practice for enabling security controls. File integrity is a review of known files and their contents against a database. When attackers comprise systems, they often modify the system files to disable controls, hide their presence, or prevent legitimate operations from occurring. Databases of known one-way cryptographic file hashes can be used to compare with the actual files on a system to verify that the files have recognized and trusted contents. Direct testing is used to exercise controls under specific conditions. Here are some examples of testing. When systems and software are developed, testing can be utilized in an iterative manner in the development process. There are development methodologies that utilize testing specifically in the design of the system and others that integrate testing throughout the lifecycle. Testing in development can discover issues early on in the process and result in a more secure system when deployed. Network scanning is a test looking for addressable network systems and the network services they offer. This type of scanning can determine the presence of systems and the effectiveness of network security access rules. Vulnerability identification can use the result of a network scan to identify vulnerabilities in configuration errors on the known systems in the environment. Once the vulnerabilities are corrected, follow-up scans can be used to validate that the correction efforts were effective. Password cracking is a testing technique to determine the relative security of passwords used, and whether password complexity rules are effective in practice. Penetration testing is probably a common term for most security professionals. Penetration testing goes beyond just identifying vulnerabilities and encompasses the exploitation of those vulnerabilities. It can show the level of access and compromise that a real-world attacker could attain. Network penetration testing is focused on network accessible systems within an organization. This testing can be used to test systems on an organization's network boundary, such as its public facing websites, wireless networks, and applications to test perimeter control effectiveness. It can also be used to test internal security controls against malicious insiders or attackers that compromise internal systems. Application penetration testing looks at the application's hosting environment and the effectiveness of its internal controls to limit access to sensitive data and critical business functions. Physical penetration testing can simulate attacks in a physical space. These types of tests evaluate the effectiveness of monitoring, entry controls, barriers, reception areas, and security guards against attack. Penetration testers often use social engineering techniques to acquire credentials and information, bypass gatekeepers, and attempt to avoid detection and capture.

Module Summary 

An organization needs to maintain control over its environment for security to be effective over time. The techniques we discussed in this module are key components of a security management program. In this module, we looked at the various components of an information security program for evaluating the current state of security across an organization. First we looked at auditing as a systematic process of reviewing an organization's information security program, and its conformance to a standard using an independent and objective evaluation. The audit process includes the collection of evidence and the assertions made about assets and their set of controls. Auditors review the evidence, assertions, and standards to render an opinion on the adequacy and effectiveness of controls in place. That opinion is then reported to the organization. Monitoring is a key activity in reducing risk to an organization because it involves watching over assets, detecting issues during normal operations and while under attack, evaluating the effectiveness of controls in place, and the guiding efforts needed to improve security operations. Testing is also a key activity for risk reduction, but uses a more direct approach for evaluating the presence and effectiveness of security controls under a specific set of conditions and uses the results of the testing to compare against the intended security objectives. Test results can provide the basis for the improvement of existing security controls or identify new controls that need to be implemented. For this module, we spoke only briefly about concepts that have a lot of depth and detail in other big picture courses. Here are the other courses that you'll want to look at next. Information Systems Auditing: The Big Picture covers all things auditing for information systems. Vulnerability Management: The Big Picture is a course that covers all the steps and details for proper management of vulnerabilities. Penetration Testing: The Big Picture is my own course on penetration testing. I cover a lot more detail on the use of penetration testing for organizations and the different testing approaches. In the next module, we'll look at how organizations manage security incidents, prepare for disruptions and disasters, and manage their security operations.



Managing systems and Operations 

Managing incidents and operations

Welcome back. Regardless of how well an organization has built its defenses and protections against threats, attackers, and insiders, there's always the possibility that a weakness will be found and exploited. The organization needs to prepare for these contingencies through appropriate planning, monitoring, and execution. In this module, we'll look at three specific areas, managing security incidents within the organization, planning for events that can impact the organization's ability to function, and operating key aspects of the security function of the organization. Let's start with incident management. Even organizations that have a mature information security management program will face security incidents on occasion. Those that have an incident management program in place are better able to quickly respond to incidents and limit damages to the organization. Here are some specific objectives for incident management programs. Detecting the incident and then providing visibility to the appropriate stakeholders is key. There are lots of statistics that show that threat actors invade corporate networks and remain undetected for many weeks or months. Incident managers need to ensure that incidents are detected quickly, diagnosed accurately, and reported. After the incident is detected, the organizations must move quickly to contain and isolate the threat and limit the damage that the threat or threat actor may cause. To do this, the organization must have some basic understanding of the threat and threat actors involved. Once the threat is contained and better understood, it can be removed from the environment and changes can be made to prevent the recurrence. Organizations that fail to change their environment face reinfection of malware or continued threats from persistent threat actors. Finally, normal operations need to be restored. Affected services need to be recovered so that business can continue to operate. Cleanup operations and further investigation may be needed to adapt the environment to prevent future incidents. Now let's see how organizations prepare for and manage incidents. Organizations will face a variety of threats during security incidents. The more they know about them, the better the response will be. Incident management programs need to have threat specific knowledge to be effective. Some of the threat categories include malicious software or malware. This can include viruses, Trojan horse software, worms, ransomware, and other digital threats. Denial of service is a resource attack where resources are consumed to a point that legitimate usage can no longer be supported in a timely manner or at all. Distributed denial of service attacks, or DDoS, rely on a large number of endpoints, say from a botnet, to consume network resources using a large volume of normal looking requests to take network services offline. Compromised credentials can allow attackers to impersonate legitimate users. Credentials can be harvested from social engineering or spear phishing attacks. In addition to the various threats, there are threat actors that are directly involved in the perpetration of attacks against organizations. These actors include criminals that seek to steal information, resources, or money from organizations. Hacktivists seek to embarrass or harass the organization for social change or political purposes. Advanced persistent threats, or APTs, are sophisticated attacker groups, often state sponsored, that attack specific organizations and persistent within the target environment for a long period of time. APTs try to remain undetected as they explore the organization's network environment and data and use a variety of tools to assist in achieving their goals. Malicious insiders are trusted employees and third-party vendors that have access to the organization's network. There our often less monitoring and security controls put in place for employees and third parties. These insiders may have unfettered access to the organization's information and systems and may seek to steal and exfiltrate data for profit or to expose an organization's presumed misdeeds. Each of these threats and threat actors can pose a significant challenge. The organization's incident management program must be prepared and flexible enough to manage these threats and new ones we have yet to see. Incident management programs consist of a few key components. A high level policy, a plan, and a series of procedures define the organization's responsibilities in managing incidents and the manner in which they are handled. Incident response teams are groups of skilled professionals that manage incidents. They are properly empowered to conduct this work and trained on a continuous basis so that they can handle new threats. When a security incident occurs, the teams are able to handle the incident with a methodology for gathering information, analyzing it, containing and eradicating threats, and recovering systems and restoring operations. Once the incident is understood and artifacts from any threats are collected, the team shares that information with other incident response teams in order to prevent those threats from effecting other organizations. When building an incident response capability, you should look at NIST Special Publication 800-61 and ISO/IEC 27035 as standards with a lot of good information. Incident management requires the appropriate authority and planning to be recognized as a valuable capability to the organization. A high-level incident response policy serves as a foundation of the organization's incident management program consisting of management commitment, organizational roles and responsibilities, the definition of an incident, and the associated severity ratings and reporting requirements. The incident response plan provides the roadmap of the program through descriptions of the organization's strategy and approach to incident management, communications during an incident, metrics, and a desired path to building the capabilities of the program. Procedures are the technical details used in each incident to ensure consistency and efficiency. These are typically scenario or threat based so that incidents that have been handled before are easier and quicker to handle in the future. Checklists may help ensure that all the steps are followed. Forms may be needed to gather the right information and to communicate information to others. Incident response teams vary depending on the organization's needs and staffing. In terms of the team structure, there are several models. Which is used depends on the number and severity of incidents that an organization typically handles and the availability requirement of the incident handlers. Some organizations utilize a dedicated team of professionals, or have a team of security professionals that also handle incidents on an as needed basis. Others may outsource that capability. The roles and responsibilities are typically defined in the incident response plan and include the incident response manager and security analysts. The manager is responsible for the oversight, prioritization, and communication during incidents. The security analysts handle the triage activities and specialty trained analysts handle the forensics. Some organizations may also have threat research or threat intelligence teams to gather and organize information about threats in order to have more contextual information about current incidents and to prepare for future incidents. As the threat landscape changes, so must the incident response teams. This is accomplished through training. It can consist of traditional courses, online training, certification preparation, or even forensics and incident response challenges. The team should also train together through the use of tabletop and simulation exercises. The incident handling process can vary between organizations, but there are some foundational principles that should be incorporated. The NIST Special Publication 800-61 uses the following process. Preparation is key to ensuring that the handling of incidents goes smoothly. The incident response team must have established communication paths and methods, the right set of hardware and software tools, and the appropriate organizational and technical information available. Detection and analysis are the activities needed to detect anomalous, suspicious, and malicious behaviors using the alerts and logs available to analyze that data to determine if an incident has occurred, to document the investigation activities and findings, to allocate resources based on priority of the incident, and to notify the appropriate stakeholders. Containment, eradication, and recovery is focused on the minimization of risk to the organization through the elimination of the threat and the cleanup needed to get the organization back to normal operations. Post incident activities are often overlooked, but they could be the most important to an organization that wants to mature their incident management program. Lessons learned should be incorporated into the incident response capability so that the program can mature and be more efficient with future incidents. This entire process is a cycle. Once the lessons are learned, they are incorporated into the preparation and the process begins again. Once an incident response team is in the midst of handling incidents, they are gathering threat information, malware details, and other information related to the incident and the threats. In most cases, there are other teams facing the same or very similar threats. Ideally, sharing information about the threats can help resolve incidents faster for organizations. The challenge is finding ways to share information quickly and without passing along sensitive internal data to others. Within large organizations, there may be several incident response teams, either in different business units, or distributed around different geographic areas. As the internal corporate network is likely open, internal incident response teams may be fighting the same fight against the same adversary. These internal teams need to coordinate their activities to ensure that they can contain and eradicate similar threats within the organization. Organizations in related industries also face similar threats. To aid these organizations, Information Sharing and Analysis Centers, or ISAC, have been established. In the United States, there are ISACs for state governments, research and education institutions, financial services firms, healthcare, and many others. Some of the information shared is on the Tactics, Techniques, and Procedures, or TTPs, used by threat actors. TTPs are a profile of how specific threat actors operate and includes details from the tools they use to the methods they take when entering a network to the technical details of how they compromise hosts. Several tools exist to facilitate the exchange of information. An entire course could be developed around these tools. STIX, TAXII, CyBOX, and YARA are just a few of the more common tools used.

Continuity Planning

Your company's primary revenue generating website just disappeared from the internet. What do you do now? Those next steps are the key focus of continuity planning. Let's take a look. You'll recall that the CIA triad that we discussed at the beginning of this course included the principle of availability. So far we've spent a lot of time talking about protections for confidentiality and integrity. With continuity planning, availability takes center stage. The key objective of continuity activities is centered on business survival. Some disasters and disruptions can devastate an organization to the point that it fails to continue as a viable business. Proper planning can reduce the impact of the disruption. Through the understanding of potential disruptions and planning in advance, an organization can minimize the negative effects of disruptions and disasters. The longer that an organization has to manage the internal and external impacts of disruptions, the less likely that it is to recover at all or least reduce the overall cost of the impact. Planning can reduce the time needed to restore normal business operations in a cost-effective manner. There are a variety of threats that an organization can face that disrupts normal business operations. Here are a few that an organization must consider in their continuity planning. Threats to personnel include the loss of key individuals to illness, death, work stoppage events, or retirement. As organizations are dependent on information technology, it is critically important that those systems operate correctly and are available to the business and personnel. Threats to IT systems include the business failure of current IT vendors, security breaches, connectivity issues, malware, and other digital threats, datacenter outages, significant software bugs in key business software, the theft of hardware devices, data loss, and other IT related failures. Business operations can be negatively impacted by supply chain interruptions or the loss of key suppliers, the outbreak of war, terrorist events, employment strikes, loss of or incorrect accounting data, distribution interruptions, and other operational issues. Threats to facilities can include the loss of buildings and property due to weather, environmental failures, or human-driven events. Weather events, such as floods, tornadoes, hurricanes, and lightening can damage, disrupt, or prevent access to key facilities used by the organization. Environmental control failures include heating, air conditioning and ventilation failures, electrical outages, fires, gas leaks, and hazardous material spills. Human-driven events can include damage or disruption to facilities due to war, terrorism, employment strikes, riots, and protests. Each of these types of threats may seem minor or extreme, however, to the organization, even seemingly small events can have a negative impact to the business. In contingency and continuity planning, there are several names and naming conventions of plans and planning activities. In this course, I'll focus on two types of plans. Business continuity plans, or BCPs, are planning and preparation activities to manage interruptions to normal business functions and operations. The key objective for BCP is managing the business during disruptions. Disaster recovery plans, or DRPs, are needed for large-scale business incidents that may involve the loss of or damage to business facilities. The key objective of DRP is the restoration of normal business operations at physical facilities. Some organizations have crisis management plans, which integrate the business continuity plans and disaster recovery plans in a comprehensive manner. This approach provides the organization with the appropriate integration of plans for large and disruptive events. The National Institute of Standards and Technology has special publication 800-34, which is guide for continuity planning. It is a good resource for the planning process and the various plan types. There are some important high-level processes in continuity planning to consider. There are, of course, variations of these. However, successful continuity planning needs at least these. Project initiation is obviously about starting the project. There are some important components of starting these types of projects. The most important is management support. The highest levels of management need to understand the importance of these projects and fully commit to their success, as well as allocate appropriate resources and money. The other important parts are defining the scope and objectives of the project, forming the team, and gathering the resources needed. A current state assessment is needed to understand the organization's current susceptibility to disruptions and disasters. Some components of this type of assessment include a threat assessment, risk management, and business impact assessment. Development is the process of determining the appropriate continuity strategies, developing the structure for continuity plans, and planning for testing and the allocation of resources during recovery. Implementation is the deployment of continuity plans to various stakeholders and effected organizational units, training and awareness programs, and initial walkthroughs and testing of the plans. Maintenance is the management of the continuity plans moving forward and includes the periodic review and updates of the business impact assessments, risk assessments, and continuity plans, as well as regular testing of the plans. In order to prepare for a continuity plan development, there are some specific activities needed in order to ensure that the planning process has accurate and useful information. A threat assessment is needed to understand the unique threats to the organization. These are generally organized into three types, physical and personnel security, environmental security, and information security. A risk assessment takes the understanding of threats to the business and potential impacts from the business impact assessment to understand the overall risk to the organization. The business impact assessment provides a prioritized list of critical business processes and maps their time sensitivity for recovery and the supporting resources needed. Recovery strategy development is the process of evaluating the current state information and choosing appropriate recovery strategies. The strategies should be in alignment with the time sensitivity and recovery requirements of the business processes. Here are a few of the common recovery strategies organizations may use for managing the recovery of their data. The most common and often the easiest to start with is a process of taking data offsite. This can include rotating backup tapes and other data storage media away from the primary facility. As the costs of online or cloud storage services continue to fall, organizations can shift their storage needs to storage services, either as the primary storage service or as a place to store redundant data or backups. Database mirroring is a process of copying database changes to a parallel database system located elsewhere. Electronic vaulting is a strategy of sending data to an alternate processing facility and keeping it online and available. Remote journaling is a strategy of copying transactional data in real time to an alternate processing site. Here are some strategies used for recovering IT infrastructure. Cold sites are locations that can support IT, but have no equipment in place. Warm sites have some level of IT equipment on site, but they are not complete nor configured for use. Hot sites have IT equipment and are configured for use. They can support recovery within minutes or hours. Mobile sites are trailer-based systems with some supporting IT equipment that can be brought to locations to support recovery. Mutual aid agreements are entered into with nearby organizations that have some amount of space to support a recovery. Multiple site strategies involve using other sites controlled by the organization to support recovery operations within the organization's existing property. Once the recovery strategies are developed and the continuity plans have been created, they should be tested periodically. Part of the planning process should include the level of testing needed and at what frequency. Testing serves two purposes. The first is the opportunity to identify issues, errors, and needed updates in the plans. The second is to familiarize the staff involved with the continuity plans and managing through disruptive events. There are several types of tests that can be performed. Each type has differing levels of involvement, complexity, risk, and cost. A plan walkthrough is a simple exercise of pulling the plans out and reviewing with some of the team that would be involved in implementing the plan during an actual disruption or disaster. This helps the team find obvious and sometimes subtle errors in the plans that need to be addressed or sections of the plans that need to be updated or corrected. A tabletop exercise can involve a larger group of individuals that would be involved in any plan implementation and conducting a simulation of a specific disruptive event. The situational parameters and the conditions would be established in advance and the team would use the contrived event to test the continuity plan. Using the plan, the team would evaluate the situation, make decisions based on the plan directives, and think through the steps needed. Both the plan walkthrough and tabletop exercise approaches require only a small amount of effort to conduct, are low risk to operations, and have little cost. Parallel testing is more complicated in that it involves operating business processes at the primary processing facility and its backup in parallel. If the organization has an alternate processing facility, that facility would be stood up and process the same data as the primary facility. The processing results and performance data can be compared to ensure that the parallel processing matches the primary facility's output. Full interruption testing involves switching primary processing to the alternate site and pausing or disabling processing at the primary site. This is a more complete test because the business is operating solely at the alternate processing facility. Both parallel and full interruption testing approaches are significant in the level of testing conducted and the effort required. They also have a higher risk and cost to conduct.

Security Operation

Security operations is a broad topic which includes many of the components that we touched on in other parts of this course. Most of the activities associated with security operations are tactical in nature and support the day-to-day activities of the program. Let's spend a few moments talking about how these types of operations allow an organization to manage its information security program. Security operations has a lot of moving parts in an information security program. Each organization may also categorize the components of their security operations in different ways or have components of traditional security operations managed by IT groups that do not have a security focus. Regardless, here are the objectives that security operations generally cover. Given the wide access to information technologies, events and incidents that require an investigation generally have an electronic device, organizational data, or electronic communication service involved. And information security program has to support investigations for those types of information technologies. Security operations can also be directly involved in the security of assets and users, or at least have a shared responsibility with other IT groups. Monitoring and protecting IT assets may involve specific security technologies. Security operations may be directly involved in managing those systems. Certainly the security operations personnel are utilizing these security systems to do their jobs. Security operations is also involved in the planning preparation processes to handle contingencies the organization may face. Security operations can be directly involved in investigations. The organization's own staff might be doing the work, or they might need to outsource it, depending on the skills needed, the availability of staff, or the independence needed to avoid conflicts of interest. There are several types of investigations in which they may be involved. Depending on the legal system where the organization conducts its business, the investigation approach may vary. For simplicity, there are two types to consider. An externally driven investigation is one in which the motivation for the investigation originates outside of the organization. It could be an investigation under common, civil, tort, administrative, or criminal law. The level of involvement of the security operations team may vary depending on a variety of factors. An internal investigation is generally inward focused for the organization, though the findings may lead to bringing in external entities to continue the investigation. Internal investigations may involve violation of policies or anomalous activities. During the investigation, there are specific techniques the investigators may use. The collection of evidence is needed to inform and support the conclusions of any investigation. Digital forensics may be used to explore the data or devices involved in the incident. The findings, results, and conclusions of the investigation need to be presented to stakeholders during and after the investigation. These may take the form of status reports, preliminary findings, and a final report. Asset management is focused on identifying and configuring IT assets for security purposes. An IT asset includes networking and computing devices under the organization's control. An asset inventory is needed to understand what IT assets are owned and managed by the organization. These inventories must include software, as well as hardware and virtual machine assets. Additional details, such as installed software versions, criticality ratings, and business owner information is helpful for vulnerability and risk management practices as well. Configuration management is part of the protection and prevention activities for security operations. Before assets are deployed within the organization, they should be configured to a security standard. These standards should be defined by security operations to ensure the base configuration is secure and that the appropriate security tools are installed and configured. Change management is typically an IT function that involves making changes to IT assets with minimal impact to business functions and users, or through an appropriately planned rollout. Security operations typically has to manage or at least define needed configuration changes related to security and software and firmware security updates through the organization's change management process. Media management is the classifying, labeling, storing, and eventual destruction of sensitive media. Media could be in the form of portable storage devices, backup tapes and disks, or other devices that move data physically. Controlling access to data on media can involve both physical and logical controls. Users access organizational services and information through their user accounts. You may hear the term identity and access management as the primary program for managing user accounts and access within an organization. Managing and monitoring user accounts is essential to securing the organization. While user account management may not be a primary function of security operations, security should have influence over the process and the ability to manage user accounts when needed. Account provisioning is the process of allocating user accounts and assigning access privileges to users. Security operations should be involved in defining the appropriate privilege assignments and periodically auditing account access privileges. Authentication techniques may need to vary depending on the privileges a user account is assigned. Higher privilege accounts may require multi-factor authentication to avoid account compromise. Password complexity and expiration rules may also be defined in policy and implemented or reviewed by security operations. Privileges are logical controls that allow or deny access to specific data or information services defined for a specific user or a class of users on an organization's IT systems. Privilege management is the assignment of user account privileges. Security operations should be involved in the definition of these privileges with the ability to change them when needed. Account recovery processes are needed when a user losses access to their account, either due to password expiration, a forgotten password, account compromise, or account suspension. Security operations should influence the process used in order to prevent weak processes from being exploited by attackers or an employee bypassing the defined process. Account monitoring is needed to ensure that compromised user accounts are detected and corrected quickly. An attacker with user credentials can access information and services as that user. Security operations much monitor for anomalous and malicious user activity. The termination and suspension of user accounts is needed in the event that an employee is terminated or a user account is compromised. Security operations may need to quickly disable user accounts either through a defined process with a human resources department or due to an investigation or monitoring of a user account. An organization's personnel are key stakeholders in information security. They have access to sensitive organizational data and are often the targets of attackers. Security operations may have a direct impact to personnel security. Security awareness programs help inform and periodically remind employees of their role and responsibility for protecting the organization's information and systems from attack. These programs may use visual tools, such as posters, fliers, and other printed media as reminders or informative messaging about specific attacks or threats. Security training may be required for specific compliance reasons, job function, or as a general education requirement for employees. Training can be focused on specific job functions that involve sensitive data management and use or more focused on the use of information security tools or software. Security operations are often dependent on specific security technologies to implement the controls defined in policy, procedures, and standards. The specific vendors and products used may change over time. However, the security functions generally remain the same. Here are some security technologies that security operations use and manage. We described the details for each of these elsewhere in this course, log management systems, anti-malware systems, firewalls, next-generation firewalls, and cloud access security brokers, intrusion detection and/or prevention systems, data loss prevention tools, and security incident and event management systems. As security technology evolves, I would expect this list to change. The remaining components of security operations have been covered in this course already in greater detail. As a way of summarizing these functions, I'll provide a brief listing here. Please review the sections of this course, or the other Big Picture courses for more information. These are monitoring, physical security, incident management, vulnerability management, disaster recovery planning, and business continuity planning.


Summary 

In addition to reaching the end of this module, we've also reached the end of this course. Let's quickly summarize the concepts we've studied. For this module, we focused on the management of key parts of an information security program. We started with incidents. An attacker running lose inside the network can expose the organization to significant damage. We spoke about the need for an organization to plan and prepare incident response teams, and to handle these incidents quickly and efficiently to contain and eradicate threats. Power outages, labor strikes, floods, and hurricanes can disrupt business operations and even bring about the end of some businesses. Those that prepare for the likely contingencies are more likely to survive and continue. An information security program has a number of moving parts that serve to implement the information security policies and procedures established by the organization. These activities are the operational components of the program. We spoke about the security operations that support investigations, manage the security of IT assets and users, manage the security technologies, and planning and preparation for contingencies. In our discussion in this module, we introduced some of the concepts that are explored in more detail in the other Big Picture courses. Here are courses you may want to tackle next. Advanced Persistent Threats: The Big Picture covers those pesky threats that can be driven by state actors seeking to avoid detection and to thoroughly compromise an organization. Incident Detection and Response: The Big Picture focuses on finding the threats and eliminating them. Digital Forensics: The Big Picture looks at the methods of finding and extracting evidence from digital systems. And Security Management: The Big Picture comes up again as a key course. This time, there is more in depth coverage for organizational resilience. Information security and subsequently this course can be summarized with these four key activities. Each activity is dependent on, as well as builds on the other activities. An organization needs a good foundation on which to build a successful information security program. Understanding the risks that the organization faces, applying well-understood security principles, providing the mechanisms for direction and oversight, and managing compliance with established standards can guide the organization's activities for information security. Once the organization understands risk, it can design and apply various countermeasures to protect assets. These countermeasures are the various administrative, technical, and physical security controls needed to prevent damage to or loss of valuable information and information systems. Just as an organization must change to adapt to a changing world of business, so to must an organization's information security program. Changes to the business effect risk, and new and evolving threats can render existing security controls ineffective. The information security program's direction may need to be evaluated and altered periodically. Once the information security program is established and the direction is known, there are a variety of actions that have to occur. These are the operational aspects of an information security program and include the use and maintenance of controls and the security technology infrastructure, as well as security incident handling and preparing the business for various contingencies and large-scale disasters. My hope is that you now have a big picture view of information security. You may feel a little overwhelmed by all of the pieces we outlined in this course. That's understandable and totally fine. You may specialize in just one area, but it helps to have a wider view of how it fits into the larger picture and contributes to your organization's strategy and focus on security. Information security is a journey, not a destination, and I hope you enjoy your journey.

Devopsec The bigger picture 

Course overview 

Hi, I'm Richard Harpur, and welcome to this Pluralsight course, DevSecOps: The Big Picture. We are now in the midst of a major software transformation. This transformation is empowering software engineers to build secure software. Just like agile practices enables developers to build quality in, DevSecOps enables developers to build security in. This course is a big picture course. It is designed to give you a solid, high‑level understanding of DevSecOps concepts. It is the perfect launchpad for you to start your DevSecOps journey. After completing this course, you will understand the major concepts of DevSecOps, how it enhances the DevOps process, and how it empowers developers. In this course, you're going to learn what is DevSecOps, including the DevSecOps Manifesto, what are the benefits from implementing DevSecOps, how DevSecOps can enhance a typical software development lifecycle, and how developers gain more control and become empowered. You will also learn the difference between fact and fiction when we debunk some common DevSecOps myths. You don't need to have completed any other training before you start this course. I have no doubt your organization will benefit from DevSecOps, so I'm delighted you're going to join me on this course. Let's get started.

Module overview

Hi, I'm Richard Harpur, and you're very welcome to this Pluralsight course, DevSecOps: The Big Picture. This is one of the hottest subjects in security at the moment, and I'm delighted you're going to join me on this course. Let's have a look at the first module in this course, Understanding DevSecOps Concepts. In our journey through DevSecOps, we're going to look at the DevSecOps Manifesto. This is really one of the reference documents that we're going to look at to help us understand what we're trying to achieve with DevSecOps. Then we're going to look at why DevSecOps is a good idea. We're going to look at the problem that DevSecOps tries to solve, how DevSecOps increases software assurance, and how DevSecOps is an extension of DevOps. This is the customary overview slide. As I mentioned, we're going to start by looking at the North Star for DevSecOps. That's the DevSecOps Manifesto. Then we're going to go through the reasoning why DevSecOps is such a good idea, the problem being solved, and how DevOps is extended with DevSecOps. Finally, we're going to wrap this module by looking at how DevSecOps can increase software assurance. So let's jump in and get started.

The devsecops concepts 

Before we look at the DevSecOps Manifesto, I want to make sure that we're all of the same understanding in relation to what the DevSecOps concept is. If we look at the term DevSecOps in Google Trends, we can see there is a marked increase with this term being searched for on google over the last three years, so you're definitely in the right place to learn more about this growing trend in information security. You may be experienced and already applying DevSecOps in your organization or this might be something that you've stumbled across and are very keen to understand more about. In its simplest form, DevSecOps is answering the question how to incorporate security within agile and DevOps practices. We're really looking to embed security within an already agile process. Here is a great quote from Shannon Lietz, The purpose and intent of DevSecOps is to build on the mindset that "everyone is responsible for security" with the goal of safely distributing security decisions at speed and scale to those who hold the highest level of context without sacrificing the safety required. You can read more about this and the context behind it at the link on the bottom of this slide. You can see we're trying to make security more context based. We're trying to place security decisions at the hands of those who are best placed to make these decisions.

The devsecops manifesto

I want to walk you through the manifesto. Let's look at the Agile Manifesto, firstly. Think of the manifesto for DevSecOps as a type of North Star, sort of a guiding principle. The Agile Manifesto has been around for quite some time. Let's have a look at that. If you go to agilemanifesto.org, you can see the principles that agile software development are based upon. These are very clear guiding principles. They are not absolute rules, but they're really telling you where you should be emphasizing or focusing your efforts. For example, individuals and interactions are valued more than processes and tools, working software is valued more than comprehensive documentation, customer collaboration is valued more than contract negotiation, and responding to change is valued more than following a plan. It does quite clearly set out that the items on the left are valued more than the items on the right. It doesn't mean that the items on the right can never happen in an agile software development environment. It just means that true lessons learned as part of agile software development, it has been found that items listed on the left‑hand side of the slide are more valuable and more aligned with agile development. Now let's jump over to the DevSecOps equivalent. If you go to devsecops.org, there is a manifesto that is described on this site. Again, remember the manifesto is a type of North Star or a guiding principle. If we scroll down, we will see the manifesto items are very similar in their layout and construction to the Agile Manifesto template. There are items on the left, which are valued more than items on the right. Let's have a look at this in more detail now. There are a larger number of items that comprise the DevSecOps Manifesto, nine items in all that have been identified as being significant to call out. These are valuing leaning in over always saying no. That's an important one because security and security practitioners can be branded as the naysayers, the people who simply say no. Leaning in, in other words, contributing to the team and being part of the solution is valued more than saying just simply no, you can't do something. Data and security science is valued over fear, uncertainty, and doubt. Well, this is an obvious one for anyone working in the industry. Using data to substantiate actions and remediation plans always trumps fear, uncertainty, and doubt, or FUD, as we call it. Open contribution and collaboration over security‑only requirements. Again, this is about being transparent with the holistic view of what you're trying to achieve. Consumable security services with APIs over mandated security controls and paperwork. This is critical to a DevSecOps environment. Remember I said we want to embed security within the process, hence the reference to APIs and consumable security services. No longer can we come with a clipboard just checking off a checklist. Business‑driven security scores over just rubber stamping security is pretty obvious. Red and blue team exploit testing is more valued than relying on point‑in‑time scans or theoretical vulnerabilities. Again, this is moving more towards active involvement rather than point‑in‑time scans and giving a report card. 24/7 proactive security monitoring is valued more than reacting after being informed of an incident. Again, moving our engagement to be more real time and active. Shared threat intelligence over keeping information to ourselves. We've seen this for a long time where criminals share vulnerabilities and other information amongst their own peers. So why don't we share threat intelligence to defend the systems? It makes a lot of sense to me. And finally, compliance operations over the aforementioned clipboards and checklists. We should be looking to build compliance into our systems rather than coming along later to see can we inspect compliance as part of the system?

The security problem devsecops addresses 

I want to move on and try to answer the question for you, why DevSecOps? What makes DevSecOps so powerful, and why are so many people interested in adopting it? In order to answer this question, we need to look at the software development evolution. Let's have a look at the typical waterfall methodology which has been in use for decades. Typically this starts with requirements, which roll into design, code, testing, and maintenance phases, effectively waterfalling from requirements all the way through to maintenance. Many software development organizations are now moving to a different type of methodology which involves iterative sprint planning, followed by daily shorter term scrums, or other types of short‑term deliveries, in what's called agile methodologies. Comparing the two methodologies, with waterfall, we would typically start at a point in time and then invest a lot of work and time until such point as we get value return for our effort and investment. With agile methodology, things are slightly different. We start at a point in time, but we return increments of value, smaller quantities of value, more frequently. This allows us to get a quicker return for the investment we have made. We don't have to be waiting for a very long time to get the return on investment. Instead, we can be releasing value at multiple stages along the lifecycle. Now let's have a look at how we can apply this same logic to information security, and specifically look at the friction that currently exists in the information security processes. If we look at the typical security processes in any project lifecycle, we could align them very much to the waterfall methodology, a security risk assessment followed by a security plan, code review, penetration testing, and regular audits. All follow a particular sequence and have prerequisites before they can be completed, very similar to the waterfall methodology. So before we can get information security assurance, we have to invest a lot of time, effort, and resources, and wait for all these different stages to be completed. This brings a number of different problems, and to be quite frank, these problems have been highlighted with the inability of information security to keep up this pace that software development is moving at. Some of the friction that is presented includes security being an afterthought. Security "sign off" delays projects, or that's the perception. These two points can go together. If security is not taught about up front, it can no doubt delay projects when they're nearing their release dates because issues are identified too late in the project. Another major friction point is that security is a once‑off point‑in‑time assessment. There is no continuous security built in when we approach it with a traditional waterfall methodology. The cost of re‑testing can be very high, and then security gets known for being too slow and impeding the progress and the pace at which companies need to deliver product. Another major factor causing friction is that there is not enough skills available to be securing the software development activities. Security experts are in high demand and short supply. The ratio of security experts to development experts is extremely low, causing significant gaps in secure software development. So if these are all the problems that currently exist, could we apply some of the DevOps thinking to produce DevSecOps, and would this address some of the problems we're experiencing. Dev and Ops combining to become DevOps are focused on one goal, and that is producing working software. In a true DevOps environment, the friction between development and operations teams has been removed by integrating the two teams' activities into a single lifecycle. Can we do the same for security and development to produce DevSecOps?

Devops plus srecurity

The best way for us to answer this question is to look at a real‑world example. For this example, I'm going to use the PCI DSS standard, which is the Payment Card Industry Data Security Standard. Version 3.2.1 has a requirement under clause 11.2 to run internal and external network vulnerability scans at least quarterly and after any significant change in the network. This is a longstanding requirement for the PCI requirements, and anyone operating in a PCI‑certified environment will be very comfortable and used to running vulnerability scans on a quarterly basis. But what if we changed our tanking? You recall in the previous slide we had a movement through different stages in the lifecycle from left to right. You may have heard of the term shifting left when it comes to DevSecOps. What that means is can we move the processes and activities further left to happen earlier in the lifecycle? Looking at the traditional way that this PCI DSS requirement would be achieved, we would initiate a vulnerability scan once per quarter, have a human review the results, and then retest to ensure any high‑risk vulnerabilities were addressed. If we were to shift left for this particular PCI DSS requirement, we would undertake vulnerability scans as part of our software release pipeline, and we would not release if a high‑risk vulnerability was identified. We're not constraining to once per quarter. We could do this once per week or once per day, depending on the frequency of our releases. When we embed activities such as vulnerability scanning as part of our release pipeline, we don't have an extra job every quarter to undertake. This is shifting left, but also automating our security practices. Now let's look at introducing security into the DevOps cycle to produce DevSecOps. Starting with the planning phase, we can introduce threat modeling and code standards, starting by embedding security at the planning stages of a project, shifting left. Then as code is written, we can undertake static code analysis, software composition analysis, and other types of code analysis techniques. This will catch certain amounts of vulnerabilities in code as it's being written or committed to the code repository. Once the software is built and ready for testing, we can immediately run vulnerability scans before we release it to the testers. Any major vulnerabilities can be addressed before the testing function spends their time testing software that may have major vulnerabilities in it. During the testing phase, we can run penetration testing, something that's traditionally run at the end of a lifecycle. During release, we can look at the compliance validation to ensure that the software complies with any regulatory or industry standard required. As we deploy the software, we can sign it cryptographically to ensure the integrity of the software as it's released. Moving to an operate and monitor phase, we can monitor and detect, respond and recover from security incidents. Overlaying the security practices on the DevOps lifecycle means that we can scatter the different processes throughout various phases, and we can work in tandem with the development and the operation stages. All of this encompasses better security, visibility, and control.

Security as code 

If you've watched any of my other courses, you know I'm a big fan of using examples to help us with our learning. Let's look at another example of where DevSecOps can help with traditional security requirements. In this case, we're going to look at what are the benefits of converting security practices that may be traditionally manual activities into automation, code‑based activities, so basically having our security written in software as code. There are many benefits with automating security practice. For example, code reviews could become code previews where we have automated code checkers or code being checked at commit stage so that it never gets turned into a built application that gets deployed. If we can catch the coding errors early, then it means we can reduce our overall security costs and development costs. Patching is a very common practice in good security hygiene environments. Typically, we would patch on a regular basis, either monthly or some other regular interval. However, what if we eliminated the need to patch our environments entirely? What if we were able to automate the building of environments and every new environment that was built came with all the latest patches included? So, we're effectively replacing patching as an activity by automating the building of environments. Final example is where we can replace incident response or maybe minimize it if we can't entirely replace it by increasing our time on threat modeling and other types of scenario‑based activities. If we can use threat modeling as part of our design, we may be able to minimize and reduce the number of incidents we have to deal with. Although not a fully‑automated practice, there are many tools available to help us with a lot of the heavy lifting around threat modeling. These are just some examples of how we can, again, shift left with using security as code and moving our security processes to occur earlier in the lifecycle.

CI/CD pipeline assurance 

The last topic in this module that I want to cover is in relation to continuous integration and delivery you may notice as CI/CD pipelines. They're called pipelines because they are typically a sequence of steps that occur in sequential order, usually highly automated. There are a number of different components associated with every pipeline. For instance, you will have source code as one component in the pipeline where all your code management repositories will be located. Developers will commit and pull code for modification from these repositories. From there, once source code is ready, software will be built, unit tested, and usually incur a certain amount of integration testing. Once all of this has been successfully passed, then the deployment takes place. There can be multiple different environments, including an acceptance environment, staging environment, and production where the software will live and operate for users. So you can see, there is a continuous integration pipeline and the continuous deployment pipeline. Let's now fit security on top of these different pipelines. Security design comes before the code is written, but once code is written, the security review of the code should be undertaken. Ideally, this should be an automated review. During the testing phase, security testing should be baked in an integrated part of the continuous integration pipeline. And finally, during the deployment, all deployments should be done securely by using automation to ensure that all production environments and other environments that are being deployed to are securely configured. What this approach gives us by embedding automation for security into the pipelines is a repeatable and consistent approach. Security becomes embedded in the pipeline. It's no longer an afterthought. And remember the friction points we discussed earlier where security was frequently being labeled as being too late and delaying projects. Well, that friction disappears. More than anything else, this gives us a sustainable security process for your software development lifecycle. Let's move on and sum up this module.

Module Summary 

In this module, we looked at various aspects of traditional security approaches. We identified that there's many different friction points in applying traditional security to what is now a very fast moving software development environment. We looked at how DevOps has already resolved some of the issues in existence between development and operations teams. By adding security and following the same pattern as DevOps, we can create a DevSecOps environment and reduce the amount of friction that traditional security approaches create. We need to move our thinking away from point‑in‑time manual inspection towards security as code or scripting. This will ensure that security is consistent, embedded, and automated. It will also provide better outcomes for you. As you may have concluded, DevSecOps is all about embedding security within the software development lifecycle. If you're responsible for software development teams, then you're going to find the remainder of this course very relevant for embedding and improving your security practices. Up next, we're going to look at identifying the benefits of DevSecOps.

Identifying the benefits of devsecops

You're very welcome to this module where we dig a little bit deeper into identifying the benefits of DevSecOps. In the previous module, we covered the basic concepts of DevSecOps. Now that you have that foundation under you, we're going to move on and describe some of the benefits of using DevSecOps as an approach in your organization. This is going to be a short module, but a very important one. If you're looking to start a DevSecOps initiative or indeed looking to expand it and want to get support from your peers or your managers, we're going to cover off where DevSecOps is most appropriate. It may not be appropriate in every environment, and we're going to look at that in this module. We're also going to call out the benefits of DevSecOps, and some of these benefits may not be so obvious. Finally, we're going to close out this short module looking at the roles and responsibilities within a DevSecOps environment. So let's get started

Where devsecops appropriate ?

You might already be aware of some of the benefits of implementing DevSecOps; however, one thing we must keep in mind when we're discussing DevSecOps is that DevSecOps is more about a culture and high degrees of collaboration and not solely about specific tools or products that you implement. So, for example, if you are a developer in an organization and want to advance the security controls around code that you create, DevSecOps is definitely a good way forward for improving the security posture of your code. You may, of course, need to convince or at least discuss with your manager in terms of implementing any new practices. For the manager, there are definite benefits in implementing DevSecOps also. If you're part of the IT security organization, you will really want to pursue some of the automated and continuous security checking that DevSecOps brings. Maybe you're working in IT operations keeping all the systems running, or maybe you're just tired of dealing with security incidents. In that case, DevSecOps can really bring advantages to your workload. If you already work within a DevOps environment providing automation between the development and the operations part of solutions, then DevSecOps is a natural extension of that workload. And finally, if you're looking at managing the finance and costs associated with software development, you're definitely going to be interested in exploring the benefits of DevSecOps from a cost efficiency perspective. One thing that all these different types of roles have in common is they're all trying to look for a specific justification of DevSecOps in order to provide and build support for their initiatives. So no matter what role you fit into, there is always benefits you can accrue from DevSecOps. So where is DevSecOps most appropriate? There is no hard and fast rule saying that you must have certain methodology or lifecycles in place before you can apply DevSecOps, and likewise, there is no hard and fast rule to say that you cannot apply DevSecOps in certain environments. However, some general guidelines include where you have an Agile methodology, it is highly suitable to a DevSecOps deployment. If you have existing DevOps in place, DevSecOps is a natural extension to this. You may have some of the pipelines for deploying your solutions already in place. Extending those is going to be an incremental piece of work rather than some fundamental shift in your processes. If you're doing multiple releases per year, per day or per hour, DevSecOps is definitely appropriate to your environment. If you have some automation already in place in your development lifecycle, then you already have the railway tracks laid out and you can plug some of the DevSecOps controls into this pipeline. Where you're working under a different type of structure and using a waterfall methodology, I'm not saying that you cannot deploy DevSecOps in this environment, but you may need to consider carefully how best to fit the practices of DevSecOps into that environment. If you work in a highly regulated environment that requires significant changes to any improvements or variation on the software development lifecycle, then this may not be suitable for DevSecOps. If you're only doing one or two releases per year, you may not get the payback that DevSecOps promises. So this is something you need to give careful consideration to. And finally, if you have no automation in place in your current environment, I would recommend you start with building out some automation and starting with the building block of DevOps before you look to move into DevSecOps. I'm not saying that it's not possible to undertake DevSecOps with the environments listed on the right, but there are other considerations you need to factor in in doing so.

A typical business case 

Let's use another example to help us to understand this time the benefits of DevSecOps. In this case, we'll help Ben. Ben is a software developer. He's been working with a fictitious company called Globomantics for three years now. He wants to introduce DevSecOps practices into Globomantics. We will help Ben to identify the benefits of DevSecOps so that he can get support from his manager and his peers for this initiative. So let's talk about Globomantics. They're an organization that uses an Agile methodology for their software development. They have some basic DevOps practices in place, but this is largely around deployments. Someone automated and scripted the deployments. They are open to trying new initiatives, but they have limited resources. Does that sound familiar? Does it sound like the organization you work in? They also have tight release deadlines, and this is mainly due to new successes and new customer wins they've had in the marketplace. Now that we've framed Globomantics and Ben, who works as a software developer within the engineering team, let's have a look at the typical benefits that could accrue to this scenario.

Listing benefits of devsecops

One of the most obvious benefits is that DevSecOps can reduce time on rework for security vulnerabilities. In order to help you to quantify this for your organization, we need to understand how it achieves this time reduction. Time can be saved when security issues are found within the context of where these security issues are being introduced. What do I mean by that? Well, if you have a developer who has just committed or checked in code to the code repository, and that code has a vulnerability in it, it will be far quicker for that developer to make an amendment and fix the bug in the code shortly after they've committed it to the code repository in comparison to if they have to come back weeks later and remember the code they were working on. So security issues found earlier in the lifecycle reduce the cost of remediation and rework. All that memory and brain power required to recall the code you were working on is no longer required. What this means is that more vulnerabilities will be addressed because it's easier for the developers to action them more quickly. Reducing risk for Globomantics as an organization and as customers is a real benefit. If security issues are remediated close to their point of generation, then there is a higher likelihood that the security issues will be remediated in the first place. We all know about customer requests and feature requests for products that end up on a backlog and take weeks, months or years to even get prioritized, never mind implemented. Security vulnerabilities are bugs and feature requests also. It may take weeks, months or years for some vulnerabilities to get fixed. The time it takes for vulnerabilities to get fixed is a risk exposure, both for the organization and its customers. I said earlier in this course that DevSecOps is not all about security tools or processes, but is very much about collaboration, and this is very true. It's one of the spinoff benefits of implementing DevSecOps. When DevSecOps has done well, you don't have a security team that fixes every security issue. Security becomes everyone's responsibility. The engineers that write the code must fix the issues that they have introduced. The operations people must ensure that all systems are fully patched before the deployment can take place. All of this just forces security to become everyone's issue and increases the level of collaboration. Another major benefit of DevSecOps is the introduction of more consistency or standardization through the automation that DevSecOps requires. By introducing automation, we're ensuring that consistent security scans are run at the right time in the lifecycle. We could call this continuous security because these are not discretionary steps. The minute someone checks in code, the security scans will kick off. The minute an application is built for deployment, further security scans will kick off. All of this is hands off, it's automated, meaning that you get consistency throughout the stages within the software development lifecycle. All of this leads for better compliance levels, and this has never been more important. Secure by design is a requirement by law in some jurisdictions. We have seen that the GDPR laws, the General Data Protection Regulation, requires secure by design and privacy by design to be implemented as part of development lifecycles. Other requirements from laws such as the California Consumer Privacy Act are also stipulating requirements within the development of software. Better automated security testing helps compliance with various laws.

Quantifying benefits- an example

But let's assume that Ben's boss is quite meticulous about seeking a business case for any new work or changes to the process. So let's dig into the first item here by way of example of quantifying the true benefits for Globomantics by introducing DevSecOps. We have suggested that there will be a reduction in the time required for rework or fixing security vulnerabilities as a result of introducing DevSecOps. Ben's manager may challenge us and want to know more specifics in terms of the true benefit. What we can do in this case is look at the classic bug cost diagram that has been around for decades. NIST, The National Institute for Standards and Technology, in the US and other research houses has done some significant research into this area. The findings from the research indicate that the longer it takes for a bug or issue to be identified in the code, the more it costs. It is far cheaper to identify and remediate bugs in the design and requirements phase than it is when the product is in production. Security vulnerabilities are just bugs. These are bugs that need remediation, so the same model applies to security vulnerabilities. The longer you leave it to identify and remediate security vulnerabilities is going to cost you significantly more to remediate these issues. DevSecOps is all about shifting left, moving the security testing to be as early as possible in the lifecycle. By doing this, we avoid the expensive deferral of security vulnerability remediation costs. This is a clear business case to make for your manager and your team to help you introduce DevSecOps apps in your organization.

Roles and responsibilities 

I want to talk for a moment about the various roles and responsibilities involved in building a DevSecOps pipeline. There's a number of different roles or activities I've identified here. It's important to understand that these are activities that need to be undertaken. They are not necessarily full‑time roles or team members that you need in your organization. This is particularly important if you've got a small development team. DevSecOps does not mean you need to double or triple the size of your team in order to be successful. What's more important than the size of your team is the activities that your team members perform. Tooling is important because the tooling is central to ensuring consistency and effectiveness of the DevSecOps processes. You extend the DevOps pipeline by including security testing and integrating with various security tools. Vulnerability management is another key activity that must be fulfilled, ensuring that all vulnerabilities are managed, blocking deployment in accordance with your company's risk policy. For example, if you have critical or high vulnerabilities, you may prevent the deployment from going to production. Looking at application security, it is important that you have someone competent with dealing with the application security fixes. And finally, from a compliance point of view, it's always important to ensure that you maintain proper records of the testing and the outcomes of any remediation that are undertaken. This will help you with demonstrating compliance. You should look at all of these activities and see how you can fulfill them within your existing team structure. For larger, more mature organizations, you may have the privilege of being able to hire specialists who are dedicated to each of these activities.

Module summary 

Now that we've completed the benefits of DevSecOps, let's sum up this module. We've looked at how best to leverage your existing DevOps process and extended it to include security processes. Using this incremental approach, it can be the fastest way to get the basic DevSecOps process up and running. It's important to communicate the upside and the benefits of DevSecOps. Remember, you need to involve many people and collaborate within your organization because DevSecOps is more of a culture than a dedicated team that's only focused on security. A lot of effort will be required to ensure that everyone buys in to the DevSecOps culture. And that completes our two theoretical modules on DevSecOps in this big picture course. Up next, we're going to look at more specifics on how we adopt your development lifecycle to incorporate DevSecOps. Let's have a look at that now.

Adopting DevSecOps in your software development lifecycle 

Module Overview 

Hello, and welcome to this module in our DevSecOps: The Big Picture course. In this module, we're going to look at adopting DevSecOps in your software development lifecycle. As we continue our DevSecOps journey, we've already established our foundational knowledge. We've identified the concepts of DevSecOps and also itemized some of the benefits that applying DevSecOps can bring to your organization. This module can be seen as sort of an on‑ramp to where we get into more of the technical specifics around the practices and processes you need to implement in your development lifecycle. In this module, we're going to look at positioning DevSecOps within the development lifecycle. How is your lifecycle going to be impacted when you do introduce DevSecOps practices? We're also going to look at DevSecOps from a maturity perspective. You may be starting a new initiative in your organization and trying to introduce the concept of DevSecOps and bring some supporters along with you. There is various different levels of maturity. We're going to touch on some different models that you could use to help you gauge where you are in a maturity model, but also to give you some encouragement that you don't need to have all the processes in a perfect DevSecOps environment in place from the start. Remember, making progress is more important than aiming for perfection at this stage. So let's get started.

Positioning DevSecOps within  the SDLC

We walked through this slide in a previous module. What I want to do here is take you step‑by‑step through a strategy for helping you to succeed in rolling out your DevSecOps processes. It can be overwhelming. There's lots of different phases within the development lifecycle. If you already have a product that's being shipped to production regularly, people are going to become quite nervous if you suggest making amendments to that build and deployment process, so let's have a look at how we can approach this with the greatest caution, but also being pragmatic. Starting at the planning phase, we could look at the threat modeling and coding standards and try to influence developer activity by introducing these concepts. However, most organizations should have coding standards in place. In this context, we're talking about secure coding standards. Threat modeling is something that requires some investment in time for training and experience before you can become really effective. For that reason, I would be suggesting you leave threat modeling until a later phase in your rollout. Let's move on and look at what we can do in the code section. Where code is being written, it's definitely a good opportunity to get involved. Winning over the developers is always important when you're interfering with their build pipelines. Undertaking some static code analysis can be really valuable in this phase. I suggest you start with taking some static code analysis tools and getting them integrated with your build pipeline. Developers will love hearing feedback about vulnerabilities in code they've checked in. It's far better than hearing about it just before you deploy something to production. Again, start with non‑blocking changes. Don't try to break any builds at this stage. Once you've mastered the code analysis section, moving on to the build activity, it would make sense to introduce some vulnerability scanning on the build code. There's many different scanners available, and these can be very non‑intrusive, capturing vulnerabilities before the system goes to test and before it gets released to production. I would focus on this area to start your DevSecOps rollout. The other phases in the lifecycle have many other different types of activities that can come in time. Don't be trying to attempt to deploy all of these activities at the same time. Start on the left‑hand side and gradually mature your way over to the right‑hand side to cover all aspects of the lifecycle.

DevSecOps progress and maturity models 

One thing you may have realized by now is that implementing a successful DevSecOps process is not a fast and immediate task. It's more like a marathon than a sprint. One of the techniques I really like to apply when implementing a program for change is to look at it in stages. Firstly, you need to invest significant amounts of effort to make change in your organization, but a very commonly overlooked aspect of implementing sustained change is to ensure that you allow time and monitoring to lock in that progress. Very often progress can be lost over time by people falling back into previous practices and not sustaining your new improved practice. Always follow any improvements by a period of embedding to ensure that your new practice is sustained and becomes part of the culture of your organization. There are models to help you with looking at the progress of your initiative over time. These are called maturity models, and aim to map how developed or mature your process is. There are many different maturity models applicable to different areas of our industry. I have called out two here, the first one being the DevSecOps maturity model from the DevSecOps Foundation, and you can go and look at the detail of these specifically if they take your interest. I am not prescribing one over the other, or indeed, that you have to use any of these models, but I do recommend aligning yourself to some maturity model, even if it is a custom model to suit your organization. By way of example, on this model, there are five different tiers or levels ranging from Insanity all the way through to Continuous. These different levels are rated over a number of different categories. In this model, it's Culture, Skills, Program or Outcomes, and Security Priorities. You can get more specific detail on this model at the link at the end of this slide. Another model I like is the OWASP DevSecOps Maturity Model. This covers far more granular detail in terms of measuring your development and maturity in various different areas. I have summarized these into Build and deployment; Culture and organization, you'll notice culture comes up quite a lot for DevSecOps, as it is very much a cultural change; Information gathering; Patch management; and Test and verification. This model has four different levels, which rate the understanding of security practices from Basic through to Advanced, so it's quite easy in terms of measuring the different aspects of this maturity model. You can find out more about the OWASP DevSecOps Maturity Model at the link below. Whatever you decide, I do suggest you identify one that you're comfortable with. There is no hard and fast rule that you must align to any one specific model. Select the model that suits you.

Module summary 

This module was our lead‑in to more specific details on how to implement DevSecOps in the upcoming modules. In this module, we looked at strategies to commence your DevSecOps rollout. Don't try to deploy all the tools at once. Remember, focus on progress. Don't aim for perfection straight off. As you develop and roll out your DevSecOps across the entire life cycle, it's useful to pick a maturity model that suits your organization. This can help give you a roadmap for where you should implement improvements into your DevSecOps processes. Remember, don't be afraid to customize whatever maturity model you select, making it your own. Now that we've set this context, let's move on and look at designing DevSecOps for the planning, coding, and building phases of the software development lifecycle.


Designing devsecops for plan, code, and build sdlc phases 

modules overview

Hello, and welcome to this module in our DevSecOps: The Big Picture course. In this module, we're going to be looking at designing DevSecOps for the Plan, Code, and Build phases of the software development lifecycle. You've made great progress through our DevSecOps journey, already establishing the basic concepts, understanding the benefits, and learning how to adopt DevSecOps in your lifecycle. Now, we're going to drill into more specifics around the activities required for designing DevSecOps into the planning, coding, and building phases of the lifecycle. We're going to look at tools and practices you should implement during the first three phases of the DevSecOps lifecycle, which are planning, coding, and building software. Let's get started.

Threat modeling 

I just want to remind you of this diagram, which we saw in a previous module. There are many different stages or steps in a DevOps development lifecycle. We extend the DevOps lifecycle by adding some security stages along the way. This helps us to create the DevSecOps lifecycle. By way of reminder, during the planning phase, we're going to look at threat modeling and coding standards. After that, we're going to look at static code analysis, and I'll explain the difference between SCA and SAST, and then we're going to look at the third phase, build, and how vulnerability scans can be important in this phase. Let's move on and look at threat modeling. Wikipedia defines threat modeling as a process by which potential threats, such as structural vulnerabilities, can be identified, enumerated, and prioritized, all from a hypothetical attacker's point of view. So what we're trying to achieve during the threat_modeling phase is to foresee weaknesses in the application that an attacker might be able to exploit. We're trying to get inside the mind of the attacker and build our system to be more robust to defend against these weaknesses. There are many different threat‑modeling methodologies; one of the more common is known as STRIDE, which is an acronym for spoofing, tampering, repudiation, information disclosure or leakage, denial of service, and elevation of privilege. These are all different threats that you should assess your application to ensure it's robust enough to defend against them. When attempting to threat model, it's always better to use some specific tools that are designed specifically for the process. It's quite difficult to do manually. Implementing tooling can really help you identify the threats quickly. One freely available tool, which is still being maintained, is the Microsoft Threat Modeling Tool. I've provided a link for you to download this tool. Depending on the type of solution you're building, the Microsoft Threat Modeling Tool will propose a number of different threats that you can assess your application under to determine whether it's robust enough to defend against these threats. In another one of my courses in the Pluralsight library, Getting Started with Data Loss Prevention, I dig deep into the Microsoft Threat Modeling Tool. I'll show you how to download the tool and even work through a case study where I prepare a threat model. Please go ahead and check out this course, and specifically, the Threat Modeling section to get more in‑depth information on how to prepare a threat model.

Security Code Standards 

You may already be familiar with the concept of coding standards. These are rules or patterns that are documented to ensure every developer on the project performs consistent naming conventions and programming style. The aim of coding standards is to make the code more consistent and easier to follow, therefore reduce the number of bugs that might be present in the code. Secure coding standards is similar; in this sense, it's trying to improve the code quality, in this case, reducing the likelihood of vulnerabilities or weaknesses being present in the code. Secure coding standards are language specific. So if you're using Java or Perl, or another programming language, you need to source secure coding standards that are specific to that language. The Software Engineering Institute in Carnegie Mellon University has defined their top 10 secure coding practices. These patterns can be used in conjunction with coding standards to make your code more secure. These can help you as you identify the most language‑specific secure coding standards. In fact, these practices should form part of your secure coding standards. The top 10 identified practices by the SEI include: validating input, defend against SQL injection and other types of attacks; heed compiler warnings. The compiler warning is there to give you feedback at build time to ensure you improve weaknesses in your code. Architect and design for security. It's far easier to make an insecure system than it is to make a secure system. So proper architecture and design decisions, factoring security in early in the process is essential. Keeping it simple is an often overlooked practice, but if something is too complicated, it's going to be very difficult to find weaknesses or vulnerabilities in the code. Default deny. Don't provide excess permission or access. Implement the process of least privilege. Both of these items go hand in hand. Access should only be granted on a least‑privilege basis. Sanitize data from other systems. In other words, don't trust any input, whether that's from a person or another system. Practice defense in depth. This is essential in creating a robust system. If one safeguard fails, you have another one to protect the system. Practice effective quality assurance. Quality assurance is essential to identify bugs early in the process. Having quality assurance testing that also is aware of security testing is an added bonus. And finally, as I have mentioned, adopt a secure coding standard, and make sure that all your development team complies with this standard. These are some of the key practices that will keep your code safe, and they're part of the initial plan for your DevSecOps lifecycle.

Static code testing (SAST) and sca

Now let's move on to the coding section of the lifecycle. I want to clarify something that confuses many people. We have some acronyms, S‑C‑A or SCA, and S‑A‑S‑T or SAST. These are regularly used at this stage in the lifecycle, but they are two very different activities. SAST, or static application security testing, is something that is a process that examines source code to identify weaknesses that can lead to security vulnerabilities. Think of it as a source code review and testing of the source code itself. SCA, or software composition analysis, is quite different. This is a process that looks at the open‑source components that make up your software, and checks all of these components against known vulnerabilities. Most software is comprised of some parts that are open source today. In fact, the volume of open‑source components in software today is increasing. So software composition analysis is growing in importance. The open source elements of your code are compared against publicly available vulnerability databases to identify if any of your open‑source components have vulnerabilities that need remediation. This can help avoid problems later and is always good to get done early in the lifecycle. Let's look at some of the features of SAST. SAST involves reading source code. Similar to coding standards, it's language specific, so you will have to get a language‑specific scanner for the programming language that you are using. Very often, there can be false‑positives presented, so you will have to factor in time for assessing the findings of a SAST scan. False‑positives arise because there isn't a whole lot of context available for the scanner. All they have to work with is the raw source code. SAST can be fast and automated, and integrated into your build pipeline. It finds weaknesses early in the process, which reduces overall costs for remediation. NIST, the National Institute for Standards and Technology, has published a list of source code security analyzers, which you can find on the link below. There are some open source and some commercial products listed.

Vunerability scanning 

The next stage in our lifecycle is build. This is where the source code is taken and compiled into built code. At this point, it's good to look at vulnerability scanning against the build software. I have already spoken about one technique for undertaking vulnerability scans and that's Software Composition Analysis. This will come in at this stage in the pipeline. Another practice is called DAST, or Dynamic Application Security Testing. Vulnerability scanners run on the completed or compiled code. There are many different scanners available, and all of these will compare your code against known vulnerabilities. As you can see now, we're looking at security testing for DevSecOps under various different stages along the lifecycle. We're looking at planning, before any code is written, then we look at testing the code itself as it is being written, and in this stage, we're looking at the code once it has been compiled. What you're getting is the benefit of testing in‑depth, or at various stages as the software matures from concept to delivery.

module summary

In this module, we have outlined the various security checks on your software as it progresses through the delivery lifecycle. This includes security checks at design time, before any code is written. And once code is being written, we then introduce source code testing. Once the compiled code is available, we look to run vulnerability scans. This is providing extensive depth, in terms of testing the security of your software. Next up, we're going to look at designing DevSecOps for test, release, and the operate phases of the software development lifecycle. Let's have a look at that now.

designing devsecopd in your software development life cycle

module overview

Welcome to this module in our DevSecOps: The Big Picture, Pluralsight course. In this module, we're going to look at designing DevSecOps for test, release, and operate phases of the software development lifecycle. We continue building your knowledge of the concepts for DevSecOps, and in this penultimate module, we're going to focus on the last remaining phases of the software development lifecycle, where we focus in on the DevSecOps requirements for the test, release, and operate phases.

devsecops in the test phase 
As we move into the later phases of the DevSecOps lifecycle, we focus more on testing the finished product. We have already completed analyzing the source code, so now we're going to move on to testing the built product. This will include penetration testing, validation, and also looking at what automation can be put in place to help the deployment and operations of the system. There are a number of different types of tests that can be undertaken only on the built software. These include penetration testing. This is a manual test, which attempts to compromise the security of the system using manual attack techniques. Although not automated, it is still an essential part of every delivery, including automated tasks that we've discussed in the previous modules, will lighten the load for any penetration tester, and increase the speed of which manual penetration testing can be completed. Other types of security testing include load testing, to test whether or not the system can defend against distributed Denial‑of‑Service attacks. Fuzzing is a term used for basically bashing the system with various different types of input to see, can you generate unexpected responses for the system, which may cause a security incident? Wikipedia defines fuzzing, or fuzz testing, as an automated software testing technique that involves providing invalid, unexpected, or random data as inputs to a computer program. The program is then monitored for exceptions,, such as crashes, failing, built‑in code assertions, or potential memory leaks. Typically, fuzzers are used to test programs that take structured input. Finally, this is one of the first stages in the lifecycle that multiple components come together to work as one, so a level of integration security testing is needed to have confidence that there isn't a gap in security when everything is put together.
devsecops in the deploy phase
Once we move on to the deploy phase, there are different types of testing we need to undertake. Two types of testing that are common at this phase are testing security certificates. This ensures that all transport security‑layer certificates are valid, indeed, and issued correctly. This is required to be done at this phase because normally, different types of certificates are used in different environments, and we need to make sure the certificates used in your deployed environment are valid and correct. Application hardening is also something that should be decided upon during the deploy phase. Reducing the attack surface area by hardening your servers and applications is an important task. There are many different tools available for checking the configuration of your certificates. I've run SSL Labs testing on my own website, richardharpur.com. As you can see here, a grade is given based on the checks that the Qualys SSL Labs site undertakes. In relation to hardening, it is important to understand that the more that you harden your application, the less attack surface area is available for attackers, thereby reducing the likelihood of a breach. With the Amazon Marketplace, there's a number of readily available, pre‑hardened images that you can use. So, instead of just selecting a standard Linux image, you could select the CIS Red Hat Enterprise Linux 7, hardened image, or a Microsoft Windows Server 2016, hardened image. These images are hardened to the CIS hardening standard, and this means that the attack surface area for these images is greatly reduced. As part of your deployment scripts, selecting one of these hardened images provides you with greater safeguards in relation to vulnerabilities in your environment.
devsecops in the deploy operate phase
As you move towards the operate and monitor phases, the focus changes from ensuring the application is built robustly to checking to make sure that the application is not under attack, and when it is, that we know about it as soon as possible. We also want to make sure that there's no unauthorized changes made to the configuration of the environment. Compliance as code, which checks the configuration of the environment against approved baselines and eliminates any deviation from this baseline in near real time, is important for the operate phase. Verification and monitoring, and ideally continuous checking that everything is operating within expected norms, is another key aspect of the operate phase. If we flatten out our DevSecOps lifecycle into a linear representation, we can see that all along the pipeline there's many different tools required to ensure a successful DevSecOps implementation. It can become confusing with so many different tools available on the marketplace to understand exactly where these tools fit in in terms of the specific phase they're most suitable to. One useful reference guide is this periodic table of DevOps tools, which is regularly updated. You can visit this periodic table at the link I provided on the slide. It provides various different tools for different types of activities in the DevOps lifecycle. It also includes a list of security tools, which you can see on the bottom right‑hand corner of this diagram. This is a very useful guide to help you distinguish between the suitability of various different tools for the different phases of the DevSecOps pipeline.
module overview
Let's sum up this module. What we have learned is that it's very important to focus on automated checks. By utilizing automation, we can ensure we deliver quickly and consistently. We're not relying on manual and slow activities. Automation also helps us to scale, and given the skills shortage within the security industry, scaling is an essential attribute to factor into your pipelines. It's also critical to reduce the feedback timeline between identifying an issue and providing that information back to the originator of the issue. If you can provide a developer with feedback within hours of them writing a particular piece of code, they will be far better armed to make remediation changes to that code. It will be fresh in their minds. Finally, none of this is possible without using the appropriate tooling at each phase. It can get very confusing as to what tools are appropriate for each phase. I've provided you with resources to help you to identify the available tools and see what is most suitable for the phase that you're developing in your pipeline. Up next, we're going to look at some of the myths surrounding DevSecOps, and we're going to debunk them. Let's move on and look at this final module in our course.
Debunking devsecops myths
module overview
Welcome to this module in our DevSecOps: The Big Picture Pluralsight course. In this module, we're going to look at debunking DevSecOps myths, and as this is the final module in our course, we'll have some fun describing some of the myths that surround DevSecOps. Some of the myths we're going to discuss are whether or not you need special teams to implement DevSecOps, whether DevSecOps introduces delays in your development and deployments, and whether you can go out and purchase DevSecOps just to tick the box and achieve the capability. Finally, at the end of this module, we're going to wrap up this course, and I'll give you some pointers on where you can go to get future learning on DevSecOps. Let's get started.

common devsecops myths

In an earlier module, we introduced the DevSecOps Manifesto. Let's take a quick reminder of the Manifesto right now. The DevSecOps Manifesto emphasizes certain activities over others. It's not an absolute contract to say you can do certain things and cannot do other things, but prioritizes the items on the left‑hand side over those on the right‑hand side in this slide. This is similar to the Agile Manifesto approach that has been successfully adopted for years. Let's walk through some common myths that currently exist. You might remember in an earlier module we helped Ben to describe some of the benefits that DevSecOps could bring to his organization. Ben is a software developer. He's worked with the fictitious company Globomantics for over three years. He wants to introduce DevSecOps practices, and we helped him to identify the benefits of DevSecOps so he could get support from his manager and peers. In this module, we're going to help Ben respond to some myths his team members are raising about DevSecOps. Let's have a look at the first myth. We cannot introduce DevSecOps because we need a special team dedicated to DevSecOps. This is not true. Anyone who is presenting this as a barrier to introducing DevSecOps does not truly understand what the objective is with DevSecOps. DevSecOps is all about creating empowered engineering teams, taking ownership of how their product performs all the way to production, including security. This is a quote from Larry Maccherone, a highly experienced professional in implementing DevSecOps in organizations.
more devsecops myths

Another common myth is that the security team still needs to do all security checks for us. Someone presenting this argument is making the case that even when we finish our coding, we need to hand over the code to security teams for their analysis and checks. However, again, this myth is missing the point. By handing over security checks to another team, we're introducing delays and time lag into our agile process. Instead, we should ask the security team to codify their checks so that we can build them into our development process automatically, which has tremendous benefit for time and consistency. Another myth is that we don't have enough resources to do DevSecOps. Just go and buy a tool that does it for us. DevSecOps is not about a capability. It's about a culture. Buying a tool is not culture changing. Whilst tools are required to make DevSecOps possible and efficient, these tools supplement the existing development process and help you deliver DevSecOps, if you have the correct culture in place. Purchasing a tool will not give you DevSecOps on its own. You need to ensure that you're also developing a DevSecOps culture. Next up, DevSecOps will slow down our developers. This is a common myth presented by someone who has not been through DevSecOps implementation. Remember, DevSecOps is about empowering developers to ensure their products get to production with appropriate security built in. Traditional approaches to security require testing after developers completed their coding and before deployment to production. Because this is so late in the lifecycle, it takes longer to fix and retest software compared to identifying the issue at an earlier stage in the lifecycle, so DevSecOps can actually save time and increase developer speed. It also makes it much easier for developers to fix items closer to the time that they were introduced in the lifecycle, so this myth doesn't stand up either. DevSecOps will result in our developers giving up control and won't be able to plan. With DevSecOps, developers gain control by running security checks at the best possible opportunity to help those developers fix the issues quickly and easily. No longer are developers dependent on external teams and gain control of the work and schedule. So we can see that the direct opposite of this myth is the actual result of implementing DevSecOps.

module summary

So in summary, we have learned that DevSecOps is all about empowering developers, providing more consistency and more control to the development teams. And more than anything else, it's all about developing a DevSecOps culture, ensuring that security is part of the delivered product automatically. That sums up this module. I want to give you some pointers on where you can take all the concepts that you've learned in this big picture course and drill down into much deeper implementation, in particular around security testing and other types of security activity that go along with DevSecOps. And finally, I want to wrap up the module with a thank you. Let's have a look first at where you can learn more. Congratulations on completing this DevSecOps big picture course. In this course, we covered the main concepts of why DevSecOps is a good idea and what the benefits are. However, in order to drill down further, there are a number of other courses that you can take to get into specific details on DevSecOps implementation. In the Pluralsight library, select the learning Paths tab and then Security category. Search for DevSecOps to bring up the Pluralsight path on this subject. Also, you can search for my name in the Pluralsight library and take any one of my other courses in the library. I have a number of courses in the library already, and you can also follow me on Twitter @rharpur or visit my website richardharpur.com where I post blogs from time to time on the subject of information security. But more than anything, I'd like to say a heartfelt thank you to you for taking this course and spending the time learning about DevSecOps. Hope to see you soon on another course of mine.

Introduction to information security within cloud computing 

Hi, everyone. My name is Lyron, and welcome to my course, Introduction to Information Security within Cloud Computing. I am an international cloud security consultant, author, and trainer at Profabula. The growth of technology, tools, and services in the cloud is exponential, and you can see evidences of this in your cloud management console daily. With an expanding array of cloud services and technology offerings, it is easy for the inexperienced to lack awareness of security functions in cloud offerings. In this course, we are going to identify and select secure cloud services based upon business requirements. Some of the major topics that we will cover include getting familiar with and using a common taxonomy to describe cloud computing, how to use those definitions to address business requirements through the selection of appropriate deployment in service models, and the use of a tool called the Cloud Control Matrix that will allow you to map common cloud security controls with regulatory and legal frameworks. By the end of this course, you will have the knowledge in order to select a secure cloud service that meets business requirements. I hope you'll join me on this journey to learn how to identify and select secure cloud services with the Introduction to Information Security within Cloud Computing course, at Pluralsight.

Defining cloud computing and essential characteristics

Thank you for joining me in the Introduction to Information Security within Cloud Computing. My name is Lyron. Let's get started. For those who are newly initiated to the cloud, the definition of the cloud can be vague and even a bit ethereal, like a cloud. For those who've been in technology for decades before the advent of the cloud, they can sometimes mistakenly assume that we've always had the cloud. It's just a different name now. Well, while that is not wholly true, there are definitely characteristics of technologies and services before the cloud that are exemplified in the cloud. A brief discussion of these will be helpful. Having a base or fundamental definition of the cloud will allow you to develop a roadmap for consumption that has security as it's essential base. In order to get to that definition, we will make use of leading canonical documents from the National Institute of Standards and Technology and the International Organization for Standardization and the International Electoral Technical Commission. What was before the cloud? Almost everything that we experience as a human man‑made thing was not spontaneously created from complete original thought. Think about it with one example, the smartphone. It was inspired by the cell phone, which was inspired by the phone in the home, which was inspired by the simple make‑break transmitter from almost 200 years ago. It was a generative knowledge and not spontaneous. So is the cloud. So for a moment, we'll consider some things that predate cloud computing that are part of the characteristics that are relevant to our modern‑day cloud computing. The first one is colocation and managed services. A colocation is a data center facility in which businesses can rent space for servers and other computing hardware. Typically, a colo provides the building, cooling power, bandwidth, and physical security, while the customer provides servers and storage. This was a way of deferring the direct cost of maintaining and managing a physical building. The market for colocation activities has diminished greatly over the past decade, but the principles of this have carried over into the cloud. A managed service provider deliver services such as network application infrastructure and security via ongoing and regular support and active administration on customer's premises in their MSP data center. So this could be that they're delivering their own native services in conjunction with other provider services, or that they are a direct market for the consumer and core offerings that the consumer wants. This was a way of deferring direct costs related to maintaining a talent pipeline and full‑time employees and consultants. This also has a direct carry over to the modern‑day cloud. Another model that predates the cloud was the shared service model. The IT department, along with business leads, would assess the organizational needs of departments and develop a list of critically‑needed services that spanned multiple departments in the organization. They would then design the technology platforms that would meet those technical needs, and build out the infrastructure. In some cases, it was an act of remediation, where they would address the build out by re‑configuring our augmenting existing infrastructures. The objective was to control costs and not have wasted or underutilized capacity in capitalized systems, burning electricity and unused depreciation, while waiting for workloads that rarely came. It also garnered control over expenditures or outright shadow IT as related to technology purchases. In this consolidated model, the different department would have their own SLAs or OLAs, operational‑level agreements, describing the services that they paid for through a charge‑back model, where a payment was tracked in a general ledger. A mainframe is the central data repository or hub in a corporation's processing center. It's linked to users through less powerful devices, such as workstations or dumb terminals. The presence of a mainframe often implies a centralized form of computing as opposed to a distributed form of computing. Centralizing the data in a single mainframe repository saves customers from having to manage updates to money and keeping more than one copy of their business data, which increases the likelihood that the data is current. Early mainframe systems were housed in enormous room‑side metal boxes or frames, which is probably how the term mainframe originated. The early mainframe required large amounts of electrical power in air conditioning, and the room was filled mainly with I/O devices. Also, typically, customer sites had several mainframes installed, and most of those I/O devices were connected to all of the mainframe. During their largest period, a typical mainframe would occupy 2,000 to 10,000 square feet of a facility. Starting around 1990, mainframe processors and most of their I/O devices became physically smaller. While their functionality and capacity continued to grow, mainframe systems today are much smaller than earlier systems, about the size of a large refrigerator. While some large‑scale customers outright owned their own mainframe systems, others would lease the systems, and the service was measured in MIPS, or millions of instructions per minute, and this was how customers were charged based upon their use. Hypervisors are the foundation for the creation of virtual machines as guest operating systems that can consume hardware resources with the perception that they are the only system doing so. Later in this course, in future clips, we will differentiate between the two types of hypervisors that exist in virtualization. First though, let's explain some basics. Type II hypervisors are virtualization of a guest operating system in the form of an application or process. These hypervisors predate the cloud and were used on desktops and servers to emulate environments on traditional operating systems being dozens of gigabytes in size in support of necessary application environments. Type I hypervisors, also known as bare metal, are installed on as a kernel system on the hardware as a very small form factor system measuring in size of megabytes to a few hundred kilobytes. This is the hypervisor of the modern day cloud.

Standard definition of the cloud 

We should examine the standard definition of the cloud. We will actually examine two canonical definitions of the cloud that will aid us in establishing a common taxonomy or glossary of terms when referencing the cloud. The first is the elder of the 2 published in 2011. It is entitled The NIST Definition of Cloud Computing. The Digital Object Identifier, or DOI for it, is Special Publication 800‑145. The second was published in 2014, and it is called the Information Technology Cloud Computing Overview in Vocabulary, or ISO/IEC 17788. NIST Special Publication 800‑145 tends toward an economy and brevity of words to arrive at a definition of cloud computing. Whereas the ISO/IEC 17788 document is elaborate and expansive in its definition of the cloud. Depending upon the complexity of an organization and the strategy it chooses to consume cloud services, there could be utility in either definition and efficacy with using elements of both. Note the differentiating characteristics of the cloud according to NIST. Check out the term ubiquitous, on‑demand, network access, shared pool, rapidly provisioned and released. These are actually must haves if a person is to be consuming cloud services. ISO/IEC has in its preamble some other characteristics that will help you to see when you are in the cloud, network access at a scalable and to an elastic pool of shareable physical or virtual resources, the self‑servicing, these are all generic elements of being in the cloud that have some specific implications that we will see in the next clips.

NIST definition of cloud computing 

It's time for us to examine with greater detail the NIST definition of cloud computing. Before we discuss the application in use cases of cloud services, let's clearly define the five characteristics of cloud computing from NIST Special Publication 800‑145. I like to use the acronym BRROM to enumerate the characteristics. Broad network access is a ubiquity of accessing services, which can bypass traditional tools of the web like a browser. Think about how you call your car service. It's not through a web browser, it's a cloud app. It's also the type of tools and capabilities that can be used to access services like mobile phones, tablets, laptops, and workstations. Rapid elasticity means quick expansion of resources and service consumption, but also instantly relinquishing services no longer needed. Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward, appropriated and appropriate to any quantity at any time. For those of you that have worked in technology for a few years, there was a time not many years ago when it would take 2 to 3 months to implement a new server system from initial request to configuration. Now in the cloud, the same effect can be accomplished in minutes. In the past, once you bought that server, you could not return elements of the capacity that you didn't use, but in the cloud, you can release resources immediately that are no longer being used. With resource pooling, the provider's computing resources are pooled to serve multiple consumers using a multi‑tenant model with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. There is a sense of location independence in that the consumer generally has no control or knowledge over the exact location of the provided resources, but may be able to specify location at a higher level of abstraction. On‑demand requires minimal manual engagement between the service provider and consumer. A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human intervention with each service provider. It calls to mind the repeated seeing of the TV series Star Trek, when Captain Kirk calls the engineer of his spaceship and says, Scotty, we need more power. No need to call in the cloud model, you simply consume services directly from a list in a catalog presented as a web browser management console. Measured means that you have tracking and payment due only for services consumed. Cloud systems automatically control and optimize resources used by leveraging a metering capability at some level of abstraction appropriate to the type of service that the consumer needs. Resource usage can be monitored, controlled, and reported providing transparency for both the provider and the consumer. In line with rapid elasticity, measured service is a significant business driver that help an organization change their it costs from capital expenditures to operational expenditures, that is the outright ownership of systems to just simply leasing systems as need be. This can also allow an organization to only pay for the services that they use. Do these characteristics remind you of what was in existence before the cloud? NIST defines what are called three service models. IaaS, or Infrastructure as a Service, where you have raw compute storage and networking, and consumers are responsible for launching and maintaining their own operating systems, PaaS, or Platform as a Service, where the providers are responsible for maintaining the operating system, and the consumer configures a platform like databases above it, and SaaS, where the consumer makes use of particular pre‑configured software. The provider is responsible for managing the underlying systems. These three service models will be discussed with greater detail in the clips to follow. NIST finally speaks of four deployment models. There is private, public, community, and hybrid. These also will be reviewed with greater detail, but it is important to note that the five characteristics, the three service models and the four deployment models, comprise all cloud computing. You can categorize many different details and offerings within this basic definition of the cloud.

ISO IEC 17788: Definition of cloud computing 

As stated earlier, ISO/IEC 17788 adds greater granularity in its definition of cloud computing. To begin with, where NIST includes multitenancy as a possibility or a subset of resource pooling, ISO/IEC 17788 calls it out separately and specifically as a distinct 6 characteristic. So to cover ISO/IEC adds another M to the BRROMM acronym. ISO/IEC see also speaks of cloud capability types. There are three capability types. There is the application capability, which is a cloud capability type in which the cloud service customer can use the cloud service provider's application. There is the infrastructure capability type, a cloud capability type, in which the cloud service customer can provision and use processing storage or networking resources, and finally there is the platform capabilities type, it's a cloud capabilities type in which the cloud service customer can deploy, manage, and run customer‑created or consumer‑acquired applications using one or more programming languages and one or more execution environments supported by the cloud service provider. ISO/IEC 17788 also has cloud service categories. So, it includes the service models of IaaS, SaaS, and PaaS, but it adds additional ones that include Communications as a Service, where a cloud service category in which the capability provided to the cloud service consumer is real‑time interaction and collaboration. There is the Compute as a Service, where a cloud service category displays the capabilities provided to the cloud service customer, so that they can provision use of processing resources needed to deploy and run software. There is the Data Storage as a Service, a cloud service category in which the capability provided to the cloud service customer is the provisioning and use of data storage and related capabilities. And finally, there is the Network as a Service, where a cloud service category displays the capability provided for the cloud service customer in transport connectivity and related network capabilities. These are the same as NIST in an implied way, but not stated explicitly. It is extremely important to be sure that as a cloud consumer, your organization can understand its role in the consumption process. ISO/IEC calls out cloud cross‑cutting aspects that help to understand the imperatives related to consumer due diligence when it comes to selecting and working with the provider. The first one is audit ability. This is the capability of collecting and making available necessary evidential information related to the operation and use of a cloud service for the purpose of conducting an audit. Availability is the property of being accessible and usable upon demand. Another way of saying that is whenever needed and wherever needed. Governance is the system by which the provision and the use of cloud service are directed and controlled. Cloud governance is cited as a cross‑cutting aspect because of the requirement of transparency and the need to rationalize governance practices with service‑level agreements and other contractual arrangements that are developed in the cloud service customer to cloud service provider relationship. Interoperability is extremely important, especially in the multi‑cloud world in which we live. This is the ability of a cloud service consumer to interact with cloud service, and exchange information according to a prescribed method, and obtain predictable results. Maintenance and versioning refers to changes to a cloud service or the resources it uses in order to fix problems and to upgrade capabilities. Performance is a set of behaviors related to the operation of a cloud service and the metrics that would be defined in the SLA. Portability is another important cloud cross‑cutting aspect that requires good due diligence. This is the ability of the cloud service customers to move their data or their application between multiple cloud service providers at a low cost and with minimal disruption. Protection of personally‑identifiable information is protecting and providing assurance of proper protection and consistent collection and processing, and communication of the use and disposal of personally identifiable information related to cloud services. Regulatory has to do with industry‑specific requirements or statutory requirements. These requirements could come in the form of a law or of some governing body within a particular industry. Resiliency is the ability to provide or maintain an acceptable level of service while there is a fault condition. Reversibility is the process of the cloud customer retrieving their cloud service customer data and application artifacts from a cloud service provider, and that cloud service provider to delete it, and for the cloud service customer to re‑purpose it and use it where they select. Service‑level agreements should not be thought of as being an instrument of assurance. Service‑level agreements between the cloud service provider and the cloud service consumer is basically a document of measurement properties that show the commitments that are being made in order to exact a particular service. If that service is not received, then the consumer is able to get credits from the provider. The greatest level of assurance actually comes from the audit function.
Summary 

If we follow the evolutionary track of technology from the past, it may be instructive in helping us to be ready for the future and the certain evolution that is occurring and will occur with current technology and services. Which definition does it make sense to begin using in your organization, the NIST or the ISO definition of the cloud? Do you have a complex environment that calls for a great deal of differentiation of services? Then it sounds like ISO is what you need. Are you a typical consumer that needs a basic architectural roadmap for cloud consumption, and then build details on top of that? Then perhaps NIST makes more sense. It could be a combination of the two works best. Proper definitions make ready appropriate selections of cloud services, and more importantly, set up the path for us to have a cloud experience that is architected and designed around security.
Understanding cloud depoloyment and service models 
cloud deployment models 
Next, we will look more closely at the cloud deployment and service models. As we get into this clip, we will begin reviewing the description, benefits, and responsibilities of cloud deployment models and how they would fit in to a selection criteria. Then we will define the service models, the corresponding responsibilities with them and business drivers to consume a particular type of service. We will then close out that discussion with looking at threats. Finally, we will look more closely at Cloud Security Alliance's logical model. Let's get into the four deployment models of cloud computing. The first one that we will take a look at is the private cloud description. It's important to note that a virtual private cloud, VPC, is not the same as a private cloud. A VPC is simply a representation of network‑based controls that allows public cloud consumption to be done in a logically blocked setting from other public cloud consumers. A private cloud consists of computing resources used exclusively by one business or organization. The private cloud can be physically located at your organization's onsite data center, or it can be hosted by a third‑party provider. In a private cloud, the services and infrastructure are always maintained on a private network, and the hardware and software are dedicated solely to that single organization. In this way, a private cloud can make it easier for an organization to customize its resources to meet specific IT requirements and other strong regulatory practices that may require high levels of privacy. With the private cloud, control is maintained over data, and the underlying systems and the applications. If there is a real or imagined heightened need for security, that need would be met, which is an organization maintaining a very low‑risk appetite, being comfortable with clouds that are private. It could be an organization that is heavily regulated or that comes under strong jurisdictional controls over where data is located. Private cloud responsibilities are the consumer or provider could manage the private cloud. Just recall that if the provider is managing it, then no one else is using that equipment or any of the services. The provider or consumer could own it. It doesn't matter if it's owned by the consumer or owned by the provider, or could possibly be both. The infrastructure could be on or off premise, and there is exclusive access by a single organization. Public clouds are the most common way of deploying cloud computing. The cloud resources, like servers and storage, are owned and operated by a third‑party cloud service provider and delivered over the internet. Microsoft Azure, AWS, Google, Box, Salesforce are examples of public clouds. With a public cloud, all hardware, software, and other supporting infrastructure are owned and managed by the cloud provider. In a public cloud, you share the same hardware, storage, and network devices with other organizations or cloud tenants. You access services and manage your account using a web browser. Most services that we use today, think of web‑based mail, online Office applications, storage, and testing, and development environments are all in public clouds. With a public cloud, your service provider provides the maintenance. There is an illusion of infinite resources and on‑demand resources available to meet your business needs. There is a vast network of servers that ensures against failure. The responsibility of the public cloud are as follows. The provider manages it exclusively. The provider owns the infrastructure exclusively. The infrastructure would always be located off‑premise from the consumer, and there is multitenant access. With a community cloud, here we have a shared platform, along with some common data or information with multiple and possibly disparate organizations that have an exclusive membership. While there are characteristics like a public cloud, it is not public. Governance extends to the relationships with those members of the community, not just the provider and the consumer. The characteristics are a mix of how you would approach public cloud and a hosted private cloud when it comes to governance. It can be a combination of disparate organizations with the need to share this common information that's actually driving that mixture of characteristics. The drivers for a public cloud would include having tools of governance and contracts that would have some economies of scale of a public cloud provider. You would also have an environment that's configurable based on community consensus, as with the hosted private cloud. This also includes community membership relationships, financial relationships, and how to respond when a member leaves the community. With the community cloud, the responsibilities are the consumer or the provider manages the cloud, provider or consumer owns the infrastructure, the infrastructure could be located on or off‑premise, or even both, and there is exclusive access by member organizations. Finally, let's look at the hybrid cloud. It's often called the best of both worlds, where hybrid clouds combine on‑premises infrastructure or private clouds with public clouds, so organizations can reap the advantages of both. For instance, you can use the public cloud for high volume, lower‑security needs, such as we‑based email, and the private cloud for other more sensitive business‑critical operations, like financial reporting. It also allows for a cloud bursting. This is when an application or resource runs in the private cloud until there is a spike in demand or over subscription, at which point the organization can burst through to the public cloud to tap into additional computing. resources. In a hybrid cloud, the organization maintains high levels of control, as in a private infrastructure for sensitive assets. The flexibility is increased. You can take advantage of additional resources in the public cloud when you need them. It is cost effective in that the ability to scale to the public cloud, you only pay for the extra computing power. Ease of use and transitioning to the cloud doesn't have to be overwhelming because you can migrate gradually, phasing in workloads over time. Hybrid cloud responsibilities could be the consumer or the provider is doing the managing. The provider or consumer could own the infrastructure. The infrastructure could be located on‑premise or off‑premise, and you have flexible access based off of need.
Cloud service models 

Next, let's look at the cloud service models. Recall that there are three cloud service models, IaaS, where services are raw, compute storage and networking need to be composed by the actual consumer, PaaS, or Platform as a Service, where the provider is responsible for maintaining the OS and the consumer configures a platform like a database above it, and Saas, or Software as a Service, where the consumer makes use of a particular pre‑configured software, the provider is responsible for managing the underlying systems. With Infrastructure as a Service, it reminds me of going to the market in order to purchase groceries. Normally I like to shop for raw fruits and vegetables and other items that are not pre‑made. When I acquire these items, I must have the knowledge and ability to prepare these things on my own. I cannot expect that the store is going to prepare these raw items for me, so I have to have those capabilities. In the same way, the Platform as a Service needs to have engineers and architects to actually build out the logical environment. In IaaS, the major business drivers for consuming it is an organization has a need for raw compute and storage to manage a diverse and flexible compute and storage over their data. Rackspace is an example of this, Google Compute Engine, or a GCE, DigitalOcean, or Magneto 1. These resources are pooled into an abstraction of orchestration and virtualization. The abstraction, often via the virtualization, frees the resources from their physical constraints to enable pooling. Then you have a set of core connectivity and delivery tools that are tied to this abstraction for further consumption. In IaaS, the provider is responsible for the data center, the equipment, and the hypervisor maintenance and health. A series of physical servers each run two components, hypervisor for virtualization and the management or orchestration software to tie into the servers that connect them to the compute environment. The top security threats are related to a lack of due diligence. The customers should ensure that security partitions reliably isolate tenets from one another. This isolation must be present throughout all IaaS infrastructure components, the host, virtual machines, compute, memory, network, and storage. Platform as a Service reminds me of going to an all‑you‑can‑eat restaurant or a smorgasbord, where there may be diverse food represented from all around the world. Have you ever eaten at a place that could serve Viennese, Vietnamese, Indian, American, Italian? Just as these various food types are prepared for you, so are the many different platforms that could be consumed directly without having to install and configure an operating system. Major business driver for consuming PaaS is to reduce the need for managing and configuring virtual machines. The organization is primarily concerned with developing and deploying applications upon a platform. With Platform. as a Service model, the vendor offers a pre‑composed compute system, where developers can develop and deploy applications. AWS' Elastic Beanstalk, Heroku, Windows Azure, most used as a PaaS environment. Force.com, OpenShift, Apache, Stratos are just a few examples of Platform as a Service environment. The Platform as a Service environment avoids the need to build a virtual server environment to run an application and the need to install a development environment for creating applications In PaaS, the cloud user only sees the platform, not the underlying infrastructure. For instance, a database can expand or contract based off of needs of utilization without the customer having to manage individual servers, networking, and patching. The platform, including how they configure any offered security features, would be a mix of maintenance that's done by both the consumer and in an opaque way from the provider. Top security threats include lack of service and process isolation, user‑level permissions, malware, and backdoor or trapdoor situations. Software as a Service reminds me of going to a highly‑rated restaurant that serves a specific food type. If you go to an Ethiopian restaurant and want to be served a Polish dish, that probably won't happen. Normally the menu is limited to that specific type of food, and within those constraints, there are two things that you need to bring with you to the restaurant, your appetite and your wallet. Software, as a Service has as a major business driver the consumption of a SaaS environment for a specific data need that is being furnished by a specific application. For instance, SaaS environments deliver specific services based upon specific customer markets. The customer pays for the specific service that they require from a specific menu or a catalog. Some examples are BigCommerce, Google Apps, Salesforce, Dropbox, Mailchimp, Zendesk, and DocuSign. SaaS services are full multitenant applications with all the architectural complexities of any large software platform. Many SaaS providers build on top of IaaS and PaaS due to the increase agility, resilience, and potential economic benefits. The cloud provider is responsible for nearly all security, since the cloud user can only access and manage their use of the application and can't alter how the application works. Primary threats with a SaaS environment include lack of granularity with data access controls and lack of in‑point protection mechanisms applied. It is important to understand that there is a shared responsibility between the provider and the consumer related to services being consumed. Generally, data will always be the responsibility of the consumer, but the further up the stack you go, the more responsibility actually falls on the provider, for instance, in a SaaS environment. The further down the stack you go, the more responsibility falls upon the consumer. They're responsible for patching and for maintaining their virtual machines and all of the applications that are built on top of the IaaS that they are consuming. Whereas PaaS is sort of this middle area in what we could term a water line. This water line says that the provider has greater responsibility at the top and less responsibility at the bottom, and the converse is true for the consumer.

CSA'S Logical Model

It's time for us to turn our attention to the Cloud Security Alliance's logical model. The stack of the logical model appears as follows, the Infrastructure, the Metastructure, the Applistructure, and the Infostructure. If you are not indoctrinated in the CCSK learning, you may have never heard of any of the terms listed in Cloud Security Alliance's logical cloud model, except for Infrastructure. All but one of the terms used to describe the logical model, namely Infrastructure, are terms that are coined in depicting Cloud Security Alliance's logical cloud model. While you will probably not experience the other three terms outside of the CCSK, they are important to understand if you intend on testing your knowledge as a CCSK. Let's look at these individually. The first is the Infrastructure logical model. The core components of computing systems, compute, network, and storage, is included in that model. The foundation that everything else is built on in the Infrastructure, for example, both the physical infrastructure used to create the cloud, as well as the virtual infrastructure used and managed by cloud users, are inside of the infrastructure. In a private cloud, the same organization might need to manage both. In public, the cloud provider manages to physical infrastructure, while the consumer manages their portion of the virtual infrastructure. Infrastructure security maps to infrastructure. The protocols and mechanisms that provides the interface between the infrastructure layer and the other layers is known as the Metastructure. The Metastructure binds the technologies and enables management and configuration through cloud controller tools for compute and storage. These functions are centrally controlled through what is called the management plain, and for the consumer, it's the management console. The security mapping is associated with the management console. The Applistructure contains the applications deployed in the cloud and the underlying application services used to build them, for example, Platform as a Service features like message queues, artificial intelligence analysis, or notification services. Application security maps to Applistructure. Finally, there's the IInfostructure. What's the difference between data and information, which one is more valuable? Data is typically seen as ones and zeros, or binary, data is what the computer works with as a foundational input. Information is seen as data put into context and made meaningful for the human. The value of each depends on the circumstance, in one context on structured data could possibly carry more value through machine learning than could information. In another context, the information that is gleaned in a report could carry more value than the data because it allows the organization to make an important decision. In coming clips, we will evaluate the different types of storage, where data and information are retained that will include two major families of volume and object storage as attached to a VM, or in a database, or as a file storage system. Security maps to the Infostructure when we're talking about the data. So data security maps to Infrastructure, also, this would include information.

M3 C4 Summary 

What services will you select if you are in a situation where developers are distributed across a large geographic space? Platform as a Service will give you immediate benefits in that it is a collaborative work environment. If you need more control of your underlying systems and maybe just moved from a data center, Infrastructure as a Service may give you all the control you need, as it is like a virtualized data center. If your organization comes under heavy regulations or legal requirements for data privacy, you may actually be considering a private cloud as opposed to a public cloud, which may get you your quickest return on investment. It's very important that organizationally, there is a consideration of the risk and threats that are associated with each choice. There must be a determination of how you are going to protect the assets despite the systems that are being consumed. Finally, it's important to think of the applications of the logical model and how that will define your consumption. If we do not make the connection between the layers in the stack and the ways in which we need to protect our assets, we could be walking into a situation of high risk and low reward.

CSA Enterprise Archetecture 

Let's consider what it takes to establish a secure cloud architecturer. First, we will look at the Cloud Security Alliance's Enterprise Architecture, and start from there, getting a look at what are the essential elements of good security. Then, we will take a look at the NIST reference architecture and the roles that it assigns for proper cloud governance. Finally, we will understand the tools to help us in selecting a cloud security provider. The Cloud Security Alliance Enterprise Architecture is composed of four pillars. It is both a methodology and a set of tools that enable security architectures. As you see, the architecture is actually composed of previously‑developed frameworks that are standardly used throughout the industry. Each of the four pillars has a basis or influence in pre‑existing architectures. We will break down each of the four pillars. What follows is not exhaustive, but gives a great amount of detail into the underpinnings of the Cloud Security Alliance Enterprise Architecture.

CSA-BOSS Pillar 

Let's consider the Cloud Security Alliance's Business Operations Support Services pillar contained in the enterprise architecture. First, we began with the Business Operations Support Services. It combines the best practices for reference frameworks to align with business and transform the business into a security practice across the organization. It also is a business enabler. While many security architectures may focus on technical capabilities only, this architecture particularly takes into account practices that eventually can enable business‑relevant information about the health of the organization regarding its information assets and business processes. A common concern that an organization will have is how to integrate services with cloud providers and have a sense of security from the provider that offers confidence in their exposure with the data that they have with that provider in a multitenant environment. The Business Operations Support Services helps to identify those aspects that must be considered, besides the technology solutions such as legal guidance, compliance and auditing, and human resources, and monitoring capabilities. The awareness of security throughout the whole of the organization is promoted. The BOSS architecture is based upon SABSA domains. Compliance has to do with audit planning, independent audit, third‑party audit, internal audits, contractual audit maintenance, information security regulatory mapping, and intellectual property protection. Data governance has to do with data stewardship, data classification, clear desk policies, secure data disposal, and handling and labeling of security information. The operation risk management has to do with operational risk committees, crisis management, business impact analysis, business continuity, key risk indicators, and risk assessments. Human resources focuses on employee termination, employees agreements, job descriptions, employee awareness, roles and responsibilities, and employee code of conduct. The security monitoring focuses on some technologies like seen platforms, event monitoring, application monitoring, endpoint monitoring, and event correlation in cloud monitoring. Internal investigations focus on forensic analysis and email journaling, and the legal services focuses on contracts, e‑discovery, incident response, and legal preparation.

CSA-ITOS Pillar 

Now let's review the Cloud Security Alliance's information technology operations and support pillar contained in the enterprise architecture. The information technology operations and support, or ITOS, outlines all the critical services that an IT organization will have in order to support the business needs. This domain provides alignment of industry standards and best practices like a project management office, capability maturity model integration, or the adoption of ISO/IEC 27002 controls, or applying COBIT, or, as is stated here, using ITIL version 3. It is good to note that it is ITIL version 3 and not 4 that is used to describe the Cloud Security Alliance's enterprise architecture. All of these provide a reference from two main perspectives that enables the organization to support the business needs, aligning them with industry standards and best practices. Relationships between technology components, however, are not intended to be one‑to‑one matching, so that the process touch points described in the PMO or the ISO/IEC certification process may not actually line up with what an organization is doing at one time, as opposed to what they may be doing at another time. The domains include IT operations for disaster recovery planning, IT governance, resource management, project management office, and the portfolio management process. It would connect with service delivery, the service level management and maintenance, the information technology resilience, application performance monitoring, and asset management. The service support would include configuration management, knowledge management, change management, incident management, and problem and release management.

CSA=Services Pillar

We will look at the Cloud Security Alliance's services pillar contained in the enterprise architecture. The TOGAF‑based benefits in the pillar of Cloud Security Alliance's enterprise architecture has four different services that it's concerned with. The presentation service domain is where the end user interacts with the IT solution. The security requirements for the presentation domain will vary based on the type of user and the type of service being provided. With the application services, these are the rules and processes behind the user interface that manipulates the data and performs transaction for the user. For the information services, this is all the data needs that have to be transformed into useful information that business asset owners can use to prioritize, strategize, and manage their risk portfolio. And then the infrastructure services has to do with the IaaS stack inside of the service model. It provides the basic core capabilities that support higher level capabilities in other areas of the architecture. TOGAF‑based domains include facility security, asset handling, controlled access with surveillance monitoring, and power redundancy, patch management, which would include compliance monitoring and service discovery, storage services, which is concerned with provisioning, migrating, and sanitizing physical and logical storage, equipment maintenance, which is concerned with assuring that physical infrastructure devices are appropriately maintained to assure their continuous operations, availability services that will allow information to be available wherever and whenever it is needed, controls at this level would include data mirroring between geographically dispersed sites, network services is concerned with managing the security risk posed by the network environment. Controls that this level would include proper network segmentation of, for example, provisioning basic network services such as a traceable and accurate transport of information. And then finally virtualization, which would include server, desktop, application storage, network, database, mobile device, and smart card virtualization. The other domains of TOGAF would include service delivery, the discipline concentrates on proactive delivery of service information for the Information Communication Technology Group, reporting services, which is focusing on the ability to present data in various ways, going from top level aggregated dashboard and drilling down all the way to raw data. There's the service support, this is where groups work for information sources coming from information technology operations and support, user directory services would include a system that stores, and organizes, and provides access to information about users in a lightweight directory access protocol format. With TOGAF, the domain for application services would include programming interfaces, and this is how applications are able to talk to each other. Input validation is a very important component of this process. The security knowledge life cycle is a way of building secure applications and keeping a development team up to date on the latest threats and appropriate countermeasures that they can take in their development process. The development process would have security as its focus, and security would really be treated as a solution with source code scanners and other tools used to ferret out common flaws in the code. The integration middleware focuses on tools like service buses and message queues that allow applications to exchange information without talking directly to each other. The presentation services for TOGAF would include consumer service platform, where you have the container that holds the various types of presentation modalities that is consumer oriented. You also have the enterprise service platform, where this container holds the various types of presentation modalities that are oriented to support the enterprise in the workplace, and finally endpoint and mobile and smart device protection at the presentation layer.

CSA-Risk Management Pillar 

Explore the last pillar, risk management, in the Cloud Security Enterprise's Architecture. With the security and risk management pillar, here we are focusing on the information security program. The idea is to safeguard assets and detect the access and monitoring risk inherent to operating activities within an organization. These capabilities include identity and access management, GRC, or governance, risk management, and compliance, and setting up policies and standards within the organization that would include threat and vulnerability management and infrastructure in data protection. The security and risk management domains that are based on Jericho would include the GRC program. This is the fundamental issues of governance in a enterprise risk management organization for cloud computing, and an organization would need to implement the appropriate organizational structures, processes, and controls to maintain an effective information security governance program. The infosec management is the main objective of the information security management system within an organization. Here, the implementation is designed to have appropriate measurements in order to minimize or eliminate the impact that a security‑related threat and vulnerabilities might have on the organization. There is privilege management for the infrastructure. Here, the identity and access management is following the principles of least privilege. The threat and vulnerability management focuses on the core securities such as vulnerability management, threat management,, compliance testing, and penetration testing. Vulnerabilities really focus in on the known issues, whereas penetration testing could expose the things that you do not know. Data protection in this information age is about data being an asset and really looking at it from its three levels of existence, data at rest, data in use, and data in transit, in always trying to protect it from harm.

NIST cloud computing reference architecture 

NIST Cloud Computing Reference Architecture helps to identify roles and responsibilities in the cloud. Special Publication 500‑292 expresses 5 roles, the cloud consumer, the cloud provider, the cloud broker, the cloud auditor, and the cloud carrier. This cloud computing reference architecture helps to accurately communicate the components and offerings of cloud computing. What we will focus on are the guiding principles used to create their reference architecture, namely by referencing the cloud actors. The first is the cloud consumer. This is a person that maintains a business relationship with and uses the service from the cloud providers. It could also be an organization. The cloud consumer is the principal stakeholder of the cloud computing service, and it represents a personal organization that works with the cloud provider to consume that service. That service is provided and consumed by means of a service catalog from the cloud provider. The cloud consumer requests the appropriate service, sets up the contract with the cloud provider, and then uses that corresponding service that's been provisioned and also pays for what is used. Cloud consumers need service‑level agreements to specify the technical performance requirement that would be fulfilled by the cloud provider SLA can cover terms regarding the quality of service, the quantity of service, the security, remedies that should happen if there is a service performance failure. The cloud provider is a person, or an organization, or entity that's responsible for making the service available to interested parties, namely the cloud consumer. Cloud providers acquire and manage the computing infrastructure required to run the cloud software that then provides the service. For Software as a Service, the cloud provider assumes most of the responsibility for maintaining that service. They have the responsibility of managing and controlling the applications and the infrastructure, while the cloud consumer has a limited administrative controls over the applications. The cloud consumer will always be responsible for their data. In a PaaS environment, Platform as a Service, the cloud provider manages the computing infrastructure for the platform and runs the cloud software that provides the tools, such as integrated development environments, or IDEs, or other types of software like database applications that they can scale up and down. In this case also, the cloud provider is managing the underlying systems like network servers, operating systems, or storage. In an IaaS environment, the cloud consumer, through a set of service interfaces, mostly APIs, gets in contact with the actual infrastructure for raw compute and storage. Here, they will develop or launched their own guest operating systems, be responsible for patching those operating systems. The only thing that they don't manage is the actual hypervisor or the physical equipment that's inside of the data center. The cloud auditor is a party that can perform an independent examination of the cloud service controls with the intent to express an opinion of the results of that audit. Now this audit could actually be designed specifically for looking at the security controls, the privacy impact, the performance. If it is the security controls, they may be looking at them from an operational, from a management, administrative, and technical viewpoint to see if proper countermeasures have been put into place. The cloud auditor can make an assessment of the security controls in the information system to determine the extent to which the controls are implemented correctly and operating in the way that they should. A privacy impact audit could help federal agencies, local agencies, national agencies in compliance with applicable privacy laws and regulations governing an individual's privacy, but it also could help the consumer to understand the level of compliance and obviously help the provider to know if they are compliant. As the complexity of the cloud evolves and the tools and the management of that evolves, it may be necessary to consult with a cloud broker. The cloud broker helps to manage the use, the performance, and the delivery of cloud services, and negotiate the relationship between the cloud provider and the cloud consumer. In general, there are three ways in which a cloud broker can assess the consumer. The first one is service intermediation. This is where a cloud broker helps to enhance a given service by improving specific capabilities and providing value‑added services to the cloud consumers. This improvement could be managing access to the cloud service, or identity management, or performance reporting. With service aggregation, this is where a cloud broker will combine and integrate multiple service into one or more services. The broker provides the data integration, and then ensures that the data is secure as it's moving between cloud consumer and possibly multi‑cloud providers in a multi‑cloud environment And then there's the service arbitrage. This is where the services, just as an aggregation, are made available, except that the services being aggregated are not fixed, and the cloud broker has the capability of arbitraging this service so that there is flexibility to choose from multiple agencies. There could also be a situation where the cloud consumer is taking service in at one level, say IaaS, and then giving the service out at another level, say PaaS or SaaS. The final actor is the cloud carrier. The cloud carrier is the primary intermediary that provides connectivity and transport of cloud service between cloud consumers and cloud providers. The cloud carriers provide access to consumers through network telecommunications and other devices. For example, it could be by means of smart devices, by IoT, by regular laptops, computers, and mobile phones that are being used in order to consume the service in the cloud. Does that remind you of anything? It should remind you of broad network access, The relationship that's managed between the provider and the actual cloud carrier is also done by means of SLAs. There are some very interesting constraints and issues that could come up that the consumer would probably want some transparency on when it comes to that relationship between the cloud provider and the cloud carrier. In many cases, the cloud carriers are becoming so large themselves that they are actually a cloud provider, and vice versa.

Using the cloud control matrix

Let's examine the Cloud Control Matrix. A helpful tool that we can use in determining the efficacy of the controls of a cloud provider is the Cloud Control Matrix. It allows consumers and providers to think in terms of specific controls of the cloud being mapped to specific regulations and frameworks. It provides a common set of expectations between provider and consumer, and for me, it's the best use of an Excel spreadsheet I have ever seen. All of the information concerning a breadth of regulations and frameworks contained in one location are mapped to specific controls. There are 133 controls and 16 privacy cloud control domains in number. Each domain has multiple detailed sub‑categories that helped to form the 133 controls. While we will not entertain all categories, we will give an overview or sample of the mini‑sub categories that exist. The cloud control domains consist of application and interface security, which includes customer access requirements, data integrity, and data security, audit assurance and compliance, which includes audit planning, independent audit, and information system regulatory mappings, business continuity management, which includes planning, testing, data center utilities, documentation, and environmental risk, and change control and configuration management, which includes outsource development, quality testing, and product changes. Data Security and information lifecycle management includes classification, inventory flows, e‑commerce transaction, handling, labeling, security policy, and ownership. Data center security includes asset management, controlled access points, equipment identification, offsite authorization. Encryption key management includes entitlement, key generation, sensitive data protection. Governance and risk management include baseline requirements, data focus, risk assessments, management oversight, policy enforcement, and policy impact and risk assessments. Human resources includes asset returns, background screening, employment agreements, nondisclosure agreements, training and awareness programs. Identity and access management includes audit tools access, credential lifecycle provisioned management, policies and procedures, and segregation of duties. Audit logging and intrusion detection, change detection, information system documentation, and vulnerability management are all part of the infrastructure security. And then a extremely important one, interoperability and portability, which I would suggest looking up the ISO/IEC documents on these before you consume cloud services. It's imperative if you do not want to experience lock in with a cloud service provider due to data format and translation of data elements. The mobile security includes anti‑malware approved application list, approved bring your own device software, awareness training, and device eligibility. These security incident management domain includes authority contact, in cases of emergency contact, authority for maintenance, incident reporting, incident response, legal preparation, and response metrics. Supply chain management includes transparency and accountability, data quality and integrity, and provider internal assessment governments and reviews. Threat and vulnerability management includes mobile code, anti‑virus malicious software, and patch management. Here is an example of what it looks like in the Cloud Control Matrix from the standpoint of a HIPAA requirement. You can see the actual control specified in high tech and the control that corresponds to the cloud element. Here's one for PCI DSS. What if you're an organization that makes use of payment card transactions. Note the mapping and specific cloud control to specific requirements of the PCI DSS. It's an excellent document to use in order to know where the cloud controls are with the cloud service provider.

Selecting a CSP

Finally, let's discuss selecting a CSP. In selecting a CSP, there are some primary considerations for doing this and tools that we could use. Selecting a CSP involves translating business needs into a matching service, delivery or a consumption strategy. Now, while there may be technology elements that enable a specific service, the language that is used with that business unit must be centered around end‑to‑end services. It's not about technology. You will be able to find a great deal of self‑serving information about a provider on their website or perhaps even from clients, but it's really helpful to be able to get empirical information through audit about that cloud service provider that you are potentially going to consume services from. It is not just the delivery of service that will position an organization to succeed in their mission. They also need to be aware of and adhere to whatever laws, regulations, or jurisdictional requirements are mandated for their service consumption and the corresponding data that is maintained. One tool that we could use to refer to the documented capabilities of the cloud service provider is Cloud Security Alliance Security Trust in Assurance Registry. It contains the registration of cloud service provider controls. This is a self service on‑demand location where consumers can go and assess the controls of the potential providers, and then consumers can decide to utilize a provider or not based on differing levels of assurance. The Cloud Security Alliance codifies three levels of assurance in the open certification framework. CSA STAR level 1 is a self assessment that has 2 sub levels, the Cloud STAR Self‑Assessment with the GDPR Code of Conduct and Self‑Assessment and Continuous Self‑Audit. There's also CSA STAR level 2. This is a third‑party certification that has two sub levels as well. And then there is CSA STAR level 3. This is full cloud assurance and transparency. Every time you go from one level to the next, you're actually taking all of the capabilities that were expressed in the previous level and adding to them to come to a higher level of assurance for the cloud customer.

Summary 

So what will be your approach to developing a secure cloud architecture? It is extremely important that we start there, long before we think about the actual design and the actual services that we are going to consume. That cloud architecture, if developed in a secure fashion, will mean that the services also have a greater chance of being delivered in a secure fashion as well. Have you clearly set out the delineation of responsibilities based upon the services that you are looking to consume? Remember, the lower you go in the water line, in the water mark of the stack of IaaS, PaaS, and SaaS, the more responsibility is on you as the cloud consumer. The higher you go up, the more security responsibility is on the providers, say, for instance, in a SaaS application. Delineating the responsibilities and understanding them before you consume a cloud service is key. Now that you have seen all of the ways in which we can consume cloud services, you may be looking at picking a cloud service provider. There are plenty of tools out there for you to get a wide variety of assessments. The one that we considered was the Cloud Security Alliance's STAR registry. Here, you can look up a certification or attestation on the cloud service providers. These certifications are done based off of ISO‑type process documents, and the attestation are done based off of service organization control reports or SOC audits.




Cybersecurity/ Infotech

Introduction 

In the early years of computing, security often wasn't much more than having a lock on the data center, asking people to change their passwords once a year. So, yes, things have changed quite a bit. Today the word Information Security and some people use other terms, like InfoSec or Cybersecurity, but for our purposes, we're talking about the same thing. To think about all aspects of computer systems, from the physical hardware, computers, networks, all the devices connected to those networks, not just laptops and phones. But these days, we're also talking about alarm systems and cameras and door locks and thermostats, the different software applications that all of these devices run and, of course, huge amounts of data, intellectual property, customer information, documents, databases, emails. Securing all of this is a critical and ongoing task for every organization with a direct impact on your bottom line and your ability to be agile in the market. So, to be clear, what we're about to cover will not just be a few typical personal security recommendations, like you should have strong passwords or use two‑factor authentication. Yes, that's important, but our focus here is security at the organization level. The different ways this impacts the companies that you work for, the projects that you're a part of, and the teams that you work with. We will begin by taking this vague idea of security and breaking it into more useful, specific areas. Several functions that work together mitigating different risks to your organization and combining to form the suit of armor that protects it. Any gaps in these business functions represent different types of vulnerabilities that open you up to various threats. As we talk about how to close these gaps, we'll go over some security terminology and jargon, and there's a few terms that you've probably heard of already. But there's others that might be new to you, because yes, like everything else in technology, the security landscape is always changing. There are always new kinds of attacks, new vulnerabilities that malicious actors will try to discover and exploit. But whatever your role is, you can be actively involved with this without needing to become a security expert, because over the last few years a lot of work has been done to develop principles and best practices. We have structured repeatable, cost‑effective approaches to plan and think about security. They'll bring our attention to different aspects, make sure that we avoid any blind spots, and in some cases, even just allow us to have meaningful conversations about it. To make us, and the organizations that we work with, better protected and less vulnerable. Welcome to the Executive Briefing on security.

The Threat Landscape 

Okay, nobody will disagree with the idea that security is important, but they will disagree about what that means. They'll even argue about what the most significant security threat actually is. And it's easy to get the wrong idea about this. When you hear about businesses being hacked, those high‑profile attacks that make the evening news, often there's a common theme. ‑The personal data of over 50 million customers was leaked onto the internet after one of the worst hacks in corporate history. ‑Hotel company revealed that this recent attack on its computer systems has compromised the data of 400 million users, including credit card and passport information. ‑The latest cyberattack has exposed the confidential information of over 100 million customers. Were you affected? Tell us how you feel. Call in at 1‑800‑555. ‑It's very easy to see these and think to yourself, clearly, the main focus of security is to stop hackers from stealing our customer data. I have a full understanding of this. But being over focused on one potential threat, or what we call on attack vector, can distract us from the many others that are often far less dramatic, less newsworthy but just as significant. Many security risks are simple, they're mundane, and honestly, kind of boring. For example, a disgruntled employee on their last day accesses an internal server and deletes or corrupts a bunch of internal project planning documents and data. You would not see this making the news. There was no hack, there was no leak, your customers' data was unaffected, but it can be just as impactful to your business. Some vulnerabilities are external, but many are internal. And even if attacks do come from the outside, they won't have the singular focus on getting your customer data. Today's attackers have a wide range of motives that go well beyond that. Some try to make money through ransomware to install software that will effectively lock down a computer and everything on it and only unlocked when you pay them. If this software gets onto your laptop or your desktop, it's for sure a major inconvenience, but if it gets onto the server that hosts your website or holds your corporate database, that's a whole other level of pain. But other attackers aren't looking for money, but instead will take a business offline to make a political point, sometimes called hacktivism. Others don't care about obtaining your customer data. They're looking for your intellectual property to corrupt the data that you use to make decisions or just to gain a back door to your systems so that they can quietly hang around and spy on your operations to gain an advantage. And some attackers don't have any goal at all other than to just generally disrupt a business, any business, because they can. Or worst of all, other malicious actors are looking for ways to leverage cyberattacks to cause physical real‑world harm, destruction of property, or loss of life. And no one is too large or too small to be a target. It's not unusual for an organization to be complacent and assume that if they don't have a big public profile or if they are a smaller business that no one will be interested in targeting their computer systems. But that's not the right perspective because many attacks are basically random and opportunistic. They're the equivalent of a thief walking along the street, quietly testing the door knobs. They didn't set out on a target of one specific house. They're just looking for any vulnerable house to exploit. So we need a way to bring our attention to all of the different areas of focus, the different types of security risks and vulnerabilities. If we're talking about security, what does that mean, and what is it that we're trying to secure? And luckily, there is a simple shortcut, one small word that's been around since the 1970s to help us do this.

The CIA Triad 

For several decades, we've used a simple abbreviation to explain three overall goals of security, CIA. And no, not the US Central Intelligence Agency. Here, CIA stands for confidentiality, integrity, and availability. It sounds almost too simple, doesn't it? Just three words. But we use this to help us evaluate the data and IT functions most critical to the operation and success of an organization or even one specific project within that organization. Let me walk you through what that means. Confidentiality is making sure business systems and data are only accessible to the right entities. And I use the word entities because this can mean people, employees, contractors, business partners. But it can also mean other technology systems. Our HR system needs to talk to their payroll system. Their website needs access to our inventory database. But still, we need to keep our secrets, well, secret and share information we want to share only with those that we intend to share it with. And when an organization is unable to keep personal information confidential, it can cost millions, in some cases hundreds of millions. Integrity is about making sure your systems and data are what they're supposed to be, that we can trust the information is accurate. We can trust everything is current, up to the moment, and that there's no corruption. Nothing's been added or changed or deleted. And in some situations, manipulation of trusted data can cost not just dollars, but lives. Think of computer systems that run power plants or emergency service dispatch or air traffic control. That data has to be accurate all the time and trusted or people's lives are at stake. And availability is about making sure your systems and data are online whenever they're needed, accessible from the right location. Not just ensuring that internal systems are up and running, but also that there's nothing outside interfering with anyone's ability to reach us. As a simple example, if your organization has a customer‑facing website selling products or services, every second that site is not available is money lost. So all three goals are important, and I'll say that again. All three of these goals are important. But they're not equal, and you can find them prioritized in slightly different ways from organization to organization and even project to project because all of them have a financial impact on organizational cost. And improving one aspect does not always help the others. As just one example, if you decide confidentiality is the single most important factor, you might make decisions that downgrade the availability by only allowing access at certain times or restricting connections from some locations. But with these three ideas in mind, we can drive a little deeper into the different aspects of security that make up your overall risk mitigation strategy. But a quick sidebar. While security today is an immense, multifaceted topic, I will say again, you do not have to become a security expert to engage with this. And here's what I mean. If I'm a homeowner and talking with my next door neighbor, we can have a conversation about the importance of a good lock on the front door, and neither of us need to be a locksmith. If we see cracked tiles on the roof, we can understand how it can make it vulnerable. We can recognize drainage issues or see exposed wood as a risk factor for certain pests. And sure, sometimes we may bring in experts, but it's completely possible to understand and recognize common issues, plan for, and even implement solutions without requiring deep technical knowledge. But let's keep this homeowner analogy going a few more seconds. If you've ever done any modest home renovation, you know how quickly your to do list will become overwhelming if you don't start to categorize all the different things that need to be done. You might group these tasks room by room. Here is the list of things that we need to do in the kitchen, and here's the list of things that we need to do in the bathroom. Or you might not go room by room, but instead break it into different aspects. Here's all the electrical stuff that we need to do. Here's the plumbing work that we need, the cosmetic list versus the structural alterations. Here's the ongoing maintenance tasks and so on and so on. And however you choose to categorize tasks, there's always some crossover in dependencies. But still, breaking them into different areas of focus make it easier to think about, plan for, and achieve. And it's the same here. So while there are many different ways that you can organize the elements of security, we're going to break it into six areas, and we'll go over each one to explore what value it brings to the business or, in many cases, what value it protects. We'll begin by looking at the security of various products we build and use, and this includes software applications and infrastructure. We'll then shift our focus to access management, controlling who can get to our systems and what they're allowed to do. After that, we'll cover specific issues for securing our data, an ever‑increasing focus for almost all organizations. That will lead us to governance where we'll also touch on risk management and compliance. Understanding those first four will help us in exploring security operation, which includes secure administration. Finally, is cyber intelligence and testing, a key component of a mature cyber defense and a distinct important element for your overall security plan. So let's get started.

SideBar- Hackers and Actors 

A quick sidebar. In movies and TV, if you have a character who attacks or compromises a computer system, they're usually called a hacker. Nowadays it's universally been morphed into a term to represent a villain, and they're surprisingly easy to recognize. They always seem to wear a hoody. They work alone, sitting in a dark room, furiously typing green text on a black background. The reality, of course, is not so simplistic. Compromising a computer system doesn't have a dress code, and it's rarely about how fast someone can type. Even the word hacker is problematic. It has a long history, and not all of it's negative. Describing something as a hack can mean just a quick and dirty fix, a clever solution, not necessarily an attack. So in the security community, while a hacker is, of course, understood these days, it's more common to use threat actor or a malicious actor instead. It's more clear and more intentional usage. And threat actor or malicious actor can mean an individual, but it can also mean an organized group, whether that's a criminal group or even a state‑sponsored organization called nation‑state actors, so we'll mostly use those terms moving forward. End sidebar.

Product Security 

We'll begin by talking about the area that a lot of people have in mind when the word security comes up. Product security is about making sure any products that you build and/or use, including software, are designed to be as secure as possible. And this focus doesn't just look at products we develop in house, but also the commercial software that you have paid for and implemented and infrastructure and platforms that your software runs on. If you've heard the term application security, which is identifying and fixing security vulnerabilities in your own software applications, that's part of this larger product security story. As you purchase, install, or develop any new system, whether it's part of your own infrastructure, a new software application, or a product that you are prepping for market, it should be securely designed from the outset, following current industry best practices and including security specialists in the process. This is the time that you want to bring in the experts. They're the ones who know what you're up against, and they can help you make sure your new systems have the best possible chance of being secure from the start. And even if your developers have secure coding policies and best practices, we recognize that they aren't static and must continually evolve as we learn of new and emerging trends, vulnerabilities, and exploits. And this means your development teams need to be continually learning and growing their skills as well. This includes your infrastructure teams too. Their skills need to continually evolve to maintain a secure infrastructure over time. So what are these security best practices when it comes to building and using applications and infrastructure? Well, and this is a very common answer to questions about security, it really depends. For example, the best practices to build a more secure on‑premises infrastructure where you own and control all of the hardware are different from those that you'd use when outsourcing parts of it to a cloud provider. And security best practices differ based on the kind of software, web applications, mobile apps and desktop apps that all have their own specific issues and between programming languages as well because there are technical implications of how different languages are implemented on different infrastructure. The point is to be looped into the industry best practices that apply to the specific technology that you are using and then expand on those within your teams to meet your specific needs and to mitigate the specific threats that your organization may face. But that same dedication needs to go to your existing systems. Conduct security reviews and decide what security concerns might exist and how you can mitigate them. If you've had a system running for 5 years, it may have been secure back when it was first implemented, but you need to continually review infrastructure, code, and more to make sure that is prepared to deal with today's threats. You also need to implement policies and practices that help mitigate the risk to older systems, such as controlling who can access them, from where they can be accessed, as well as implementing monitoring capabilities for vulnerabilities that you can't patch because some of these can't be patched in these older systems or they will break the application. And I'll keep coming back to one idea. Security is a continually moving target. It's something that has to be part of the day‑to‑day routine for every part of your technology estate. Whenever you decide to build or implement a new system of some kind, just be aware that you're making a long‑term, permanent commitment to keeping that system secure as the threat landscape evolves and changes. Your organization likely uses enterprise software platforms that you buy or lease from someone else, SAP, Oracle, Salesforce, Workday, you name it. You need to understand how those systems are secured. More importantly, you need to understand what elements of security will be covered by the vendor and what elements of security will not. These days, it's common to see a security responsibility matrix for cloud providers to make it very explicit what they take responsibility for, like the physical data center and the machines themselves, versus what they expect the client to handle, like users and account management. Your team needs to understand how to harden those systems against attack, how to monitor them, just like you would monitor your own assets. And when it comes to products, software, and infrastructure, the thing to remember is that nothing is secure forever. Systems are updated, configured, and developed in real time. And with each change, you are creating the potential for a new vulnerability. There are teams of people, both benevolent and malicious, poking and prodding at hardware and software systems alike, looking for vulnerabilities and creating exploits. I myself am occasionally one of those people, depending on the hat that I'm wearing. So even when nothing is being changed, old systems become vulnerable as new vulnerabilities are found. Part of any organization's core competency needs to be keeping their systems, both the ones that they build and the ones that they buy, capable of dealing with the modern threat environment.

Identity and Access Management 

Identity and access management, often shortened to IAM, is part of security that deals with two simple, but incredibly important questions. First, who are you? And second, what are you allowed to do? We begin with identity. These are the mechanisms that you have to have to define and validate who's who in your organization. These might be a single directory users log into to prove who they are, or it might be multiple different directories that authenticate users for specific purposes. But there's more to identity management than just creating user accounts and making people change their passwords. We recognize that there are different types of identity, ways to represent employees or devices, partners, contractors, or other software systems. But nothing in the business world is static, so beyond the process to create accounts, you also need processes to regularly review all accounts and make sure that they're still needed because a neglected or orphaned account that still has access is a really great way for a malicious actor to gain entry to your systems. Some of these processes can be automated, like automatically removing a contractor whose contract has ended, or they might be part of a larger process, like when an employee leaves the organization, or maybe even something that's manual from doing a periodic review. But there are important aspects beyond just, is this account active or is it not? For example, if we have an employee based in Iowa, but their credentials are suddenly being used to log in at 3 a.m. from halfway across the world, that might be legitimate, but it could indicate that their account has been compromised and someone is attempting to impersonate them. This kind of situation is something that your systems should automatically be raising as an alert and where you would have someone to receive that alert and quickly look into that situation. And that's exactly what security operation centers are for, which is something that we'll get into in a few minutes. But even when you can confidently match real‑world entities like employees to an account and validate their identity, there's the separate issue of maintaining what they should have access to. If we create an account for a new employee and they've successfully logged on, that does not mean that they should instantly be able to change the home web page of our website and delete all of our databases and then access the payroll system and make themselves the CEO. There are multiple levels of permissions and privileges, even when someone's proven their identity. And just like identity has a lifecycle, what the identity has access to also has a lifecycle. If someone needs to use a system, does that always mean forever or until the job role changes? Or do they just need it for a month, or just a day, or maybe just an hour? There's a security guideline called the principle of least privilege that wherever possible, any user account should only have the absolute minimum permissions necessary to complete the current task. And this doesn't just apply to entry‑level employees, in fact, it's more important for advanced administrator roles. Many organizations have a separate set of elevated administrative privileges with access to the most critical assets in the organization, and one bad apple can subvert your entire system, so it's important to closely limit the scope of what an administrator can do at any moment. So smart organizations have processes that remove most capabilities from their administrator's users accounts, and when an admin needs to do something advanced, they use special tools to temporarily request elevated permissions, and when they do that, it will trigger an alert to log that request. In normal use, the security systems will ignore that alert because the action is part of the admin's normal job, it's part of their behavioral baseline. However, if it occurs too frequently over a given period of time, it might indicate a problem that someone should look into. But most of the time, the admin can finish the task they are performing, and their elevated privileges will be automatically downgraded. They're able to do their job, but in a controlled and monitored fashion that keeps everybody safe. If that seems like a lot of work, well, it really is. And that's why many organizations use access management tools to help automate that work. With the right tools and skilled people to implement them, you can create an actively secure environment without creating a lot of manual repetitive work for your teams. Identity and access management are the foundation for everything else that your security plans are built on, and so it's important to take the time to do them right.

Data Security

Today's businesses accumulate an incredible amount of data and not just the usual suspects like customer addresses and order information. We have data gathered from advertising campaigns, customer behavior, and these days, they've been generated by machine learning artificial intelligence systems. Beyond that, there is a company, financial, and operational data, legal data, proprietary engineering designs, and the list goes on and on and on. Whatever you have, it's your most valuable intellectual property. Your competitive edge, hence the security of your data that's paramount. Companies need to have a plan for securing data, and you can break that plan down into three basic elements. We start by securing data at rest wherever it's actually stored and this might be on your own on‑premises servers, but it also might be in a cloud provider, it might be data in an electronic document, or it might be in a database, any of your data, no matter where it is stored and no matter what form that it's in. We think about how this at rest data is protected from accidental or unauthorized disclosure, even protected from physical theft. So first, we need that well‑defined identity and access management that we just talked about earlier, but on top of that, we can also implement encryption so that even if other layers of security fail and the stored data is stolen, the data is encrypted and not usable, and we have effectively mitigated that security threat. This concept of multiple layers of fail safes is described as defense in depth. Next, we think about how the data is protected when it's in transit being communicated from one place to another. Again, various times of encryptions are used to make sure that data in transit isn't seen or modified by anyone else. And finally, one thing people often forget about is the security of data that we've generated or received from someone else or someplace else. Can that data be trusted? Did we receive it in a way that's secure so that we can verify it wasn't tampered with? We are likely making business decisions with that data, so we want to know where it came from and where it has been. Another way to think about these three elements is where is our data? Where did our data come from, and where is our data going? Making sure that you have a plan to provide the right level of security in each case is the key to an effective data security program. Data really is the lifeblood of your company. You can imagine how important it is to make sure that your data is not only protected, but also trustworthy. Smart businesses make all of their big decisions based on their data and information, and you want to ensure that you're operating from a safe, secure place when you do so.


Governance 

Most organizations use the words governance, risk management, and compliance to refer to distinctive activities that each have specific goals and outcomes. In practice, though, they're quite interrelated, and some businesses use terms like GRC or integrated risk management to cover all of them. First, let's talk about governance. This is one of those words that you see used in politics and corporations, land management, healthcare, and it can seem a bit ambiguous and vague. In the context of security, governance describes how we formalize those strategic high‑level responsibilities, policies, and procedures around security efforts. And while there are specific low‑level security techniques and practices your developers and administrators need to focus on, governance describes the higher level decisions to drive your security goals. How do you ensure your overall security approach stays intact and relevant in the hustle and bustle of day‑to‑day business? How do you decide on security priorities and share information across teams? What processes and procedures are in place to ensure that security won't be forgotten in a moment of pressure? Governance also includes understanding the involving threats that your organization faces and developing plans to mitigate them. It even includes the human side of security, such as establishing policies that direct employees, contractors, and others to take the best approaches to staying safe. So now let's look at risk management. This is a big part of any effective security plan, and it's not just about identifying the risks that your company faces, it's about weighing those risks against the cost of mitigating them. Think about it this way. It is easy to achieve complete security, just shut off all your computers, unplug them, and go home. That will keep everything secure, but it won't really make for a very effective business model, will it? So risk management is not about preventing every possible risk. We can't do that. What we can do is evaluate the risk from plausible threats and assess the potential business impact. The result is either a qualitative, or even more detailed quantitative value that serves as the basis for a level of security applied to mitigate the risk, or in some cases, to offset the risk by transferring it to a third‑party like a cyber insurance company or a managed security provider. It's about understanding the risk and managing it to whatever degree the business requires. And for many companies, that takes us into the realm of compliance, which typically means the need to demonstrably conform to some external security requirements, like laws or regulations. These vary significantly by region and business function, but examples include the European Union's General Data Privacy Regulation, or GDPR, or in the U.S., the HIPPA security rules for healthcare information. Your risk management activities will be based in part on the compliance that you're required to meet, and your governance activities will need to take those into account on an everyday basis. That includes not just securing your data, but maintaining that security and being able to report adherence on an ongoing basis as well. And how do you know that you're doing all the right things when it comes to risk management and governance and compliance? One way is to partner with a respected consulting firm. Most of them have created assessment models that let them review an organization and determine if they're seeing the right kinds of activities and processes happening. It's like a second set of eyes filling out a scorecard. They let you know when you're doing the right things, and they let you know when you have room to improve.

Secure Ops 

In this section, what we're calling Security Operations, we'll cover two topics. First, there's secure administration. This is really just your ordinary IT operations team doing their job every day with a firm focus on security at all times. They're managing your servers, administrating your network and other infrastructure, deploying applications, backing up the email server, and everything else that they do every day, all done with the firm awareness on the importance of security in every action. You can't underestimate the role that secure administration plays in your total security picture. Everything from making sure that only the right systems and the right devices can attach to your network to ensuring that those fancy smart light bulbs are updated and secure all falls under secure administration and operations. It's a lot of work, and it goes on every single day. Then there's the dedicated set of activities often handled by a SOC, or security operations center. These individuals are the moment‑to‑moment operators of your dedicated security activities. They're the ones watching your entire environment moment to moment and responding to incidents. They are the...hold on a second. Excuse me. Speaking of incidents, yeah, okay, I think we're good. Looks like our security operation center is already on it. But that's exactly what I'm talking about. We registered some unusual activity, a potential security problem, and our systems are configured to send alerts. But someone has to see those alerts and respond to them, which is what they're doing right now. It could be anything from an administrator suddenly resetting a lot of account passwords or an application seeing an unusual number of logins from an unusual location or, well, really, just anything. And, you know, this is a really good example of how some of the major pieces of security fit together. For a lot of organizations, it starts with activities like software development, one of the places where new technologies and systems come to life. A software dev team focused on security might make sure their app does things like run security self‑tests before it starts and making sure the application is instrumented to send activity events to a central location. The operations team works to ensure the infrastructure is secure, and that might include making sure a central location is available for all those events and alerts to be sent to. And the security operation center receives and responds to those events and alerts, deciding if something is a security problem, and if it is, handling it. You've probably heard of this term, DevOps, which, to use a really short definition, means the developers and operations individuals in the organization working closely together to put applications into production. But that trend is now actually going a step further to DevSecOps, which pulls in the security teams in the loop from the outset. The idea is to create and deploy technologies that are easily secured right from the start, and to make sure both operations and security operations teams have what they need to do their jobs effectively. It's a great way to make sure that you're not only building secure systems but that you're building systems that are designed to be monitored and kept secure over the long haul. But, as well as the day‑to‑day operational procedures, we should also have procedures in place for our response to any security incident. How do you conduct postmortems after an incident? How do you measure the impact of an incident on the business, make sure any fixes applied will work, and ensure the same thing can't happen again? And this leads us into cyber defense.

Cyber Intelligence 

Finally, is cyber intelligence and testing. It's all about how your organization prepares to combat specific threats and what processes you have in place to deploy those defenses when the need arises. It's about your resiliency in the face of attack. Some organizations roll their cyber defense activities into an existing area of their org chart, and that's fine. Others build a special team on the org chart to handle these tasks, and that's fine, too. What's important is to recognize that these activities provide specific and unique value to the businesses and that they need to be things that you do all the time, continuously in conjunction with the other security functions. Let's start with threat intelligence. This is how your business knows on a day‑to‑day basis what you're up against, what's going on in the world, and how it might affect you. Was there something in the news that might cause your specific business to become a target for hackers or for people looking to make a public statement? What new attacks like ransomware or malware are suddenly in circulation? This means keeping an eye on the news on industry reports and more all on the ongoing basis. Penetration testing, or pen testing, is another important activity. This is where people deliberately try to gain access to your systems and data without authorization. They try to find the holes in your security so that you can patch them before a real threat actor gets in. These people might be working for you, or they might be consultants that you hire. They're generally interested in testing your actual technology systems, which is an important part of a strong defense. A step up from pen testing is the idea of red teams and red team operations against blue teams, which tends to be a more holistic activity. The red team role, often performed by trusted outside consultants, pretends to be an actual threat actor. Like a pen tester, they're trying to gain access to your systems, but unlike a pen tester, they'll try to use every dirty trick that the actual adversary would and proceed past your external boundary to have persistent post exploitation operations. This allows for your defense to be tested over a longer period of time and focus on achieving specific outcomes or actions on their objectives, following the full lifecycle of an attack. The idea is to expose the holes wherever they may exist in your organization so that they can be fixed. They may try to use social engineering against the people in your company. Let me show you how that works. Social engineering isn't a technology, quite the opposite. It's low tech, even what some would call no tech. It's using psychological ways to get information from people or to get them to do something. This includes phishing emails or text messages. Even something as basic as just phoning some random employee at the company and saying, ‑Hello? hey, hey, it's Bob from tech support. Yeah, yeah, we're running a security audit. Oh, I know. It's always something from us, right? Hey, look, could you tell me what you're currently using as a password? Uh huh, yeah, uppercase P? Oh, is that your cat's name? Oh, that's adorable. Yeah, yeah, that great. Thank you so much. You're good to go, and thanks again. Have a great day. Yeah, yeah. You, too. You might be surprised at just how often that can work, and a red team will do those kinds of things. Testing not only your technical defenses, but also your processes, and procedures, and people. A red team can work entirely on their own just like an organized adversary would. Sometimes, though, you might want to run scheduled red team versus blue team exercises where your own security staff, they're the blue team, actively respond to the red team, cut off their access, mitigate the red team's threat, and so on. It really becomes kind of a cyber knife fight. It's a great way to test not only your technology, but also your processes, and procedures, and, most importantly, your people. An organization's approach to security, that is, the specific tools that they use, the actions they take, and the processes that they use will always be changing. After a red team test, for example, you may modify your processes or engage in different kinds of security training for your teams based on the results. After an actual incident, you will conduct a postmortem to consider permanent mitigations for whatever permitted the instant to occur, also known as the root cause. You'll engage in threat modeling to test your implementations and create confidence that they're actually effective. It's always an evolving process, anticipating and responding to whatever the world throws at you. Many organizations will appoint a dedicated chief information security officer, or CISO, to oversee these specialized cyber defense efforts, along with all of their other security‑related efforts as well. Some organizations will rely on a trusted consulting partner to provide these services and still others will put these efforts directly under the CEO or another executive. But wherever these functions fit in the org chart, you hopefully can see the importance of cyber defense as part of a complete security plan. And let's be clear, a security plan is a mandatory part of any modern organization. It's a way to provide executive‑level direction that embeds security practices throughout the organization, making security everyone's job.

Summary

We've covered a lot at a high level. We've looked at product security, identity access management, data security, governance, security operations, and cyber defense. As you've seen, there's a lot going on with security in our modern world. Organizations need to understand the risks and implement not only appropriate safeguards, but also implement ongoing processes and organizational investments, as well as executive‑level sponsorship to mitigate those risks. And remember, it's all about CIA, confidentiality, integrity, and availability. Security is a space every bit as complex and robust as finance, legal, human resources, or any other part of the organization. And at this point, you should have a better idea of what security looks like in an organization, how to think about it, and how it supports the business over the long haul. Thank you for joining me.

Introduction to Information Security 

Course overview 

Hello everyone. My name is Keith Watson, and welcome to Introduction to Information Security. This is the first course in Pluralsight's Information Security Big Picture series. I'm an information security professional at Optiv Security, and I've been involved in information security for more than 20 years. If you've paid attention to the news lately, it seems that security has not improved. If anything, it seems to be getting worse. Between the many organizations that have suffered security breaches and the vulnerabilities that continue to be uncovered, we seem to have a significant problem. And as we continue to rely on technology and online services in our daily lives, these security issues threaten our private information, our personal devices, our businesses, the national economy, and our lives. Government and private industry have responded, albeit slowly and reluctantly, to the threats through increased security budgets and hiring. As a result, the demand for information security professionals has increased significantly. Unfortunately, current projections show that open positions for information security professionals will far exceed the number of professionals available. Through this course and the Big Picture series, we hope to prepare you to join this exciting field and help us face these information security challenges. In this course, we're going to introduce most of the key concepts, methods, and management techniques in information security. Some of the major topics that we will cover include an introduction to key security concepts and principles, governance, risk management, and compliance, the protection of assets through security controls, auditing information systems for compliance, monitoring networks for anomalous and malicious activity, managing security incidents and security operations. By the end of this course, you'll know the basics of information security and should be ready to dive in to other courses in the Big Picture series, as well as courses in the Pluralsight Information Security library. I hope you'll join me on this adventure into information security with the Introduction to Information Security, the Big Picture, here, at Pluralsight.

Security Principles, Governance, Risk, and Compliance 

Course Introduction 

Welcome to Pluralsight. My name is Keith Watson, and this is Introduction to Information Security. This course is your introduction to information security, as well as to a series of Pluralsight courses on information security. We like to call these Big Picture courses because they paint a detailed picture of specific information security topics. While they are not in depth technology-specific courses, they provide a high-level overview and a discussion of key concepts that you need to know. Together, these courses will get you started in learning more about information security and improving your career options as an information security professional. In the last 30 years as more organizations moved online, they have been forced to consider security more than they did in the past. They've had to develop an information security program to manage how they collect, store, and share information with employees, customers, and partners. These programs are developed around understanding what the business does and how it does it, which includes the overall business objectives and its willingness to take on some risk to meet those objectives. To move the business forward and manage those risks, protecting the business becomes paramount, which will include the business information and physical assets, the current financial assets and ability to bring in revenue, the reputation of the business, the ability of the business to sustain operations during business or operational challenges, and the regulatory environment in which the business operates. While I use the word business here, these ideas apply to nonprofit and governmental organizations as well. All organizations have assets. These could be anything of value. Organizational assets can include intellectual property, which includes valuable intangible creations. These creations include intellectual goods such as written works, business processes, designs, software, games, music, videos, and many other types. Most of these are legally protected by trademarks, copyrights, patents, or are defined as trade secrets. In today's business environment, there's a greater need for and reliance on information about clients and customers. Organizations have collected more data and analyzed and developed context around it to build a valuable store of information and insights into their customers. Organizations carefully construct recognizable brands, and strive to establish a positive reputation in the marketplace. These are investments that allow organizations to distinguish themselves with their customers and drive sales. Without access to these assets, an organization could struggle to exist. If competitors had access to this information, or could impugn or damage the organization's brand or reputation, an organization could struggle to succeed or even exist. Therefore, protecting these assets should be of paramount importance to any organization. In my own time in the information security field, I've heard a lot of interesting statements about security. There are a particular few fallacies that I want to address here. You may hear something similar in the future, so hopefully this will help you address them. I want this to be 100% secure. This is one of my favorites. There's a belief that security is absolute, that if we do the right things, we can get complete security forever. However, security is not guaranteed. Threats change over time. Controls can be bypassed. We need to think in terms of strong and weak security, not absolute or perfect. We are compliant, so we are secure. I've heard this one after a department or an organization passes an audit and goes through a compliance assessment. Being compliant does not mean that you're secure. Compliance frameworks define a desired state, typically with a prescriptive set of controls. There can be a focus on tackling only the minimum number of controls with the goal of checking the box on a compliance form. Compliance is only part of a complete security story though. With the right security technology in place, we will be secure. This is an attitude that believes that security equals security technologies. However, information security is more about the appropriate balance of administrative, technical, and physical controls than it is a single piece of security technology. Our awesome IR team detected and stopped the bad guy. So it's all good. Hey, congratulations to you and your team. What are you and your team doing about the areas of the network where there is no monitoring in place? How are you improving visibility through the environment to catch other bad actors roaming your network? Our attitude here shouldn't be on the single victory, but the iterative improvement in capabilities. Securities hard to get right, so we shouldn't worry about it. Or, put another way, security is inconvenient or complicated or gets in our way. Sure, it's hard. You'll get no arguments for me on that. Whether it's hard or not, there are bad actors that will take advantage of your lack of security. We need to understand the risks, apply the right level of security, and refine it over time. As an industry, our attitudes towards information security must change. We need to see information security as an investment, and like other investments putting time and attention, training, and money into information security now pays dividends later. This can lead to fewer significant security incidents, lower compliance costs, lower remediation costs, and a more agile organization that can adapt to changes in the business and the threat landscape. Information security is also a business enabler. An adaptive information security program can quickly adjust for new opportunities that the organization needs to pursue to remain competitive. It understands risk and can implement controls quickly to enable those opportunities for success with a manageable amount of risk. Information security is also a responsibility of everyone in the organization, not just the information security staff members. The guy working the loading dock can impact security as much as the executive on the 20th floor. It's critical that we train everyone on that responsibility and equip them with tools to protect the organization's assets. One of the common analogies I've come across in the information security field frequently is this one. We put the brakes on cars so we can go fast. The similarities lie in how the use of security controls can allow organizations to operate successfully in risky environments. It's not a great analogy because security controls are not the beginning or end of security programs. However, I think it sets the stage for what is to come in this course. Our objectives in this course are focused on getting you started on your way to learning more about information security. Again, this is a Big Picture course and that's our approach. We are looking at the Big Picture on information security. We have two related objectives. First, we will explain important principles, domains, and concepts. Our objective here is to increase your understanding of the foundations of information security. Second, we will describe how those foundational concepts apply within organizations. Our objective is to help you understand how organizations build an information security management program. So even if you're new to information security or have been a practitioner for many years, this course has something for you. This course is organized into four modules. The current module will cover information security principles, governance, risk management, and compliance. We start with the core principles of security and then discuss the high-level organizational concepts for managing information security. The module titled protecting and defending assets looks at the use of security controls to safeguard the organization's assets. Auditing, monitoring, and testing focuses on auditing those security controls, monitoring systems, and conducting security testing. Managing incidents and operations discusses the management of security incidents, preparing for disruptions, and operations related to security management. In this module, we'll be discussing some of the foundational topics of information security. We will start with a discussion on information security principles. These are fundamental concepts that are applied to information security. Understanding how to apply these concepts is important in creating effective information security programs, implementing security controls, and designing more secure systems. Governance is a way of defining and distributing the responsibilities, roles, and procedures for making decisions on information security for an organization. With this in place, an organization can define specific security objectives, enable the attainment of those objectives, and evaluate performance. An organization needs to understand the various risks that it will face. Risk management is not only about identifying and understanding those risks, but enabling the organization to control those risks to a reasonable level. Compliance is about adhering to a set of defined guidelines, requirements, or objectives that seek to improve an organization's information security posture. These guidelines may be defined internally by the organization or mandated by government, trade groups, or standards bodies as a way of applying common practices across a variety of organizations. Here we talk about some of the more common compliance frameworks such as HIPAA, PCI DSS, the ISO 27000 series, and the CIS critical controls.

Security Principles 

Security principles are the foundational concepts in information security. There are three core principles and other fundamental ones. You need to understand each of them and how they can be applied. Let's talk more about those principles. The three core security principles, often referred to as the CIA triad or just CIA, are confidentiality, integrity, and availability. Confidentiality is the prevention of unauthorized disclosure of information. Basically, we want to make sure that only specific and intended individuals can view information that they are authorized to see. Confidentiality can be mandated in policy and implemented through access controls, system design, and encryption. Integrity is the prevention of unauthorized modification or destruction of information. Essentially, we want to make sure that information cannot be deleted or modified accidentally or intentionally by individuals that should not be able to do so. Integrity can be found in the implementation of file systems, digital signatures, and file provenance. Availability guarantees access to information and information systems to authorized individuals when they need it. When needed is the key phrase with availability. To ensure available systems, redundancy is often used in the design of the primary systems and the supporting infrastructure. Other key principles include the identification of unique individuals requesting access to information or information systems and the means by which they prove their identity in a verifiable way, the process of granting rights to authenticated users to access specific information or to execute specific functions, the methods used to ensure that the sender and recipient of information cannot later deny that the exchange occurred, and the ability to record the activities of individual users and the periodic review of those activities to ensure compliance with policies or for investigations. There are other principles that you may encounter during your studies in information security. So this should not be considered an exhaustive list. In designing computer systems and applications, there are other core principles that should be considered and utilized to protect information. We will explore more of these principles in a later portion of the course on security architecture and engineering. I want to mention the sources of those here. In 1975 Salter and Schroeder wrote The Protection of Information in Computer Systems, which included a list of security design principles. Starting in 1992, and with several updates later, the Organization for Economic Cooperation and Development, the OECD, published nine high-level guidelines. The National Institute for Standards and Technology, or NIST, has published several documents that include security principles. The special publication 800 series includes 800-14, Generally Accepted Principles and Practices for Securing Information Technology Systems, and 800--27, Engineering Principles for Information Technology Security. The Open Web Application Security Project, or OWASP, has created its own list of Security by Design Principles on its wiki. Most of these principles come from Howard and Le Blanc's book, Writing Secure Code.

Governance 

Organizations are led through governance, which consists of the leadership roles and their assigned responsibilities and the rules and procedures for making organizational decisions. Corporate governance is often defined by a structure consisting of a board of directors, shareholders, managers, committees, and policies. Governance also applies to IT and information security. Let's start with the structure of corporate governance as an example. Corporate governance typically starts with the shareholders. They are the owners of the business, and they seek some amount of transparency in how the business operates for their investment. A board of directors represents the shareholders of the business and provides oversight. Each director is often one of the largest shareholders or someone with a particular specialty that can guide the business. The board exists to set the direction for the organization, hire the appropriate managers, and review the performance of the organization. The board hires or is influential in hiring the senior managers of the organization. This typically includes the CEO, CFO, President, and other senior positions. With the board setting the direction for the organization, it is the managers that lead the way and manage the daily operations of the business. Governance can and should be applied to information technology because when appropriately applied, IT is a business enabler. It's also a significant financial investment for the organization. So IT has to deliver value to the business. IT can optimize manual processes, reduce the amount of staff dedicated to specific tasks, automate key business functions, and improve internal communications. The advantages to using IT the right way is significantly valuable to the growth and stability of the organization. If the wrong strategy or no strategy is set for the organization to take advantage of technology, then the business may suffer with outdated, inefficient, or the completely wrong technologies. If IT resources are not managed well, then the business may lose out on the possible efficiency gains and availability of data and systems. IT risks also have to be managed. In addition to the vulnerabilities, threats, and attacks that you may be familiar with, there are other IT-related risks that also affect the organization. Here are a few examples. An expensive IT upgrade project that is not completed, the lack of qualified system administrators for the number of deployed systems, and the repeated failure of the phone system during peak call times. The organization has to define the right strategy, create appropriate policies and procedures, and then apply security controls in the right way to manage IT risks. Governance of information security involves establishing the program that will protect the business and its information and has the following expected outcomes. The strategy for information security must align with the objectives in the strategy of the business. Information security should be an enabler. To enable the business to reach particular objectives, the security strategy should be crafted specifically to support those business objectives. Just like IT, information security is a significant investment for organizations, and must be guided with the right strategy and managed well to deliver the expected value to the organization. The information security program has to understand and manage risks. We'll talk more about risk management in an upcoming section. Just like other parts of the business, an information security program has attached financial, technological, and human resources that have to be managed well and in accordance with the strategy. Information security programs also need metrics and data that can be used to determine how well the organization performs the information security functions. This data must be analyzed and reported. There are also opportunities to identify deficiencies to correct and optimizations needed. Guidance is a general term used to describe the various documents an organization has that define what needs to be done and how it will be accomplished. Policies are the guiding documents of the organization. They are high-level summaries of corporate philosophy and strategic thinking of management. Policies are generally developed by the board and senior managers and apply to all parts of the organization to ensure consistency. Division and department level policies can be created that are unique to those groups, but they must be in alignment with the top-level policies to avoid conflicting objectives. Procedures are derived from policies and contain more details on implementation. These are typically step-by-step instructions written by middle management in alignment with the top-level policies. While a policy may state that complex passwords must be used for user accounts, a procedure would describe how to implement the password complexity requirements that the system should require. Standards provide the technical requirements for implementation. Organizations create standards to group and simplify the security controls that are needed. Standards can specify the types of security controls needed for a variety of technologies that an organization supports. There can be standards that detail how password complexity controls are applied in the various directory services, network devices, Microsoft Windows operating systems, Linux, and Mac OS systems. Baselines provide descriptions of the security controls that must be applied to entire systems. These are used to ensure consistency throughout the organization. A baseline ensures that a desktop operating system will have the same security controls in place regardless of what IT support group deployed it or manages it. It would describe the required password complexity controls for each operating system. Guidelines are optional controls that allow a user or administrator to make a decision on whether to use those recommended security controls based on the circumstances. Guidelines provide best practices and recommended approaches. A guideline could provide details on enabling password features to prevent password reuse, which may not be specified in a policy but is generally considered a best practice. All of these types of guidance documents should be reviewed periodically. If the policies are written well, they will require the least amount of change. However, the other documents will need more frequent updates to stay current with the organization as it changes over time and to adapt to changes in technology and threats Organizations use oversight as a means to set the direction of specific initiatives and to monitor performance over time. One common way of providing oversight is through committees. These offer a way to provide diversity of thought and input from a variety of business units and stakeholders within the organization. Strategy committees are typically board-level advisory committees that provide strategic direction to the organization. Some organizations have general IT strategy committees with responsibilities for security. Other organizations may use a security-specific strategy committee. These committees can consist of board members, senior management, and specialists that provide advice to the board on a variety of IT and security topics related to current and future needs and issues. Steering committees are executive-level committees that assist the senior management with the implementation of a strategy. These types of committees are more concerned with the initiation and management of IT and security projects, allocating money, defining project goals and objectives, monitoring project progress, and reviewing performance. Audit committees are usually board-level committees that are responsible for financial reporting. For publicly traded companies in the United States, they are required by law and have specific responsibilities. One area of focus for an audit committee is on risk management. Audit committees set the practices used to identify and respond to organizational risks, as well as providing reporting on those risks. Change control boards are typically IT management-level committees consisting of IT and business unit members that oversee changes to information systems. Over time, these systems will likely need to be changed in some way, such as software and hardware upgrades, configuration changes, security control changes, and vendor-driven changes. Any change to a complex information system can introduce instability and downtime to users of the system. Change control boards try to manage those changes to prevent significant unintended downtime that can negatively impact the business units that rely on these systems. There are some specific roles and functions in information security that we need to identify. The Chief Information Security Officer, or CISO, or Chief Security Officer, CSO, is the senior executive in charge of information security for the entire organization. The CISO is responsible for establishing and managing the organization's information security program. Security architecture and engineering are technical individuals involved in developing and managing security controls. Some organizations have security architects that have a broad view of security and work with various business units to design and plan appropriate security controls. Security engineers are typically technical individuals that are responsible for implementing and managing security controls. Security operations are typically teams of technical analysts and investigative staff members that oversee the daily operation of the security program. They monitor threats, vulnerabilities, and security controls, investigate incidents, and may develop intelligence on various threat actors to improve their response. Compliance is responsible for ensuring that the organization meets its various industry mandated or legal requirements. Members of a compliance team may include technicians with a deep understanding of the various compliance frameworks and possibly individuals with a legal background. Internal audit typically consists of auditors focused on evaluating and reporting on the organization's management of resources. In addition to the traditional role of reviewing financial records and business processes, internal audit teams look at an organization's approach to risk management, implementation of controls, and governance processes.

Risk Management 

Organizations need to understand risks to the business and reduce them to manageable levels. Let's talk a little more about risk management. Risk management for organizations involves operating with the understanding that there is the possibility that future events may cause harm to the business. The key message of risk management is that organizations can never eliminate risk entirely, but they can reduce it through appropriate management. To do this, organizations have to establish a risk context or an environment in which decisions are made based on risk. The goal is to create a risk management strategy that defines the organization's approach to understanding risk and making decisions about how to manage it. Assessing risk is needed to know what the risks are and how the organization has planned for them. The assessment process looks at weaknesses, threats, and any security controls in place to mitigate those threats, as well as the potential impact to the organization. The result is an understanding of risk. Once the risks are known, the organization has to respond. These can be actions that include evaluating, developing, and implementing options for response to reduce or limit the risk. The organization will change over time. Those changes to the business environment and the potential threat landscape mean that the approach taken to address specific risks will have to change. Monitoring risk over time includes periodic review of the approaches taken to address risk and determining the effectiveness of those approaches. There are some basic principles in risk management. Organizations are guided by these principles in their risk management programs. We will be specifically talking about IT and information security here, but these principles apply to all aspects of business and risk. Risk avoidance is avoiding activities that carry some risk. For example, if an organization knows that placing an outdated and potentially vulnerable information system on their perimeter network increases the likelihood of a breach, then the organization is better off if it decides not to use that system, or at least not place it on the perimeter network. Risk transference or sharing risk is often linked with insurance. But that's only part of the picture. The legal responsibility is not transferred because insurance only provides compensation for losses. Outsourcing your IT infrastructure to a cloud provider is one method of sharing risk in that your organization shares the responsibility for the security of the technology stack with the cloud provider. Risk mitigation or risk reduction involves minimizing risks through the use of countermeasures and security controls. Organizations can deploy security and monitoring tools, configure systems using security guidance, and train staff as ways to reduce their exposure to different risks. Risk acceptance or risk retention means that the organization is willing to accept the loss, should it occur. There may be some risks where the cost of reducing or transferring the risk is high and the likelihood that it would occur is low. The organization may have an old static website with non-sensitive information. They may decide to continue to operate the site knowing that it may be breached in the future from a currently unknown vulnerability. These principles describe the steps needed for organizations to properly manage their risks. In order to apply the appropriate treatments for risk, an organization has to know what their risk is. This is accomplished through process of risk assessment. First, they have to identify vulnerabilities. These are weaknesses that a threat might exploit. Then identify threats, which are events in which there is an adverse or negative impact to an organization. Determine the likelihood of occurrence that a threat can take advantage of a vulnerability, which can be based on intent and capability of that threat. And the negative impact to the organization has to be determined should the threat event occur. Then using likelihood and impact information, risk can be determined, which must be reported to the organization so that decisions on addressing the risk can be explored through the selection of controls. For a good overview of the risk assessment process, take a look at the NIST Special Publication 800-30.

Compliance 

Organizations need to manage their security programs against specific guidelines. Which guidelines they use, how they manage to those guidelines, and how they identify and correct gaps in the coverage of those guidelines are all part of an organization's compliance program. Let's look at how organizations manage compliance. Control frameworks are the guidelines that organizations use for compliance purposes. These frameworks help organizations by ordering and grouping various security controls. Each framework may have a different focus and group controls differently toward that purpose. However, most are based on a common set of security controls. Depending on an organization's current security program, control frameworks can be used to design a new security program or refine an existing one. For organizations that have not invested in security or haven't had a specific strategy around security, control frameworks can be helpful in designing a security program. Control frameworks can also be used to audit an organization's security program. Since frameworks define a series of required security controls, they can be used as a basis for determining if an organization has implemented those controls. An organization may need to follow specific control frameworks for several reasons. These reasons might include legislative or regulatory purposes. Some organizations are required by law or through government regulation to have specific security controls in place to protect the public in general or some specific groups such as investors or healthcare patients. Industry-specific controls might be required. These are typically mandated by industry, governing partners, or trade groups. Industries that are at a higher risk for loss or hold key portions of a nation's infrastructure are often required to have security controls in place. When doing business with other business partners or even government agencies, there may be a contractual requirement for some specific security controls or control frameworks that are required due to the critical nature of the business conducted or sensitive nature of the information exchanged between the two entities. In order to build or maintain a competitive advantage, organizations may choose to implement and follow specific security control frameworks. Compliance with a specific control framework demonstrates commitment to information security and may assure clients concerned about how the organization will protect their information. Other organizations recognize the advantages to having a standard or consistent application of security controls across the entire organization. They may choose a control framework that best matches their business objectives and apply that as a set of best practices. They may choose to implement key parts of their chosen framework or a majority of the controls as needed. Once an organization has a controls framework to follow, they need to periodically review how well they comply with that framework. While it's one thing for an organization to state that they follow and are in compliance with the framework and at specified controls, it's another to be able to prove that internally and to other entities. This is where auditing for compliance with the controls framework becomes necessary. There are several outcomes from auditing that can assist an organization in achieving that compliance. First, they need to determine the level of compliance to the framework that they have reached. Auditing can identify the overall compliance against the frameworks controls. Next, organizations need to identify gaps in their control implementation. There may be missing controls that were never implemented in the first place. There could be controls that are partially implemented. Controls can also be implemented incorrectly. Finally, organizations need to know whether the controls they have in place are effective. An audit opinion may provide some level of assurance on the effectiveness of those controls and whether the controls aid the organization in achieving specific business and security objectives. Auditing can be an internal exercise or one provided by an external auditor. In some cases, external auditors are required depending on the regulations, industry requirements, or contractual requirements. External auditors also provide the organizational independence needed for an objective audit opinion not unduly influenced by the organization as internal auditors can be. There are several key regulations that specify security controls for organizations. Here are a few key ones to know. The Health Insurance Portability and Accountability Act of 1996, or HIPAA, mandates protection for patient health information in physical and electronic forms. There are two major parts to this law--the privacy rule and the security rule. Most security professionals are concerned with the security rule because it deals with electronic patient health information, and it specifies administrative, technical, physical, and documentation controls that must be implemented. The Sarbanes-Oxley act of 2002, or SOX, was enacted to improve financial reporting and disclosures for publicly traded companies in the United States after several large companies were involved in fraudulent accounting and reporting. The key portions of this law for security professionals are sections 302, Disclosure Controls, and 404, Assessment of Internal Controls. Controls are required to be in place to ensure the integrity of financial data, and the internal controls have to be reviewed to determine their effectiveness. The General Data Privacy Regulation, or GDPR, is a European law to protect the private data of EU citizens. It empowers citizens to have more control over their own private data with a series of defined privacy rights. This law applies to any organization that collects the personal data of EU citizens, regardless of the company's location. Internal security controls are needed to protect the personal data, as well as enable the rights of access, erasure, and portability. There are industry-specific compliance frameworks as well. These apply to specific types of business and their transactions. Some of the more well-known ones include the Payment Card Industry Data Security Standard, or PCI DSS, or just PCI. Service providers and merchants have to protect cardholder data and demonstrate PCI compliance. In addition to the implementation of specific security controls, PCI entities are required to review their security controls and report their compliance to their acquiring banks. The Federal Financial Institutions Examination Council, or FFIEC, has many standards for supervision of the banking industry. The council has created standards for information security and conducting examinations and audits. For IT systems, there is the Information Technology Examination Handbook known as the IT handbook and the Information Security Booklet, which is part of the IT handbook. The North American Electric Reliability Corporation Critical Infrastructure Protection Standards, commonly known as NERC CIP, provides standards for the reliability and security of the bulk power system in Canada, the US, and parts of Mexico. The CIP standards include cybersecurity and physical controls for key portions of the information systems that manage the power system. There are several standards for building or auditing information security programs. The International Standards Organization and the International Electrotechnical Commission have the 27000 series. These are a series of standards for best practices around creating an information security management system. There are several standards documents in the series to define the management system and various implementation techniques. The National Institute for Standards and Technology, or NIST, is a US Federal Government agency that defines various standards for the Federal Government. The Special Publication 800 series is focused on information security. While the standards are required for federal agencies, these standards may be required for contracts with the Federal Government. Most of the standards are generic enough for implementation by private organizations too. The Center for Internet Security has a list of 20 basic controls currently known as the CIS controls. The controls are designed to provide the basic actions needed to counter the most common threats and increase the use of automation to limit human error. A consensus process with a significant number of contributors is used to collect ideas and develop the best set of controls.

Module Summary

So that was a quick introduction to some of the foundational concepts in information security. Now let's do a quick review and look at some related courses. In this module, we focused on those core principles and high-level concepts that define organizational approaches to information security. We started with the core concepts of confidentiality, integrity, and availability, the CIA triad. Then we spent some time on other security principles of identity, authentication, authorization, non-repudiation, accountability, and auditing. All information security programs should be built on these guiding principles. In our discussion on governance, we talked about the high-level organizational stakeholders and purposes of corporate IT and information security governance. We also explored some of the key components and guidance, as well as the methods of oversight. Organizations that establish the appropriate guidance structures are more likely to have effective security programs. For risk management, we focused on how organizations assess, respond to, and monitor risk. In responding to risk, organizations can choose to avoid, transfer, mitigate, and accept risks. We followed that up with a discussion on the risk assessment process. Organizations that understand their own risks make better decisions in managing those risks. In compliance, we spoke about the needs that organizations have in selection, implementation, and assessing their compliance with specific control frameworks as required by law, industry, contract, or best practice. We highlighted some specific controls frameworks such as HIPAA, PCI, and the ISO 27000 series. Organizations have to meet their compliance obligations, whether those were internally chosen or mandated in this. In this module, we introduced some of the concepts that are explored in more detail in other Big Picture courses. Here are the other courses that you may want to tackle next. Security Management looks at the security of organizations and their strategies, principles, and risk management approaches. ISO/IEC 27001 Information Security: The Big Picture covers the international standard on information security management systems. PCI DSS: The Big Picture dives into the details of the payment card industry's Data Security Standard. In the next module, we will discuss what assets are and how organizations protect them using security controls.

Protecting and defending assets 

Welcome back. Organizations are basically a collection of resources from which value is derived. When properly applied, those resources or assets can be used to promote the growth of the organization in its pursuit of goals. So it's crucial that the organization protect and defend those assets. In this module, we will focus on three key ideas. First, the protection of assets through the application of controls. We will also define assets more clearly. Second, we will look at how organizations design and build various controls through security architecture and engineering processes. Finally, will look at the use of security operations centers and how they can be used to coordinate people and technology along with define processes as part of an active and effective information security program. There are two types of organizational assets at a high level. Both types have value to an organization. Tangible assets are those that are physical in nature or exist in a physical form. Intangible assets do not have physical form. Let's look at some examples of both and how organizations apply them. As I said, tangible assets are physical forms. Buildings are a great example and perhaps the most obvious tangible asset an organization has. Buildings are where human assets are assembled to conduct the business of the organization. Manufacturing facilities are tangible assets where an organization brings together human resources, machinery, automation, raw materials, and components to assemble the products. Data centers are spaces that contain centralized computing, storage, and network equipment-- tangible IT systems. Distribution systems are vehicles, equipment, and workers used to transfer goods from one location to another. They are often used to move completed goods from the manufacturing facility to the stores that sell them or to other distribution centers. Inventory can be the physical goods that the organization has created for sale, or it can be the components and/or raw materials used to create those goods. Transportation includes the physical vehicles used to transfer people or goods from one location to another. Depending on the type of vehicle, think airplanes versus bicycles, the overall value and technology integration can be significantly different. In each of these examples of tangible assets, the organization has an investment of significant financial resources in order to purchase, maintain, and operate the assets. The loss or disruption to one of these assets can result in difficulty to continued business operations. Intangible assets as opposed to the physical assets we discussed do not exist as physical objects. They are the ideas, concepts, designs, written works, and other forms of human ingenuity and creativity. Some of these works are or can be protected by legal protections such as patents, trademarks, and copyrights. Here are some examples of intangible assets. Intellectual property is typically a generic term applied to intangible assets from which an organization can derive value through the creation of products and services. These are typically protected legally with patents and copyrights. Strategy is anything related to how the organization plans to conduct future business. The specific details of strategy is information that is typically not shared publicly. Financial data is detailed information about the company's performance in the marketplace and the metrics related to those results. Business processes are the step-by-step procedures to conduct internal business operations. These might include processing orders, managing inventory levels, chemical processes, developing software, or any number of internally developed methods. Some of the processes may be considered trade secrets. Customer data is any data gathered about an organization's customers or potential customers. As organizations have taken advantage of the internet and built relationships directly with their customers, they have collected more data about them. Internal data are any other types and forms of data that an organization uses to operate. These can be employee data files, human resource information, facilities information, or anything that may not have a direct impact on the company's business but are needed to keep the organization moving forward. All of these intangible assets are essential to the success of the organization. If any of these assets were to be disclosed publicly, the organization could suffer some harm. There are threats to assets that can cause operational issues and challenges for organizations. Consider for a moment how well an organization might conduct normal operations if it were to suffer the loss of information, inventories, distribution, or transportation systems, buildings, or some other damage or destruction that prevented the continued use of an asset. That would cause significant challenges to an organization, especially if the assets cannot be restored or recovered. Disruptions could prevent an organization from normal operations. Datacenters with no network connectivity, manufacturing facilities with no access to inventory or distribution systems, building without working environmental controls, workers with no access to their electronic files, or any other type of disruption could limit an organization's ability to operate normally. Exposure of sensitive or business-critical information assets can limit an organization's ability to compete in the marketplace. The disclosure of information might lead to legal action, loss of legal protections, or inability to offer viable products and services. These types of threats can prevent organizations from conducting normal operations, limit business success, or bring about the end of the organization. In order to prevent threats from affecting assets, an organization can protect those assets. There are three basic objectives in that protection. First, an organization can prevent the threat action. It can detect when the threat action occurs. It can respond once the threat action has occurred. You can think of these objectives as acting before, during, and after the threat action. The asset protection objectives are put into effect through the design and implementation of security controls. You may hear the term internal controls. They are similar in purpose and may be applied to financial information or internal processes. Security controls are safeguards or countermeasures, which can be a procedural, logical, or physical means of minimizing, detecting, avoiding, or reacting to identified security risks. We defined a risk earlier as the negative impact of a threat and its likelihood of exercising a vulnerability. When we look at where on a timeline a security control can effect a threat event, it can be before the event occurs in that it prevents specific actions or activities from occurring. It could be during an event when specific activities are noticed or side effects are recognized. And it can be after the event has occurred, and the results of the event are observed or actions are taken to correct the effects from the event. In formal terms, there are three specific security control categories. Detective controls identify key activities and distinguish unique characteristics about the event as it happens. Preventive controls disrupt or stop specific actions before the event would have occurred. Corrective controls limit the effect specific actions have once the event has occurred. There are three security control types. Administrative controls are procedural or process-oriented security controls. These types are focused on the human either as an instrument of the control for oversight review or for following a defined set of steps, or as means to determine the next actions required. Physical controls are tangible objects or mechanisms that serve as barriers, deterrence, or devices used during a threat event. Technical controls are logical tools that can limit access, prevent specific actions, detect and analyze activities, or correct or restore normal operations. Now that we know more details about security controls, let's look at some examples that should help you identify them by type and category. And in some cases, a security control is contained in multiple categories. Remember, we started this discussion around assets, so think about the asset and how it is protected in these examples. Background checks are used to review a person's past and to make decisions about them that can affect their future. Sounds ominous, right? Organizations are trying to decide if a particular individual can be trusted with access to sensitive information or placed in a position of authority. One technique that helps in that determination is to find out if the person actually has the education or certifications that they claim to have, or that they haven't been convicted of a crime. Think about this security control. It's administrative because it involves investigation and review of a human. It's preventive when used in the hiring process because the organization is denying employment to individuals that cannot pass the review. It can also be detective when used for current employees. Antivirus software is used on computer systems to find and remove malware. It's a technical security control since it operates on the computer systems. It is preventive in that it detects known malware before it can be installed or executed. It is also corrective in that it can remove some malware after it's been installed or while it's executing. A security guard can provide a physical presence in a location to prevent specific actions and serve as a deterrent. A guard can also detect anomalous or malicious activity or respond to an event that has happened. Here are a few more examples of security controls. User roles in software provided separation of duties between different users and limit the actions a specific role can take. For example, you may have a standard user role that provides most of the functionality of the software, except for the ability to create new accounts. That functionality is limited to an administrator role, which can create and assign those new accounts. This is a technical control since it is integrated into the software. It is preventive in that it limits functionality to the role capabilities assigned to the specific user preventing access to key higher-level functions. Door access controls are entry controls that can restrict access to the interior space of the building or to specific areas inside the building. There are variety of these controls from pin pads and badge readers to hand geometry scanners to man traps. They all provide the same function even though they require different methods for valid access. These controls are physical and preventive in that they limit access to authorized individuals. Mandatory training is required training for all or specific members of an organization's staff. For example, training on security policy might be mandatory for all staff members, whereas PCI or HIPAA training might be mandatory for only the staff members that deal with those types of information. These are administrative controls that attempt to discourage bad behavior and encourage desirable actions for the staff. Hopefully these examples of security controls demonstrate how they can be grouped and categorized as part of an information security program. When we talk about intangible assets and an organization's sensitive and critical information, we need to talk about information management. While this can be a wide ranging discussion, I will only introduce it in this course. Organizations have different types of information, and each type may have a different value to the organization. For example, financial performance information may have a higher value than shipping invoice data. Both are important, but the integrity, availability, and confidentiality of the financial data is valued more than the integrity and availability of the shipping invoice data. Organizations need to create a classification system to identify the various types of data, assign a level of sensitivity to each, and define the required security controls needed to protect each level. You may have heard the terms top-secret or secret used to describe sensitive government information. These are the levels of information sensitivity. Organizations may use restricted, internal use only, or public to describe their sensitivity levels, to label specific sets of information, and to apply security controls. Once the classification scheme is developed, organizations may need to explore their collections of data, looking for those different types of information, and ensuring that it is labeled and stored on the right systems with the appropriate security controls in place. This may include manual system reviews or the use of data discovery tools to locate the information on large file systems or databases. After the information is more clearly defined and labeled, organizations can use preventive and detective security controls on computer systems and networks to avoid or limit the loss of sensitive information. Users can be blocked from sending sensitive data in email or file transfers to people outside the organization. Automated file system searches can locate sensitive data that is not located on the appropriate server or network. Border network devices can examine network traffic looking for sensitive data and blocking it before it leaves the organization's network. Information management deals with information in all its form, electronic and physical, as well as its use in business processes across the organization. The security controls needed can involve administrative, technical, and physical.

Security Architecture and Engineering 

Now that we understand more about security controls, we will discuss the overall design and implementation of those controls through security architecture and engineering. Here is my definition of security architecture. It is a process of selecting security controls that meet both objectives of the business and the security objectives of the organization. Security engineering can be defined as taking the security control plans, implementing the security controls, and monitoring their effectiveness. There are some specific design principles that should be considered in designing systems, software, and security controls. In 1975 Saltzer and Schroeder described design principles in a paper titled, The Protection of Information in Computer Systems. It included the principles which I'll describe briefly here. I would encourage you to read the whole paper for more details. Economy of mechanism means design protections that are small and simple. The challenge with complex protection systems is that there are likely implementation errors and vulnerabilities. Remember, complexity is the enemy of security. Fail-safe defaults is the idea of creating a base configuration in which the default action is to deny access. The assumption is that security protections somewhere along the line may fail. Having the system access restricted by default limits the impact of protection failures. Complete mediation is the verification of access and authority for all objects and actions. This is a system-wide approach for checking the source of all requests. Open design is the idea that the design of any protection system should be open for review and scrutiny. Systems built on secrets lack the critical outsider perspective that can identify flaws and vulnerabilities. We should always be cautious of crypto algorithms, security products, and blackbox systems in which the designers do not share the details of the design. Separation of privilege is the division of responsibilities among the authorized users of the system. It prevents a single user from executing a critical action without the oversight or review of someone else. Whether the action is accidental or intentional, there is another user that can prevent it from proceeding. Least privilege is limiting a user to the smallest set of privileges needed for their job. This prevents users from having too many privileges on the system and limits the damage from errors, accidents, and intentional actions. Least common mechanism is related to shared functions that are depended upon by all users. The better design choice is to avoid supervisory functions in favor of common library functions. Psychological acceptability is related to the design with consideration for the humans that use the system. There is a need for ease-of-use so that protections are applied routinely, consistently, and automatically without hesitation. The design must also consider how to handle human error. Defense in depth is actually not part of the principles from Saltzer and Schroeder, but it's a common one to which most professionals are aware. It's the idea of building multiple layers of controls and defenses. If one layer of protection fails or is bypassed, there are additional layers that can prevent attacker success or raise the likelihood of detection. Here are some resources for design principles that you should explore. There is the Saltzer and Schroeder design principles paper, the US CERT site has a modern list of design principles, some of which are based on the Saltzer and Schroeder ones. The Open Web Application Security Project, or OWASP, has a list of design principles as well. The process of designing security controls should ideally occur early in the IT service design stage. It's at this stage where the security controls can be determined and integrated into the architecture of the service. However, you will often find that security is an afterthought, and that the security team is engaged late in the process. It's still possible to protect IT assets and information with security control design late in the process. There may be issues with the completeness of the control coverage or implementation and integration issues though. The first consideration should be on understanding the business objectives of the IT service. This helps ensure that any conflicts with IT or security strategy are addressed and that the security controls do not add any significant limitations to the service provided. At this point, alternative controls can be considered if the standard set of controls are not suitable. The security policy and relevant procedures and standards must be considered to ensure that the security controls provide the required protection and that the implementation is consistent. The security control selected should be cost-effective. Using the existing security infrastructure and controls should be the preferred approach whenever possible. Adding in new, untried security controls or building new security infrastructure adds cost perhaps unnecessarily. Some of the cost could include new product licensing, hardware, training, integration with other security systems, and internal chargebacks. Choosing security controls that are simple is also preferred. By reducing the complexity of the protection provided, the security controls in place will be easier to understand and explain. There are fewer opportunities for errors in implementation and management. Simpler controls are also easier to test an audit. While they should be simple, the protection provided should be adequate for the identified risks. Cryptography is a large area of information security practice that provides secure communications in the presence of adversaries. It is a much larger topic than we will cover here since it can involve a discussion on mathematical theory, computer science, cryptographic algorithms and protocols, key links, and implementation challenges. We have another Big Picture course that covers that level of detail. Let's focus on the high-level uses in security controls. The first purpose is the exchange of sensitive information. Cryptography can allow two or more parties to share information. Adversaries that capture the data exchanged cannot read it without knowledge of the private keys. Organizations rely on cryptography to exchange information with their employees, clients, and customers through tools like secure TLS enabled websites and the SSH protocol. Cryptography can be used to prevent the unauthorized modification of information. This use is helpful in information exchange to ensure the message sent matches the one received. It can also be useful in detecting data that had been modified after it was created. Cryptography can help improving the identities of various entities. Public key cryptosystems are one tool in proving identities. Each user creates or is issued a unique crypto key, which, through its use in key exchanges and digital signatures, proves their identity. TLS protected websites have public key certificates that have been digitally signed by trusted third parties to prove their origin and identity. Security controls can be applied across the network based on the security objectives of the organization. Network security controls need to continually change to adapt to new threats and to keep up with changes in network infrastructure. As a key metric for consideration, we can expect that bandwidth needs will continue to grow based on the reliance of modern organizations on networked devices, cloud services, and Software as a Service. Network security devices haven't always kept pace with the expanding bandwidth needs of the organization. One key component in network infrastructure is the use of an isolated network segment for external internet connections to reach the organization's public network services such as their website, email server, DNS servers, and other public-facing network services. DMZs are designed to limit external connections to carefully configured and closely monitored services and servers. If one or more of the DMZ servers are compromised, the attackers are limited in their access to organizational data and internal servers. The monitoring in the DMZ should be sufficient to detect attacks and attacker progress quickly. Network intrusion detection and intrusion prevention systems provide network awareness of malicious and potentially anomalous activity on the networks. Intrusion detection systems are passive in that they monitor the network and raise alerts when suspicious or malicious activities occur. Intrusion prevention systems are active in that they affect network traffic when malicious activities occur, in addition to raising alerts. Firewalls and originally routers with access control lists in place specifically block bad network traffic based on its source, destination, and network port. Modern firewall technology is advanced in capabilities for stateful inspection and deep packet inspection. Next-generation firewalls are more application aware and have features of intrusion prevention systems. As more business is conducted using external web resources through Software as a Service, social media, and external web APIs, attackers have been using more of those web resources as an attack vector. Web proxy servers and more modern cloud access security brokers, or CASBs, can inspect web traffic and block some services and untrustworthy sites. With the various attack vectors on the network and the multiple devices in place, organizations need a centralized way of managing events for review and action. A security information and event management system, or SIEM, collects log and event data from multiple sources and presents that information to analysts and can initiate response actions automatically. Along with the SIEM is the need to manage log data from a variety of devices. The log data can be fed directly or selectively into the SIEM, or the log data can be collected in mass through a log management system and then fed into the SIEM for analysis. Log data is also helpful for reviewing events during an investigation and for the creation of new rules for the various network security devices we discussed here. An endpoint is often the term used to describe various computing devices. Endpoints can include desktops, laptops, mobile devices, and servers. Security controls for endpoint devices are generally the same across the different devices but may have varying levels of configurability. These controls should be based on the intended use of the device and the risks associated with it. Antivirus and anti-malware are common tools with which even the most non-technical users are familiar. The first antivirus products for PCs became available in the late 1980s. The technology in antivirus has changed very little over the decades, but the number of virus and malware variance has exploded. In enterprise environments, centralized management and the deployment of special malware signatures on demand are key features. Storage encryption is one control used to protect data at rest on hard drives, solid-state drives, and other forms of storage media. The primary threat here is the loss of a portable device. Encryption protects the contents from disclosure. Application white listing is a security control with two primary purposes. One is limiting the user to authorized software. The other is to prevent the introduction of malicious software to a system. When you contrast this with antivirus controls, which allow all activities except those known to be bad, white listing allows only specific activities and blocks everything else. Data loss prevention controls prevent the accidental or intentional disclosure of sensitive and classified data. As we discussed previously, organizations with information classification schemes can enforce the required protections on the endpoints with data loss prevention tools that can cover actions in email, file transfers to removable media, and posting information online. Intrusion detection and response tools are security controls that aid in both detecting intrusions on the endpoints and responding to intrusions in real time. All of these security controls can be used to protect server, desktop, and mobile endpoints and should be used in conjunction with appropriate network security controls based on the security objectives of the organization. Configuration guides are documents that describe various configuration options for hardware, operating systems, and applications that improve the operational and security readiness of systems deployed in a hostile environment. Hardening guides provide specific instructions on improving the security of systems over their out-of-the-box configuration. NIST has the National Checklist Program, which is a repository of hardening guides. Some are published by government agencies such as the Defense Information Systems Agency and the National Security Agency. Some hardening guides are published by the vendors themselves and are typically supported configurations. The Center for Internet Security publishes security benchmarks, which are hardening guides created by the consensus of independent security professionals. The traditional systems or software development lifecycle was focused on repeatable and consistent practices in design, creation, and deployment of a complex system. Unfortunately, the security of the overall system may not be considered until the system is deployed or about to be deployed. Security issues discovered later in the lifecycle are generally more costly to correct, and some issues may require the redesign of significant parts of the system. Due to schedule pressure, some systems may be deployed with known security flaws. Hopefully it's obvious that the risks of delaying security practices until the end of the system lifecycle are significant. When security practices are built into the SDLC processes, then security is considered during the requirements and design phases. The system has the right foundation to operate in hostile environments with security constraints identified, threat models developed, and security controls integrated into the system design. Practices during the development phase include secure coding practices to avoid common programming errors that lead to security flaws, and manual and automated code review to catch errors in the entire code base. In the testing phase, manual application penetration testing can be used to find vulnerabilities, and prototype environments can be documented for secure deployment of systems. Of course, there are several ways that security can be built into an organization's SDLC. The Building Security in Maturity Model, or BSIMM, and the Open Software Assurance Maturity Model, or OpenSAMM, are two great resources. In addition to the IT-related security controls we've discussed, an organization's human and physical assets need protection. There are physical security controls that can be used in an integrated manner that involve the people employed by the organization, including all the staff members, as well as those tasks with physical protection. Procedures designed to ensure that equipment, material, and visitors entering and leaving the facility are verified, logged, and monitored, and that specific actions are taken during an incident. And use of specific kinds of equipment and construction techniques to provide facility protection. The integrated approach to physical security provides what is known as a physical protection system. A properly designed and integrated physical protection system can detect malicious and suspicious activity by an attacker, delay the attacker's progress, and allowing the response to stop or disrupt the attacker. Here are some types of physical security controls that can be used to protect physical access. Entry controls allow authorized individuals access to a facility or spaces inside a facility. For example, entry doors can be unlocked when a user enters a pin, has their badge read, or provides some biometric. Sensors and alarms provide monitoring and alerting. Sensors can detect a variety of conditions or system states or provide visibility. Alarms use data from the sensors to alert personnel when there is a violation of a defined baseline or activity. Reception areas are locations in a facility where visitors enter and are greeted and processed by staff to determine the validity of their presence there, and to contact the appropriate staff to manage the visitors. These places are typically building lobbies with trained staff members to follow a set of procedures. Reception areas can also be used to protect internal spaces such as datacenters where contractors, vendors, and some staff members need to enter that space. Physical barriers serve to deter, delay, or trap attackers. Fences, lighting, walls, berms, locked doors, checkpoints, vehicle bollards, and other physical mechanisms can be used to prevent attackers from approaching and entering facilities. Guards provide some deterrence for attackers in an active response during incidents. Visitors that are escorted while on the premises are logged and monitored during their entire time at the facility. In our discussion so far, we've touched on two security control types--technical and physical. Administrative controls also serve a purpose in protecting an organization's assets. Just as technical and physical controls have to be designed and implemented effectively, the same applies to administrative controls. This is a human factor type of information security control. Some examples of administrative controls include policies, procedures, guidelines, and standards, which are the documented security guidance that start with the high-level requirements, provide step-by-step instructions for specific actions that must occur, and describe technical implementations. Hiring and termination procedures are needed to ensure that the appropriate people are brought into the organization with the position of trust and that should they leave or be asked to leave, their departure does not add risk. Training and awareness is needed to educate and periodically remind staff members of their responsibilities for information protection. This can include onboarding procedures and new hire training to inform staff members of their information security responsibilities and job-specific training on security techniques and practices. Oversight and review are methods to verify that specific actions are executed as defined by written procedures. Oversight is used before an action can proceed further. Review can be used after the action has been completed to check compliance with those procedures. Audits are often used as a way to determine compliance with policies and procedures and their effectiveness.

Security Operation Center 

Now that we've looked at the architecture and engineering involved in security controls, let's look at how some of those controls are used in a centralized approach known as the Security Operations Center, or SOC. My definition of the Security Operations Center is the centralized application of people, process, and technology used to manage information security operations. Similar to a lot of what we talk about in information security, there is that pivotal role that people, process, and technology play in any successful program. A SOC is no exception. In fact, it is probably even more critical to an organization's ability to adequately manage its security defenses than any other aspects in information security program. Let's explore a little more. Here are some key pieces that a SOC should have. The SOC needs a defined and declared mission. This helps define its purpose for the organization instead of a place where all the security magic happens. Mission focus is also helpful in honing the attention of SOC staff members and for teambuilding. Executive level support is a requirement for a SOC to be successful in terms of budget, headcounts, and physical space. Having the right people and the right processes in place is more valuable than having the right technology. A SOC needs to have smart people doing the right things. The technology used in the SOC will change over time. The organization needs to realize that protections will fail. The SOC should prioritize detection capabilities over protection. Additionally, knowing about an incident but not being able to do anything isn't helpful. So a SOC needs to have the right response capabilities and support. While the mission of the SOC can be more specifically and formally defined by the organization, there are two basic objectives for a Security Operations Center. The SOC protects the information assets of the organization. The policy of the organization defines the needs for information protection, and the SOC is an instrument of that protection. The SOC also ensures the continued operations of the business. Security incidents and disruptions can negatively impact the organization's ability to conduct business. The SOC monitors for and works quickly to resolve these issues preventing or minimizing the impacts to business operations. The SOC has a specific set of functions to achieve those objectives. The specific activities for each SOC have to be defined. Some SOCs may prioritize some activities above others, or they may add activities as their capabilities are developed over time. Monitoring involves the direct and indirect observation of security devices and systems through their activities, events, and log data. Any anomalous, suspicious, or malicious activity is recognized and an investigation is conducted. The SOC can also monitor the performance and effectiveness of defenses. Baselines are the configuration standards that are applied to a variety of systems based on the organization's threat environment, known attacker methods, and best practices. A SOC can define those baselines and test configuration effectiveness against attacks. A response is needed once events are detected that indicate an incident or attack. The SOC can gather and analyze information related to the incident and take the appropriate steps towards containment, eradication, and recovery. Vulnerability detection is a proactive approach to defending assets. The SOC can actively manage the automated scans and manual testing needed to identify and report vulnerabilities within the organization. Intelligence is an active approach to preparing for attacks and developing defenses and responses for known and new threat actors. The SOC can develop internal intel and work with outside organizations to research, collect, and develop attacker indicators of compromise and tactics, techniques, and procedures. Coordination during security incidents may be needed internally to the SOC within the organization and with external parties. The SOC is the central hub of the organization's security operations and must develop and maintain cooperation with multiple entities. A Security Operations Center can exist in several forms. A dedicated or in-house SOC is one that is located on-premises at the organization's facility and is typically staffed with full-time employees or contractors. This type of SOC is typically centralized within the organization security program and serves the organization across all lines of business and all points of presence. The distributed SOC is one in which there may be functions spread across several locations. If the organization is a multinational, then there may be a need to distribute security functions and operations in several localities or regions. There may be a global SOC with several regional ones. Some specialized functions or activities such as forensics or threat intelligence could be centralized. An outsourced SOC can include all or some security operations that are handled by a service provider. A completely outsourced SOC might include a third-party provider handling the entire SOC. A partially outsourced SOC might have a managed security service provider, an MSSP, providing some management of security devices such as firewalls or IPS or the SIEM or after-hours support. While outsourcing may save initial costs of rebuilding a dedicated SOC, there are challenges in getting value from outsourcing a SOC. A virtual SOC is an approach that does not employ a dedicated facility. It may offer a portion of the normal SOC functions or operate only during an incident. The employees of a virtual SOC may have normal office workspaces and may only work part time on SOC duties. The physical space of a Security Operations Center may vary based on the needs and mission of the SOC. Not every organization has the resources to build a sophisticated SOC with dedicated space, or the SOC may start as a virtual SOC and then grow to include a dedicated space later. Here are some of the features that organizations utilize in the design of their SOCs. Having isolated rooms with limited access provides the SOC employees with a reasonably quiet area that is focused on the mission. Keeping other non-SOC employees away allows for concentration and collaboration during security incidents and avoids the spread of rumors and the interruptions that can occur when other people can enter the SOC. Well-designed status displays and dashboards allow the SOC employees to quickly assess the status of the SOC, the current workload, and the state of various defenses deployed. Some SOCs have large video walls displaying information or various monitors throughout the SOC. SOC employees can change these displays as needed, or share their screens on the large display for the team working an incident. Each employee will have a workstation to conduct their work. Some SOCs offer desks assigned to an individual employee. SOCs that operate around the clock in shifts may have shared workstations for the staff members of each shift. SOC employees may need to step away from the workstations for collaboration, discussion, forensics work, and training. Having these spaces nearby allows the SOC employees to focus on other aspects of their work. These spaces may have specialized equipment and the additional workspace needed. While we spent a little time talking about what SOCs are and what they look like, let's turn the discussion towards the key components of a SOC. First, the people that make up the SOC are critical to its operation. There are specific roles for personnel. And to make sure that everyone knows what to do, training is needed. Process provides the steps and guidance needed to ensure the right resolution of events and consistency in the approaches used. Process can include the step-by-step procedures, the methods for escalating incidents for assistance and visibility, and then reporting and metrics related to incidents, as well as the overall operations of a SOC. Technology aids the SOC personnel along with the processes in place. The right tools can simplify and bring the scale needed to handle the large volume of data associated with today's network environments. These tools can help the SOC function in monitoring, assessing, and defending the organization's information assets. I would argue that the combination of people and process outweighs the technology component. If you have the right staff with the right training and the right processes, then the technology employees won't matter. In fact, it'll change. A lot of SOCs fail when they're built around a specific tool or a suite of products from a vendor. There are some specific roles and responsibilities for SOC staff members. The SOC manager role is generally the leader of the SOC and has to manage the human resources, the processes, and technologies used. Some organizations include a security architect within the SOC. Their responsibility is generally around the organization security controls and the technology used within the SOC. If there's a security architect attached to the SOC, then security engineers may be as well. Their responsibility is to install, configure, and maintain the technologies used by the SOC. Lead analysts are senior members of the SOC team. They provide technical leadership, mentorship, and training to the other analysts and may have responsibility for multiple SOC functions. They may work collaboratively with other analysts on more complex incidents. Security analysts handle a majority of the SOC functions. Their responsibilities can include triaging incidents based on available information, managing the incident response process, gathering and disseminating threat intelligence, and participating in threat hunts to identify adversaries already inside the organization's network and systems. In addition to having the right staff members in the SOC, they need to be trained. While some staff members may come to the SOC with good experience and knowledge of incident response and the technologies in place, no one will have all the knowledge needed on their first day. Training is essential for SOC staff members so they can grow as employees, take on additional responsibilities, and to practice collaboratively with their team members. Processes and procedures are key to the effectiveness of the SOC. Each staff member should be trained on them. Education and training can include university courses, certification courses, technical training, vendor training, and other formal or even informal methods of learning. Tabletop exercises are scenario-based exercises in which members of the SOC team gather to walk through the scenario while discussing the next steps and reviewing procedures. A facilitator guides the team through the exercises and solicits input from the team on their approach and procedures Simulations and games are learning opportunities with a hint of fun. These can include digital forensics challenges in which the team collaboratively examines gathered evidence from a recent incident or through one of the online forensics challenges. A SOC may have several processes and procedures for their work. Most of them may center around managing incidents. While each incident will vary, there are some basic processes that occur. Triage is the process of reviewing the known details of incoming alarms, alerts, warnings, indicators, user reports, and other notifications to determine the next actions to take. Some may warrant further analysis to determine whether an incident has occurred. Escalation is the process of bringing in additional analysts or other resources to assist in working an incident or the process of informing management about an incident. If the incident requires technical skills beyond the current analyst or the effort requires additional resources due to the volume of work, the incident can be escalated. The investigation itself will have processes to follow for consistency and documentation. Each organization may define some basic processes to follow and documentation elements that are needed. As the analysis and investigation proceeds, the analysts must have some flexibility in their approach that can adapt to the direction the investigation takes. The response to an incident may involve containment, eradication, and recovery. Containment is that set of actions taken to limit the incident either from causing more damage or consuming more resources. Eradication is elimination of the threat from the environment. Recovery is the restoration of systems and services to their normal operations. There are some key tools that SOCs can use to aid in managing their processes and procedures. Playbooks or run books are scenario-based tactical guides for incident handling. They can be specific to the types of attacks anticipated and have detailed steps listed for the investigation and response. These are typically organization specific as they may have details about the specific roles and deployed security tools. Communication may need to occur with teams outside of the SOC either externally to the organization or internally. External contacts might include upstream network providers, related organizations with shared resources, and even information sharing and analysis centers, or ISACs. Internally, the SOC may need to coordinate analysis or response activities with other technology teams. Having an updated list of contacts is needed. Post-incident reviews are opportunities to evaluate the SOC team's actions and to make improvements to the process. These reviews are essential to iteratively improving the performance of the SOC. A knowledge base is a tool for collecting, grouping, and storing information about the organization's environment, threats seen, techniques, tools, documentation, and any other piece of information that might be useful to the SOC team. Individual team members should be encouraged to store information they have acquired over time in a common repository for the benefit of the whole team. Another process for the SOC is metrics and reporting. Organizations want to know if the investment made in building and maintaining a SOC is paying off. The value of the stock is measured through a variety of metrics. Each organization may want to know specific details about their SOCs, but the types of metrics can be categorized. Volume metrics are related to the number of alerts, threats, incidents, cases, or tickets. As the organization makes changes to the security program or introduces new security controls, there may be an increased scrutiny of volume metrics to determine the effectiveness of these changes. Time metrics show the time to detect, manage, and resolve incidents. Some of the metrics could include the time spent on incidents. The elapsed time from the beginning of the incident to various stages of the process and the time to respond to the initial report. Impact metrics show the business units impacted by the incident in the downtime and outages attributed to the incident. Cost metrics can include the costs of an incident, costs avoided through attack remediation, and manpower costs. Now let's talk about some of the technology used by SOCs. In addition to the various security controls in place, there are tools to collect and present the event data and to manage data used in the SOC. Log management tools collect raw log data from security controls and systems in the organization. These tools can store, filter, and archive log data. Most log management tools can provide advanced search options. Log data can be forwarded into a security, information, and event management system, or SIEM. It's a platform for aggregating and parsing log data, correlating log data from different sources into events, and presenting that enhanced information to analysts. SIEMs offer data enrichment through external threat feeds, integrating with internal data sources, and geolocation data. Automation with the SIEM can be enabled through the use of rule sets, external process execution, and integration with other security systems. Governance, risk, and compliance, or GRC, systems can be integrated with SOC operations as well. These tools can aid in presenting overall risk data and managing the incident workflow processes. Ticket systems allow analysts to manage incidents through a centralized information management tool where each incident has its own ticket. Tickets can be dynamically created by the SIEM when rules or alarms are triggered. As we discussed in the security architecture and engineering section, there are a variety of technical security controls that an organization can deploy. Some of the technology areas that the SOC may use as part of its mission focus include perimeter, network, endpoint, application, user, and data. The specific technologies can include the following controls-- intrusion and prevention or intrusion detection systems, which monitor network traffic looking for known malicious patterns that can indicate attack. An IPS can block attacks in real time, though too many rules enabled may affect the network performance. An IDS can generally look for more patterns in the traffic without impacting network performance. The SOC can monitor log data and alerts from these systems and actively change the rules to look for new patterns of attack. Distributed Denial of Service or, DDoS, mitigation systems and services monitor inbound network traffic looking for indicators of a denial-of-service attack. The SOC and monitor these systems and enable attack mitigation when needed or configure them to start automatically once certain thresholds are met. In addition to allowing only permitted traffic, firewalls can generate log data for packets and sessions that were dropped. The SOC can monitor the log data for attacks and alter firewall rule sets as needed to contain attacks. Endpoint management can aid in managing security baseline configurations and the recovery of systems from incidents. It can also help threat hunters looking for attackers or malware already inside systems. Proxy servers are generally used to monitor web traffic for known threats, block specific types of websites, and monitor traffic inside encrypted sessions. The SOC can set up website blocking, adjust the rules, and monitor alerts for threats identified in web traffic. Malware management systems include antivirus and anti-malware software, and the central management tools needed to operate in a large organization, as well as malware sandbox systems. The SOC can review alerts from these systems, collect malware samples from quarantine for further analysis, add in new signatures for new malware variance, and detonate malware in a sandbox to see what it would do.

Module Summary

It's important to understand the assets an organization has and the unique threats it faces in order to design and implement an appropriate set of security controls to protect them. An organization also needs to consider how to maintain visibility into those controls and react quickly to threat events, attacks, and incidents to contain and limit damage to those assets. In this module, we took a look at assets, security controls, and security operations. First, we defined what assets are to an organization. These are the tangible and intangible assets of value. Of course, where there is value, there are threats that must be considered. The design and implementation of security solutions requires an understanding of the appropriate security controls needed to protect information assets. Security architecture is an approach for understanding the business and security objectives and choosing appropriate security controls. Security engineering is the implementation and monitoring of those security controls. Managing and monitoring the security of the organization can fall to a Security Operations Center, which is a centralized approach to watching over the security controls of the organization. The security analysts that work in the Security Operations Center are involved in monitoring and testing the security of assets and managing incidents. As we did in the previous module, we discussed some of the concepts that are explored in more detail in other Pluralsight Big Picture courses on information security. Here are the other courses that you may want to explore next. Cryptography: The Big Picture is all about crypto. Security Management: The Big Picture covers the basics of an information security program. Threat Modeling: The Big Picture looks specifically at how to understand threats. Security Architecture and Design: The Big Picture explains the design and use of security controls for an organization. In the next module, we will look at how organizations use auditing, monitoring, and testing to evaluate and assess security controls and actively identify threats and vulnerabilities.


Auditing and Monitoring 

Auditing

Welcome. Organizations conduct their business using a variety of valuable assets. They may employ internal and security controls to protect those assets. They need to ensure that those assets have appropriate controls in place, monitor those assets and controls, and validate that the controls are operating effectively. In this module, we'll look at three specific areas, auditing, focused on information systems, monitoring of information assets and the security controls that protect them, and testing techniques to determine the effectiveness of security controls. Let's start with a definition of auditing. It is a process conducted by a competent and independent auditor that collects evidence associated with assertions made about a system and evaluates those assertions and forms an opinion and reporting on whether the assertion conforms to a standard. That's a lot to consider, so let's address each piece. Auditing is a systematic process in that there are specific and repeatable methods used to conduct the audit and develop consistent results. Audit independence applies to both the auditor and the organization. Both need to maintain independence from the area or activity under review to provide an objective assessment. In some cases, an organization may need to employ an outside audit firm to maintain the needed independence and objectivity. Evidence is the information and observations needed to determine that the business and security objectives are being met by the internal and security controls in place to protect the organization's assets. Assertions are the claims made by an asset and its set of controls. The auditors evaluate the assertions and render an opinion on the adequacy and effectiveness of controls in place. This opinion is provided through reporting. There are a variety of internal and external standards or frameworks with which an organization must demonstrate compliance. There are many types of audits. Some are related to or specifically for information systems or information security. Integrated audits are a combination of financial, operational, and information system audits. This type of audit is needed based on the use of and dependence on information technology in business. An information systems audit, or IS audit, evaluates the protection of information assets and various factors in how the systems meet business and control objectives. A specialized audit is one that is typically defined by an independent audit organization for a specific audit target or objective. In information security, there is this Statement on Standards for Attestation Engagements #18 or SSAE 18, which is used by certified public accounts to evaluate controls at service organizations. Forensic audits are audits focused on discovering and disclosing possible fraud and criminal activity. These can be conducted to support lay enforcement or corporate investigations. One thing that has bugged me over the years as an information security professional is the inappropriate use of the term audit. Those not familiar with the audit process typically make assumptions and inaccurately use the term to describe some aspect of testing or review as an audit. Here are some examples I've heard. Technical testing can be an important component of understanding an organization's risk and control effectiveness, but by themselves they are not audits. Vulnerability scanning is not an audit, but could be used as evidence of testing in a vulnerability management program audit. Penetration testing is also not an audit because it looks at the controls in place and not at the decisions made to select those controls or the remediation steps taken and documented as an outcome of testing. There are also some administrative activities that by themselves are not audits. A risk assessment is not an audit by itself. That type of assessment can be used as a data source for auditing and it can be evidence of key activities in an information security program. Tabletop exercises are not audits either. They are valuable in that they can be a training and process improvement tool. An audit may look for evidence that they're being conducted and that the results are used to make improvements. Be wary when someone asks for an audit or claims that some testing was an audit. They may be mistaken in their understanding of auditing. Organizations that understand their risks through a risk analysis process can take steps to mitigate those risks. The risk analysis information is also helpful to auditors as well. They identify unique risks to the business and identify their threats and threat actors, the negative impacts that may be experienced and the probability that those threats may occur. To an auditor, this is key information because it identifies the risks that the organization needs to address through controls. Risk analysis information can aid in the determination of high-level audit objectives. The information is also helpful in evaluating the controls to determine their effectiveness. And, with risk information about the organization, audit decisions can be made in a risk-based fashion. Audit and auditors use some key terms. Let's explore a few of them here. An audit charter is similar to high-level policy that is specific to audit. It defines the purpose, authority, responsibility, and accountability of the audit function for an organization. The audit subject is an area or activity that is the target of the audit. The audit objective is the purpose of the audit, for example, the audit may verify that systems deployed in a DMZ follow defined procedures for configuration and testing. The audit scope defines the specific systems and activities that are part of the audit. Audits themselves have risk in that there's a possibility that the audit opinions and reporting could be wrong based on errors in the collection of evidence. Some audit findings are more material or significant than others depending on the audience of the audit report. Some levels of management may not view all findings with the same concern. Evidence is the information gathered during the audit and used to determine if the audit subject is following the audit objectives. Auditors must use evidence that is sufficient and relevant. Auditors can start with the documentation associated with the target of the audit. This can include policies, procedures, standards, installation, configuration, or any other documentation associated with the activity or systems being audited. Auditors may conduct interviews with personnel involved in the creation, operation, or management of the target of the audit. The notes from the interviews may be used as evidence. Auditors may observe the activities of personnel involved in the audit target to determine if appropriate steps are taken. They may also ask for the current configuration files or log entries for review. Testing is used to determine if the controls are in place and operating as expected. A compliance test determines whether the controls are in place. A substantive test determines whether the controls are effective. There are time and cost considerations that must be addressed when an auditor conducts testing. It isn't reasonable to inspect or test every system, transaction, or activity that is under review. The auditor must select a smaller number. This is referred to as a sample. A statistical sampling approach may be used to objectively select the size and evaluate the results to make an inference on the entire population. This approach is based on statistics and probability. A nonstatistical sampling approach is a subjective method that uses auditor judgement to select the sample size. This method may select items that have a higher risk or those that are more relevant to the audit. Once the auditor collects information and reviews the audit objectives, she has to create an audit opinion. This opinion is an evaluation of all of the evidence gathered along with the original objectives established at the start of the audit. The opinion is a judgement that is arrived at through experience and not based on a specific method or process. It is not always obvious which controls are effective and which meet the control objective. This requires serious thought and consideration of the controls and the control objectives. There are strengths and weaknesses of any set of controls. These need to be identified. Recommendations need to be provided to correct or improve the controls in place or to suggest controls that are missing or are incomplete. One tool that can aid in determining the effectiveness of controls is to use a controls matrix. It's a simple approach that identifies the known threats on one axis and the known controls for those threats on the other. The matrix is then filled with information gathered from testing that shows the effectiveness of the controls and areas of weakness or missing controls. Compensating controls are situations in which a weak control is compensated for by a strong one. You may also have to consider that having only one control, even if it is a strong one, may not be adequate for the control objective. Other controls, such as compensating or overlapping controls, must be considered before an opinion is given. The audit opinion has to take a variety of factors into consideration before a conclusion can be determined. Once the audit opinion is determined, the audit details and findings need to be reported to the appropriate members of the organization and the documentation for the audit needs to be collected and organized. And executive summary is a concise report for executive management. This type of report avoids the technical terms and sticks to a business focus for the audience. An audit report contains most of the details that need to be provided. It includes information about the objectives, scope, and methodology used, the opinion on the effectiveness of the controls reviewed, and a detailed list of audit findings along with recommendations. Visual aids can be helpful in simplifying how the information is presented to the audience of the audit report. Graphs and charts showing the results of testing and the controls matrix we discussed earlier are good methods of visually representing more detailed data. We mentioned that findings and recommendations are part of the audit report. Depending on the audience and the materiality of the findings, the findings and recommendations details may need to be customized. For example, the technical and operational personnel may need a sufficient level of details to respond and implement changes to the systems affected. Follow-up activities are post-audit work needed to verify that the corrective actions to address the audit findings are completed. Audit documentation is the collective set of information and data created and collected during an audit. It may need to be archived by legal or contractual requirements. It includes all of the documentation from the planning phase all the way to the audit report that was issued. Audit programs should be designed to meet the needs of the business while simultaneously using limited audit resources effectively. The first approach is to ensure that the audit process is in line with where the business is heading. By knowing the business strategy, the audit program can ensure that audit activities are valuable to help the business achieve those critical business and security objectives. Most audits are periodic in nature. Continuous auditing uses an automated approach to audit transactions and activities in real time. This approach allows auditors to mark and review activities that deviate from the norm as they occur. A controls self-assessment is an approach that collaborates with the stakeholders in gathering data. There are a variety of methods that can be used, but the main idea is to include those stakeholders in reviewing the risks to the business and internal controls used to manage those risks. These can be as simple as questionnaires about controls, all the way to facilitated workshops.

Monitoring 

Organizations that monitor their assets and the activities within their systems are more likely to detect issues and react quickly to reduce risk. Let's look at monitoring in detail. There are a variety of monitoring activities and programs that an organization can build to improve their security posture in the face of adversaries. Monitoring activities have some shared objectives which we explore here and then dive into some specific monitoring programs shortly. Any monitoring program from information security should contribute to the reduction of risk for the organization. Ideally, these programs should be deployed enterprise wide so that the coverage for the organization is complete and consistent. The purpose of monitoring is for the observation, detection, and evaluation of issues. You have to find the problems, events, and activities that are known to be or suspected to be detrimental. Monitoring capabilities and the extent of that monitoring is critical to ensure detection. In most monitoring programs, the effectiveness of security controls is key to the success of the program. Either the program itself evaluates those controls or is dependent on the controls to provide data. Having effective controls in place ensures that monitoring processes operate as intended. The evaluation of those controls can lead to improvements. The output of monitoring activities initiates and drives the remediation and resolution work. Monitoring provides data that can start the initial evaluation of a finding or enable the correlation of other data sources to provide context around an event. There are two important monitoring programs that we will discuss. Network security monitoring is a program for monitoring network resources to identify malicious, suspicious, and anomalous activity. The tools used in monitoring alert incident responders and they drive the investigation and response. This type of program assumes that prevention will fail at some point and a breach is inevitable. Vulnerability management is a program for monitoring an organization's assets to detect vulnerabilities and to remediate them. By reducing the number of vulnerabilities available to attackers, the organization can reduce its risk. Another monitoring program that you may encounter is continuous monitoring, which is an overarching program for maintaining an accurate and real-time view of risks across the organization. Some of the elements in continuous monitoring can include vulnerability management and network security monitoring as sources of information. Network security monitoring, or NSM, is network-based in that the primary source of data used in this type of monitoring comes from the network. NSM is focused on threats and not vulnerabilities. It operates under the assumption that we talked about earlier that prevention will fail at some point. NSM works to foil attackers within the network through quick detection and remediation efforts to limit damage. NSM works with both the asset owners and the incident handlers to build processes to prepare for adversaries, protect assets, detect activities, and respond appropriately. NSM is primarily tool driven, since there is a significant reliance on tools to enable the program. The tools are part of the security control suite for the collection, delivery, and presentation of event and activity information. Establishing the network security monitoring program needs some processes in place to ensure success in detecting and mitigating threats within the environment. Some of these processes include planning and preparation for the monitoring, evaluation, escalation, and remediation of threats. These planning activities can include determining areas of the network that need monitoring, determining the steps needed during investigation, and reporting requirements. Network monitoring equipment needs to be deployed and managed. Choosing the right location to monitor on the network is critically important. Network visibility is an important consideration. Depending on where this sensitive data and critical business operations occur, the placement of sensors is important in capturing the right network traffic. Once the sensors and sensor platforms are deployed, periodic system maintenance and updates are likely needed. Once the sensor platforms are in operations, they will be collecting network traffic and evaluating rules. Sufficient storage space is needed for the network packet captures and a mechanism for transferring those captures to the analyst for analysis is also needed. If an active threat or attacker is detected, then a response is needed to contain and limit adversarial progress inside the network. Once the threat is contained, then the cleanup and restoration of normal services are needed. There are several types of tools used in network security monitoring. These include collection tools, such as network taps, span ports, sensors, and packet capture devices. These tools copy, route, and filter traffic of interest and apply a set of rules looking for suspicious or malicious activity in network traffic. Events, activities, and packet captures are then collected by delivery tools. These tools take the data generated by the collection tools and transports it or stores it in databases or files used by the presentation tools. These tools are used by security analysts to dive deeper into the information about events and activities of interest. The presentation tools simplify and enhance the job of the analyst by adding capabilities and efficiency to the analysis process. The collection, delivery, and presentation tools are bundled into software distributions to allow quicker setup and use. The collection tools can be distributed quickly to systems and networks that need monitoring. The presentation tools can be up and running in short order to get a network security monitoring program operating quickly. Vulnerability management is a program to identify and reduce the number of vulnerabilities within the organization's information systems. This type of program is focused on vulnerabilities and not specific threats or threat actors. The need to reduce vulnerabilities applies across the organization. If one area is not covered by the program, then the vulnerabilities that go untreated may be used to establish a foothold by attackers and leverage that access to attack the rest of the organization. The purpose of the vulnerability management program is to reduce risk for the organization. Risk management is a key piece of the vulnerability management program and provides useful information to determine the prioritization and approach to treating discovered vulnerabilities. Most organizations view vulnerability management as a simple scan and patch approach and lack any formalized process to manage it. The challenge for organizations is the abundance of systems with vulnerabilities and the lack of resources to address them. The result can be critical business systems with exploitable vulnerabilities, fragile systems that require significant effort to maintain, or even security breaches. Here are the key processes needed for a successful vulnerability management program. Organizations should have a risk management program in place already in order to understand and manage risks. Information from risk management is needed to identify areas and systems of higher risk so the treatment of vulnerabilities can be prioritized. An asset inventory can include the hardware and various software components used within the organization. That inventory should include details on the systems and their criticality. Vulnerability identification uses technical tools to identify vulnerabilities in the deployed systems and rank those vulnerabilities by severity. There are tools that can scan for known vulnerabilities through the network. This also includes gathering information from external sources on recently identified vulnerabilities. Organizations with critical business systems need to maintain stability and reliability to prevent issues that can negatively impact business operations. Change control is a formalized process of identifying, ranking, reviewing, and deploying changes, patches, and upgrades to systems. It is a key component in treating vulnerabilities quickly and lowering the likelihood of system issues from the changes applied. Configuration management is used to maintain the configuration of system across the organization. It includes knowledge of the configuration of systems through a configuration management database. The other part is being able to affect that configuration is needed without having to manage each individual system. Finally, we get to the actual patch and upgrade process. All of the previous processes are key drivers in managing this part of the process. I hinted at a few of the tools needed to implement the vulnerability management program. Keep in mind that the formal processes need to be in place first and supported by the tools. Organizations that prioritize tools over process may find that the prioritization and system impact issues will eventually become challenges that have to be addressed. Here are some specific tools. Asset management and configuration management tools are often overlooked as key components in vulnerability management. As I mentioned previously, organizations that use only scan and patch processes may not identify all of the critical vulnerabilities within the organization because they fail to keep pace with changes to the organization's infrastructure and assets. Tools are needed to identify new assets, both hardware and software, maintain an inventory of known assets and their configuration, and to change the configuration of systems when needed. Vulnerability scanners are used to periodically scan the network looking for known and new assets and to test for known vulnerabilities. These tools use a continually updated database of vulnerabilities and tests to discover vulnerabilities. They also build various reports and dashboards using current and past scanned data to show the vulnerabilities discovered and treated. As the primary purpose of vulnerability management is to reduce risk for the organization, it is essential to understand how well the program is working. Various metrics can be created from the status of the program over time. There are some challenges to reporting in that new vulnerabilities are being discovered all the time, and sometimes a single vulnerability may appear suddenly that affects a majority of the assets within the organization.

Testing

Testing in a security context involves confirmation or validation that security controls are working as intended. The testing process exercises the controls and evaluates their response. Testing can be applied at several points in the development and deployment of a system. So let's explore testing further. The purpose of security testing can be specified in some specific objectives. Similar to other programs we've talked about, security testing should reduce risk for the organization. Testing results provide the organization with details on their current security posture which can be used to determine the next actions needed to further reduce risk. The testing process evaluates whether a security control is in place and whether that control is effective given a specific set of threats and threat actions. If the control is not present, then the test may show a lack of protection in place. Some of the tests may show the presence of a security control, but identify weaknesses in the control that allow it to be disabled, bypassed, or delayed in ways that reduce its effectiveness. Some testing can be used as an approach to evaluate the response capabilities of the organization to attack. This type of testing can be a secondary factor to technical testing, or it can be a well-designed exercise to simulate specific types of situations that the organization may face. The testing can be a training opportunity or a measurement of an organization's current procedures and staff knowledge and capabilities. For technical security testing, there are several resources to consider. The National Institute of Standards and Technology, or NIST, publish special publication 800-115, the technical guide to information security testing and assessment. This guide provides an overview of the process and procedures used for security testing without the technical details. It could be used to develop a formal program for security testing within an organization. The Penetration Testing Execution Standard, or PTES, is a living document in the form of a wiki that provides the key details for penetration testing from the beginning of a testing engagement to the reporting of results. It has a companion technical guide with specific details on the technical testing aspects. The Open Source Security Testing Methodology Manual, or OSSTMM, provides a process for measuring the operational security of an organization through consistent testing and evaluation. The last formal release for version 3 was in 2010. The Open Information Systems Security Group published the Information Systems Security Assessment Framework, which is a thorough reference for testing, though a bit old. The NIST guide and the PTES are probably the more appropriate references to study. The others offer some good detail, but seem to be stagnant in terms of adapting to the current technology landscape. Before we talk more specifically about the various techniques, let's define terms. A security assessment determines the effectiveness of an assessment target in meeting a specific set of security objectives. The objectives are sometimes loosely defined in policy or more specifically in a security plan or system definition. The assessment process can include direct testing, interviews, and observations. Security testing is the process of exercising the various security controls. In testing, the conditions are simulated, but similar to real attacks. Through the testing process, the state of the controls can be determined and then compared to the intended behavior. One of the techniques used in assessments and tests is observation. The assessor reviews the artifacts of the system and evaluates the controls. These artifacts can include the documentation of the system. Some of the documents can include the security plan, configuration, management, and response documents. A well-documented system addresses the human side of the security management of systems. Logs, alerts, and event data are generated during the operation of a system. Testing that simulates attack conditions should result in the generation of unique and specific messaging, which can then trigger a system or human-driven response. Packet captures contain network packet data recorded in flight on the wire and stored for analysis. Packet data can show the presence of network packets and their header and payload details. Assessors may use packet captures as evidence of the effectiveness of network level security controls. Configuration reviews are a comparison of actual system configuration details against a known configuration standard, baseline, or best practice for enabling security controls. File integrity is a review of known files and their contents against a database. When attackers comprise systems, they often modify the system files to disable controls, hide their presence, or prevent legitimate operations from occurring. Databases of known one-way cryptographic file hashes can be used to compare with the actual files on a system to verify that the files have recognized and trusted contents. Direct testing is used to exercise controls under specific conditions. Here are some examples of testing. When systems and software are developed, testing can be utilized in an iterative manner in the development process. There are development methodologies that utilize testing specifically in the design of the system and others that integrate testing throughout the lifecycle. Testing in development can discover issues early on in the process and result in a more secure system when deployed. Network scanning is a test looking for addressable network systems and the network services they offer. This type of scanning can determine the presence of systems and the effectiveness of network security access rules. Vulnerability identification can use the result of a network scan to identify vulnerabilities in configuration errors on the known systems in the environment. Once the vulnerabilities are corrected, follow-up scans can be used to validate that the correction efforts were effective. Password cracking is a testing technique to determine the relative security of passwords used, and whether password complexity rules are effective in practice. Penetration testing is probably a common term for most security professionals. Penetration testing goes beyond just identifying vulnerabilities and encompasses the exploitation of those vulnerabilities. It can show the level of access and compromise that a real-world attacker could attain. Network penetration testing is focused on network accessible systems within an organization. This testing can be used to test systems on an organization's network boundary, such as its public facing websites, wireless networks, and applications to test perimeter control effectiveness. It can also be used to test internal security controls against malicious insiders or attackers that compromise internal systems. Application penetration testing looks at the application's hosting environment and the effectiveness of its internal controls to limit access to sensitive data and critical business functions. Physical penetration testing can simulate attacks in a physical space. These types of tests evaluate the effectiveness of monitoring, entry controls, barriers, reception areas, and security guards against attack. Penetration testers often use social engineering techniques to acquire credentials and information, bypass gatekeepers, and attempt to avoid detection and capture.

Module Summary 

An organization needs to maintain control over its environment for security to be effective over time. The techniques we discussed in this module are key components of a security management program. In this module, we looked at the various components of an information security program for evaluating the current state of security across an organization. First we looked at auditing as a systematic process of reviewing an organization's information security program, and its conformance to a standard using an independent and objective evaluation. The audit process includes the collection of evidence and the assertions made about assets and their set of controls. Auditors review the evidence, assertions, and standards to render an opinion on the adequacy and effectiveness of controls in place. That opinion is then reported to the organization. Monitoring is a key activity in reducing risk to an organization because it involves watching over assets, detecting issues during normal operations and while under attack, evaluating the effectiveness of controls in place, and the guiding efforts needed to improve security operations. Testing is also a key activity for risk reduction, but uses a more direct approach for evaluating the presence and effectiveness of security controls under a specific set of conditions and uses the results of the testing to compare against the intended security objectives. Test results can provide the basis for the improvement of existing security controls or identify new controls that need to be implemented. For this module, we spoke only briefly about concepts that have a lot of depth and detail in other big picture courses. Here are the other courses that you'll want to look at next. Information Systems Auditing: The Big Picture covers all things auditing for information systems. Vulnerability Management: The Big Picture is a course that covers all the steps and details for proper management of vulnerabilities. Penetration Testing: The Big Picture is my own course on penetration testing. I cover a lot more detail on the use of penetration testing for organizations and the different testing approaches. In the next module, we'll look at how organizations manage security incidents, prepare for disruptions and disasters, and manage their security operations.



Managing systems and Operations 

Managing incidents and operations

Welcome back. Regardless of how well an organization has built its defenses and protections against threats, attackers, and insiders, there's always the possibility that a weakness will be found and exploited. The organization needs to prepare for these contingencies through appropriate planning, monitoring, and execution. In this module, we'll look at three specific areas, managing security incidents within the organization, planning for events that can impact the organization's ability to function, and operating key aspects of the security function of the organization. Let's start with incident management. Even organizations that have a mature information security management program will face security incidents on occasion. Those that have an incident management program in place are better able to quickly respond to incidents and limit damages to the organization. Here are some specific objectives for incident management programs. Detecting the incident and then providing visibility to the appropriate stakeholders is key. There are lots of statistics that show that threat actors invade corporate networks and remain undetected for many weeks or months. Incident managers need to ensure that incidents are detected quickly, diagnosed accurately, and reported. After the incident is detected, the organizations must move quickly to contain and isolate the threat and limit the damage that the threat or threat actor may cause. To do this, the organization must have some basic understanding of the threat and threat actors involved. Once the threat is contained and better understood, it can be removed from the environment and changes can be made to prevent the recurrence. Organizations that fail to change their environment face reinfection of malware or continued threats from persistent threat actors. Finally, normal operations need to be restored. Affected services need to be recovered so that business can continue to operate. Cleanup operations and further investigation may be needed to adapt the environment to prevent future incidents. Now let's see how organizations prepare for and manage incidents. Organizations will face a variety of threats during security incidents. The more they know about them, the better the response will be. Incident management programs need to have threat specific knowledge to be effective. Some of the threat categories include malicious software or malware. This can include viruses, Trojan horse software, worms, ransomware, and other digital threats. Denial of service is a resource attack where resources are consumed to a point that legitimate usage can no longer be supported in a timely manner or at all. Distributed denial of service attacks, or DDoS, rely on a large number of endpoints, say from a botnet, to consume network resources using a large volume of normal looking requests to take network services offline. Compromised credentials can allow attackers to impersonate legitimate users. Credentials can be harvested from social engineering or spear phishing attacks. In addition to the various threats, there are threat actors that are directly involved in the perpetration of attacks against organizations. These actors include criminals that seek to steal information, resources, or money from organizations. Hacktivists seek to embarrass or harass the organization for social change or political purposes. Advanced persistent threats, or APTs, are sophisticated attacker groups, often state sponsored, that attack specific organizations and persistent within the target environment for a long period of time. APTs try to remain undetected as they explore the organization's network environment and data and use a variety of tools to assist in achieving their goals. Malicious insiders are trusted employees and third-party vendors that have access to the organization's network. There our often less monitoring and security controls put in place for employees and third parties. These insiders may have unfettered access to the organization's information and systems and may seek to steal and exfiltrate data for profit or to expose an organization's presumed misdeeds. Each of these threats and threat actors can pose a significant challenge. The organization's incident management program must be prepared and flexible enough to manage these threats and new ones we have yet to see. Incident management programs consist of a few key components. A high level policy, a plan, and a series of procedures define the organization's responsibilities in managing incidents and the manner in which they are handled. Incident response teams are groups of skilled professionals that manage incidents. They are properly empowered to conduct this work and trained on a continuous basis so that they can handle new threats. When a security incident occurs, the teams are able to handle the incident with a methodology for gathering information, analyzing it, containing and eradicating threats, and recovering systems and restoring operations. Once the incident is understood and artifacts from any threats are collected, the team shares that information with other incident response teams in order to prevent those threats from effecting other organizations. When building an incident response capability, you should look at NIST Special Publication 800-61 and ISO/IEC 27035 as standards with a lot of good information. Incident management requires the appropriate authority and planning to be recognized as a valuable capability to the organization. A high-level incident response policy serves as a foundation of the organization's incident management program consisting of management commitment, organizational roles and responsibilities, the definition of an incident, and the associated severity ratings and reporting requirements. The incident response plan provides the roadmap of the program through descriptions of the organization's strategy and approach to incident management, communications during an incident, metrics, and a desired path to building the capabilities of the program. Procedures are the technical details used in each incident to ensure consistency and efficiency. These are typically scenario or threat based so that incidents that have been handled before are easier and quicker to handle in the future. Checklists may help ensure that all the steps are followed. Forms may be needed to gather the right information and to communicate information to others. Incident response teams vary depending on the organization's needs and staffing. In terms of the team structure, there are several models. Which is used depends on the number and severity of incidents that an organization typically handles and the availability requirement of the incident handlers. Some organizations utilize a dedicated team of professionals, or have a team of security professionals that also handle incidents on an as needed basis. Others may outsource that capability. The roles and responsibilities are typically defined in the incident response plan and include the incident response manager and security analysts. The manager is responsible for the oversight, prioritization, and communication during incidents. The security analysts handle the triage activities and specialty trained analysts handle the forensics. Some organizations may also have threat research or threat intelligence teams to gather and organize information about threats in order to have more contextual information about current incidents and to prepare for future incidents. As the threat landscape changes, so must the incident response teams. This is accomplished through training. It can consist of traditional courses, online training, certification preparation, or even forensics and incident response challenges. The team should also train together through the use of tabletop and simulation exercises. The incident handling process can vary between organizations, but there are some foundational principles that should be incorporated. The NIST Special Publication 800-61 uses the following process. Preparation is key to ensuring that the handling of incidents goes smoothly. The incident response team must have established communication paths and methods, the right set of hardware and software tools, and the appropriate organizational and technical information available. Detection and analysis are the activities needed to detect anomalous, suspicious, and malicious behaviors using the alerts and logs available to analyze that data to determine if an incident has occurred, to document the investigation activities and findings, to allocate resources based on priority of the incident, and to notify the appropriate stakeholders. Containment, eradication, and recovery is focused on the minimization of risk to the organization through the elimination of the threat and the cleanup needed to get the organization back to normal operations. Post incident activities are often overlooked, but they could be the most important to an organization that wants to mature their incident management program. Lessons learned should be incorporated into the incident response capability so that the program can mature and be more efficient with future incidents. This entire process is a cycle. Once the lessons are learned, they are incorporated into the preparation and the process begins again. Once an incident response team is in the midst of handling incidents, they are gathering threat information, malware details, and other information related to the incident and the threats. In most cases, there are other teams facing the same or very similar threats. Ideally, sharing information about the threats can help resolve incidents faster for organizations. The challenge is finding ways to share information quickly and without passing along sensitive internal data to others. Within large organizations, there may be several incident response teams, either in different business units, or distributed around different geographic areas. As the internal corporate network is likely open, internal incident response teams may be fighting the same fight against the same adversary. These internal teams need to coordinate their activities to ensure that they can contain and eradicate similar threats within the organization. Organizations in related industries also face similar threats. To aid these organizations, Information Sharing and Analysis Centers, or ISAC, have been established. In the United States, there are ISACs for state governments, research and education institutions, financial services firms, healthcare, and many others. Some of the information shared is on the Tactics, Techniques, and Procedures, or TTPs, used by threat actors. TTPs are a profile of how specific threat actors operate and includes details from the tools they use to the methods they take when entering a network to the technical details of how they compromise hosts. Several tools exist to facilitate the exchange of information. An entire course could be developed around these tools. STIX, TAXII, CyBOX, and YARA are just a few of the more common tools used.

Continuity Planning

Your company's primary revenue generating website just disappeared from the internet. What do you do now? Those next steps are the key focus of continuity planning. Let's take a look. You'll recall that the CIA triad that we discussed at the beginning of this course included the principle of availability. So far we've spent a lot of time talking about protections for confidentiality and integrity. With continuity planning, availability takes center stage. The key objective of continuity activities is centered on business survival. Some disasters and disruptions can devastate an organization to the point that it fails to continue as a viable business. Proper planning can reduce the impact of the disruption. Through the understanding of potential disruptions and planning in advance, an organization can minimize the negative effects of disruptions and disasters. The longer that an organization has to manage the internal and external impacts of disruptions, the less likely that it is to recover at all or least reduce the overall cost of the impact. Planning can reduce the time needed to restore normal business operations in a cost-effective manner. There are a variety of threats that an organization can face that disrupts normal business operations. Here are a few that an organization must consider in their continuity planning. Threats to personnel include the loss of key individuals to illness, death, work stoppage events, or retirement. As organizations are dependent on information technology, it is critically important that those systems operate correctly and are available to the business and personnel. Threats to IT systems include the business failure of current IT vendors, security breaches, connectivity issues, malware, and other digital threats, datacenter outages, significant software bugs in key business software, the theft of hardware devices, data loss, and other IT related failures. Business operations can be negatively impacted by supply chain interruptions or the loss of key suppliers, the outbreak of war, terrorist events, employment strikes, loss of or incorrect accounting data, distribution interruptions, and other operational issues. Threats to facilities can include the loss of buildings and property due to weather, environmental failures, or human-driven events. Weather events, such as floods, tornadoes, hurricanes, and lightening can damage, disrupt, or prevent access to key facilities used by the organization. Environmental control failures include heating, air conditioning and ventilation failures, electrical outages, fires, gas leaks, and hazardous material spills. Human-driven events can include damage or disruption to facilities due to war, terrorism, employment strikes, riots, and protests. Each of these types of threats may seem minor or extreme, however, to the organization, even seemingly small events can have a negative impact to the business. In contingency and continuity planning, there are several names and naming conventions of plans and planning activities. In this course, I'll focus on two types of plans. Business continuity plans, or BCPs, are planning and preparation activities to manage interruptions to normal business functions and operations. The key objective for BCP is managing the business during disruptions. Disaster recovery plans, or DRPs, are needed for large-scale business incidents that may involve the loss of or damage to business facilities. The key objective of DRP is the restoration of normal business operations at physical facilities. Some organizations have crisis management plans, which integrate the business continuity plans and disaster recovery plans in a comprehensive manner. This approach provides the organization with the appropriate integration of plans for large and disruptive events. The National Institute of Standards and Technology has special publication 800-34, which is guide for continuity planning. It is a good resource for the planning process and the various plan types. There are some important high-level processes in continuity planning to consider. There are, of course, variations of these. However, successful continuity planning needs at least these. Project initiation is obviously about starting the project. There are some important components of starting these types of projects. The most important is management support. The highest levels of management need to understand the importance of these projects and fully commit to their success, as well as allocate appropriate resources and money. The other important parts are defining the scope and objectives of the project, forming the team, and gathering the resources needed. A current state assessment is needed to understand the organization's current susceptibility to disruptions and disasters. Some components of this type of assessment include a threat assessment, risk management, and business impact assessment. Development is the process of determining the appropriate continuity strategies, developing the structure for continuity plans, and planning for testing and the allocation of resources during recovery. Implementation is the deployment of continuity plans to various stakeholders and effected organizational units, training and awareness programs, and initial walkthroughs and testing of the plans. Maintenance is the management of the continuity plans moving forward and includes the periodic review and updates of the business impact assessments, risk assessments, and continuity plans, as well as regular testing of the plans. In order to prepare for a continuity plan development, there are some specific activities needed in order to ensure that the planning process has accurate and useful information. A threat assessment is needed to understand the unique threats to the organization. These are generally organized into three types, physical and personnel security, environmental security, and information security. A risk assessment takes the understanding of threats to the business and potential impacts from the business impact assessment to understand the overall risk to the organization. The business impact assessment provides a prioritized list of critical business processes and maps their time sensitivity for recovery and the supporting resources needed. Recovery strategy development is the process of evaluating the current state information and choosing appropriate recovery strategies. The strategies should be in alignment with the time sensitivity and recovery requirements of the business processes. Here are a few of the common recovery strategies organizations may use for managing the recovery of their data. The most common and often the easiest to start with is a process of taking data offsite. This can include rotating backup tapes and other data storage media away from the primary facility. As the costs of online or cloud storage services continue to fall, organizations can shift their storage needs to storage services, either as the primary storage service or as a place to store redundant data or backups. Database mirroring is a process of copying database changes to a parallel database system located elsewhere. Electronic vaulting is a strategy of sending data to an alternate processing facility and keeping it online and available. Remote journaling is a strategy of copying transactional data in real time to an alternate processing site. Here are some strategies used for recovering IT infrastructure. Cold sites are locations that can support IT, but have no equipment in place. Warm sites have some level of IT equipment on site, but they are not complete nor configured for use. Hot sites have IT equipment and are configured for use. They can support recovery within minutes or hours. Mobile sites are trailer-based systems with some supporting IT equipment that can be brought to locations to support recovery. Mutual aid agreements are entered into with nearby organizations that have some amount of space to support a recovery. Multiple site strategies involve using other sites controlled by the organization to support recovery operations within the organization's existing property. Once the recovery strategies are developed and the continuity plans have been created, they should be tested periodically. Part of the planning process should include the level of testing needed and at what frequency. Testing serves two purposes. The first is the opportunity to identify issues, errors, and needed updates in the plans. The second is to familiarize the staff involved with the continuity plans and managing through disruptive events. There are several types of tests that can be performed. Each type has differing levels of involvement, complexity, risk, and cost. A plan walkthrough is a simple exercise of pulling the plans out and reviewing with some of the team that would be involved in implementing the plan during an actual disruption or disaster. This helps the team find obvious and sometimes subtle errors in the plans that need to be addressed or sections of the plans that need to be updated or corrected. A tabletop exercise can involve a larger group of individuals that would be involved in any plan implementation and conducting a simulation of a specific disruptive event. The situational parameters and the conditions would be established in advance and the team would use the contrived event to test the continuity plan. Using the plan, the team would evaluate the situation, make decisions based on the plan directives, and think through the steps needed. Both the plan walkthrough and tabletop exercise approaches require only a small amount of effort to conduct, are low risk to operations, and have little cost. Parallel testing is more complicated in that it involves operating business processes at the primary processing facility and its backup in parallel. If the organization has an alternate processing facility, that facility would be stood up and process the same data as the primary facility. The processing results and performance data can be compared to ensure that the parallel processing matches the primary facility's output. Full interruption testing involves switching primary processing to the alternate site and pausing or disabling processing at the primary site. This is a more complete test because the business is operating solely at the alternate processing facility. Both parallel and full interruption testing approaches are significant in the level of testing conducted and the effort required. They also have a higher risk and cost to conduct.

Security Operation

Security operations is a broad topic which includes many of the components that we touched on in other parts of this course. Most of the activities associated with security operations are tactical in nature and support the day-to-day activities of the program. Let's spend a few moments talking about how these types of operations allow an organization to manage its information security program. Security operations has a lot of moving parts in an information security program. Each organization may also categorize the components of their security operations in different ways or have components of traditional security operations managed by IT groups that do not have a security focus. Regardless, here are the objectives that security operations generally cover. Given the wide access to information technologies, events and incidents that require an investigation generally have an electronic device, organizational data, or electronic communication service involved. And information security program has to support investigations for those types of information technologies. Security operations can also be directly involved in the security of assets and users, or at least have a shared responsibility with other IT groups. Monitoring and protecting IT assets may involve specific security technologies. Security operations may be directly involved in managing those systems. Certainly the security operations personnel are utilizing these security systems to do their jobs. Security operations is also involved in the planning preparation processes to handle contingencies the organization may face. Security operations can be directly involved in investigations. The organization's own staff might be doing the work, or they might need to outsource it, depending on the skills needed, the availability of staff, or the independence needed to avoid conflicts of interest. There are several types of investigations in which they may be involved. Depending on the legal system where the organization conducts its business, the investigation approach may vary. For simplicity, there are two types to consider. An externally driven investigation is one in which the motivation for the investigation originates outside of the organization. It could be an investigation under common, civil, tort, administrative, or criminal law. The level of involvement of the security operations team may vary depending on a variety of factors. An internal investigation is generally inward focused for the organization, though the findings may lead to bringing in external entities to continue the investigation. Internal investigations may involve violation of policies or anomalous activities. During the investigation, there are specific techniques the investigators may use. The collection of evidence is needed to inform and support the conclusions of any investigation. Digital forensics may be used to explore the data or devices involved in the incident. The findings, results, and conclusions of the investigation need to be presented to stakeholders during and after the investigation. These may take the form of status reports, preliminary findings, and a final report. Asset management is focused on identifying and configuring IT assets for security purposes. An IT asset includes networking and computing devices under the organization's control. An asset inventory is needed to understand what IT assets are owned and managed by the organization. These inventories must include software, as well as hardware and virtual machine assets. Additional details, such as installed software versions, criticality ratings, and business owner information is helpful for vulnerability and risk management practices as well. Configuration management is part of the protection and prevention activities for security operations. Before assets are deployed within the organization, they should be configured to a security standard. These standards should be defined by security operations to ensure the base configuration is secure and that the appropriate security tools are installed and configured. Change management is typically an IT function that involves making changes to IT assets with minimal impact to business functions and users, or through an appropriately planned rollout. Security operations typically has to manage or at least define needed configuration changes related to security and software and firmware security updates through the organization's change management process. Media management is the classifying, labeling, storing, and eventual destruction of sensitive media. Media could be in the form of portable storage devices, backup tapes and disks, or other devices that move data physically. Controlling access to data on media can involve both physical and logical controls. Users access organizational services and information through their user accounts. You may hear the term identity and access management as the primary program for managing user accounts and access within an organization. Managing and monitoring user accounts is essential to securing the organization. While user account management may not be a primary function of security operations, security should have influence over the process and the ability to manage user accounts when needed. Account provisioning is the process of allocating user accounts and assigning access privileges to users. Security operations should be involved in defining the appropriate privilege assignments and periodically auditing account access privileges. Authentication techniques may need to vary depending on the privileges a user account is assigned. Higher privilege accounts may require multi-factor authentication to avoid account compromise. Password complexity and expiration rules may also be defined in policy and implemented or reviewed by security operations. Privileges are logical controls that allow or deny access to specific data or information services defined for a specific user or a class of users on an organization's IT systems. Privilege management is the assignment of user account privileges. Security operations should be involved in the definition of these privileges with the ability to change them when needed. Account recovery processes are needed when a user losses access to their account, either due to password expiration, a forgotten password, account compromise, or account suspension. Security operations should influence the process used in order to prevent weak processes from being exploited by attackers or an employee bypassing the defined process. Account monitoring is needed to ensure that compromised user accounts are detected and corrected quickly. An attacker with user credentials can access information and services as that user. Security operations much monitor for anomalous and malicious user activity. The termination and suspension of user accounts is needed in the event that an employee is terminated or a user account is compromised. Security operations may need to quickly disable user accounts either through a defined process with a human resources department or due to an investigation or monitoring of a user account. An organization's personnel are key stakeholders in information security. They have access to sensitive organizational data and are often the targets of attackers. Security operations may have a direct impact to personnel security. Security awareness programs help inform and periodically remind employees of their role and responsibility for protecting the organization's information and systems from attack. These programs may use visual tools, such as posters, fliers, and other printed media as reminders or informative messaging about specific attacks or threats. Security training may be required for specific compliance reasons, job function, or as a general education requirement for employees. Training can be focused on specific job functions that involve sensitive data management and use or more focused on the use of information security tools or software. Security operations are often dependent on specific security technologies to implement the controls defined in policy, procedures, and standards. The specific vendors and products used may change over time. However, the security functions generally remain the same. Here are some security technologies that security operations use and manage. We described the details for each of these elsewhere in this course, log management systems, anti-malware systems, firewalls, next-generation firewalls, and cloud access security brokers, intrusion detection and/or prevention systems, data loss prevention tools, and security incident and event management systems. As security technology evolves, I would expect this list to change. The remaining components of security operations have been covered in this course already in greater detail. As a way of summarizing these functions, I'll provide a brief listing here. Please review the sections of this course, or the other Big Picture courses for more information. These are monitoring, physical security, incident management, vulnerability management, disaster recovery planning, and business continuity planning.


Summary 

In addition to reaching the end of this module, we've also reached the end of this course. Let's quickly summarize the concepts we've studied. For this module, we focused on the management of key parts of an information security program. We started with incidents. An attacker running lose inside the network can expose the organization to significant damage. We spoke about the need for an organization to plan and prepare incident response teams, and to handle these incidents quickly and efficiently to contain and eradicate threats. Power outages, labor strikes, floods, and hurricanes can disrupt business operations and even bring about the end of some businesses. Those that prepare for the likely contingencies are more likely to survive and continue. An information security program has a number of moving parts that serve to implement the information security policies and procedures established by the organization. These activities are the operational components of the program. We spoke about the security operations that support investigations, manage the security of IT assets and users, manage the security technologies, and planning and preparation for contingencies. In our discussion in this module, we introduced some of the concepts that are explored in more detail in the other Big Picture courses. Here are courses you may want to tackle next. Advanced Persistent Threats: The Big Picture covers those pesky threats that can be driven by state actors seeking to avoid detection and to thoroughly compromise an organization. Incident Detection and Response: The Big Picture focuses on finding the threats and eliminating them. Digital Forensics: The Big Picture looks at the methods of finding and extracting evidence from digital systems. And Security Management: The Big Picture comes up again as a key course. This time, there is more in depth coverage for organizational resilience. Information security and subsequently this course can be summarized with these four key activities. Each activity is dependent on, as well as builds on the other activities. An organization needs a good foundation on which to build a successful information security program. Understanding the risks that the organization faces, applying well-understood security principles, providing the mechanisms for direction and oversight, and managing compliance with established standards can guide the organization's activities for information security. Once the organization understands risk, it can design and apply various countermeasures to protect assets. These countermeasures are the various administrative, technical, and physical security controls needed to prevent damage to or loss of valuable information and information systems. Just as an organization must change to adapt to a changing world of business, so to must an organization's information security program. Changes to the business effect risk, and new and evolving threats can render existing security controls ineffective. The information security program's direction may need to be evaluated and altered periodically. Once the information security program is established and the direction is known, there are a variety of actions that have to occur. These are the operational aspects of an information security program and include the use and maintenance of controls and the security technology infrastructure, as well as security incident handling and preparing the business for various contingencies and large-scale disasters. My hope is that you now have a big picture view of information security. You may feel a little overwhelmed by all of the pieces we outlined in this course. That's understandable and totally fine. You may specialize in just one area, but it helps to have a wider view of how it fits into the larger picture and contributes to your organization's strategy and focus on security. Information security is a journey, not a destination, and I hope you enjoy your journey.

Devopsec The bigger picture 

Course overview 

Hi, I'm Richard Harpur, and welcome to this Pluralsight course, DevSecOps: The Big Picture. We are now in the midst of a major software transformation. This transformation is empowering software engineers to build secure software. Just like agile practices enables developers to build quality in, DevSecOps enables developers to build security in. This course is a big picture course. It is designed to give you a solid, high‑level understanding of DevSecOps concepts. It is the perfect launchpad for you to start your DevSecOps journey. After completing this course, you will understand the major concepts of DevSecOps, how it enhances the DevOps process, and how it empowers developers. In this course, you're going to learn what is DevSecOps, including the DevSecOps Manifesto, what are the benefits from implementing DevSecOps, how DevSecOps can enhance a typical software development lifecycle, and how developers gain more control and become empowered. You will also learn the difference between fact and fiction when we debunk some common DevSecOps myths. You don't need to have completed any other training before you start this course. I have no doubt your organization will benefit from DevSecOps, so I'm delighted you're going to join me on this course. Let's get started.

Module overview

Hi, I'm Richard Harpur, and you're very welcome to this Pluralsight course, DevSecOps: The Big Picture. This is one of the hottest subjects in security at the moment, and I'm delighted you're going to join me on this course. Let's have a look at the first module in this course, Understanding DevSecOps Concepts. In our journey through DevSecOps, we're going to look at the DevSecOps Manifesto. This is really one of the reference documents that we're going to look at to help us understand what we're trying to achieve with DevSecOps. Then we're going to look at why DevSecOps is a good idea. We're going to look at the problem that DevSecOps tries to solve, how DevSecOps increases software assurance, and how DevSecOps is an extension of DevOps. This is the customary overview slide. As I mentioned, we're going to start by looking at the North Star for DevSecOps. That's the DevSecOps Manifesto. Then we're going to go through the reasoning why DevSecOps is such a good idea, the problem being solved, and how DevOps is extended with DevSecOps. Finally, we're going to wrap this module by looking at how DevSecOps can increase software assurance. So let's jump in and get started.

The devsecops concepts 

Before we look at the DevSecOps Manifesto, I want to make sure that we're all of the same understanding in relation to what the DevSecOps concept is. If we look at the term DevSecOps in Google Trends, we can see there is a marked increase with this term being searched for on google over the last three years, so you're definitely in the right place to learn more about this growing trend in information security. You may be experienced and already applying DevSecOps in your organization or this might be something that you've stumbled across and are very keen to understand more about. In its simplest form, DevSecOps is answering the question how to incorporate security within agile and DevOps practices. We're really looking to embed security within an already agile process. Here is a great quote from Shannon Lietz, The purpose and intent of DevSecOps is to build on the mindset that "everyone is responsible for security" with the goal of safely distributing security decisions at speed and scale to those who hold the highest level of context without sacrificing the safety required. You can read more about this and the context behind it at the link on the bottom of this slide. You can see we're trying to make security more context based. We're trying to place security decisions at the hands of those who are best placed to make these decisions.

The devsecops manifesto

I want to walk you through the manifesto. Let's look at the Agile Manifesto, firstly. Think of the manifesto for DevSecOps as a type of North Star, sort of a guiding principle. The Agile Manifesto has been around for quite some time. Let's have a look at that. If you go to agilemanifesto.org, you can see the principles that agile software development are based upon. These are very clear guiding principles. They are not absolute rules, but they're really telling you where you should be emphasizing or focusing your efforts. For example, individuals and interactions are valued more than processes and tools, working software is valued more than comprehensive documentation, customer collaboration is valued more than contract negotiation, and responding to change is valued more than following a plan. It does quite clearly set out that the items on the left are valued more than the items on the right. It doesn't mean that the items on the right can never happen in an agile software development environment. It just means that true lessons learned as part of agile software development, it has been found that items listed on the left‑hand side of the slide are more valuable and more aligned with agile development. Now let's jump over to the DevSecOps equivalent. If you go to devsecops.org, there is a manifesto that is described on this site. Again, remember the manifesto is a type of North Star or a guiding principle. If we scroll down, we will see the manifesto items are very similar in their layout and construction to the Agile Manifesto template. There are items on the left, which are valued more than items on the right. Let's have a look at this in more detail now. There are a larger number of items that comprise the DevSecOps Manifesto, nine items in all that have been identified as being significant to call out. These are valuing leaning in over always saying no. That's an important one because security and security practitioners can be branded as the naysayers, the people who simply say no. Leaning in, in other words, contributing to the team and being part of the solution is valued more than saying just simply no, you can't do something. Data and security science is valued over fear, uncertainty, and doubt. Well, this is an obvious one for anyone working in the industry. Using data to substantiate actions and remediation plans always trumps fear, uncertainty, and doubt, or FUD, as we call it. Open contribution and collaboration over security‑only requirements. Again, this is about being transparent with the holistic view of what you're trying to achieve. Consumable security services with APIs over mandated security controls and paperwork. This is critical to a DevSecOps environment. Remember I said we want to embed security within the process, hence the reference to APIs and consumable security services. No longer can we come with a clipboard just checking off a checklist. Business‑driven security scores over just rubber stamping security is pretty obvious. Red and blue team exploit testing is more valued than relying on point‑in‑time scans or theoretical vulnerabilities. Again, this is moving more towards active involvement rather than point‑in‑time scans and giving a report card. 24/7 proactive security monitoring is valued more than reacting after being informed of an incident. Again, moving our engagement to be more real time and active. Shared threat intelligence over keeping information to ourselves. We've seen this for a long time where criminals share vulnerabilities and other information amongst their own peers. So why don't we share threat intelligence to defend the systems? It makes a lot of sense to me. And finally, compliance operations over the aforementioned clipboards and checklists. We should be looking to build compliance into our systems rather than coming along later to see can we inspect compliance as part of the system?

The security problem devsecops addresses 

I want to move on and try to answer the question for you, why DevSecOps? What makes DevSecOps so powerful, and why are so many people interested in adopting it? In order to answer this question, we need to look at the software development evolution. Let's have a look at the typical waterfall methodology which has been in use for decades. Typically this starts with requirements, which roll into design, code, testing, and maintenance phases, effectively waterfalling from requirements all the way through to maintenance. Many software development organizations are now moving to a different type of methodology which involves iterative sprint planning, followed by daily shorter term scrums, or other types of short‑term deliveries, in what's called agile methodologies. Comparing the two methodologies, with waterfall, we would typically start at a point in time and then invest a lot of work and time until such point as we get value return for our effort and investment. With agile methodology, things are slightly different. We start at a point in time, but we return increments of value, smaller quantities of value, more frequently. This allows us to get a quicker return for the investment we have made. We don't have to be waiting for a very long time to get the return on investment. Instead, we can be releasing value at multiple stages along the lifecycle. Now let's have a look at how we can apply this same logic to information security, and specifically look at the friction that currently exists in the information security processes. If we look at the typical security processes in any project lifecycle, we could align them very much to the waterfall methodology, a security risk assessment followed by a security plan, code review, penetration testing, and regular audits. All follow a particular sequence and have prerequisites before they can be completed, very similar to the waterfall methodology. So before we can get information security assurance, we have to invest a lot of time, effort, and resources, and wait for all these different stages to be completed. This brings a number of different problems, and to be quite frank, these problems have been highlighted with the inability of information security to keep up this pace that software development is moving at. Some of the friction that is presented includes security being an afterthought. Security "sign off" delays projects, or that's the perception. These two points can go together. If security is not taught about up front, it can no doubt delay projects when they're nearing their release dates because issues are identified too late in the project. Another major friction point is that security is a once‑off point‑in‑time assessment. There is no continuous security built in when we approach it with a traditional waterfall methodology. The cost of re‑testing can be very high, and then security gets known for being too slow and impeding the progress and the pace at which companies need to deliver product. Another major factor causing friction is that there is not enough skills available to be securing the software development activities. Security experts are in high demand and short supply. The ratio of security experts to development experts is extremely low, causing significant gaps in secure software development. So if these are all the problems that currently exist, could we apply some of the DevOps thinking to produce DevSecOps, and would this address some of the problems we're experiencing. Dev and Ops combining to become DevOps are focused on one goal, and that is producing working software. In a true DevOps environment, the friction between development and operations teams has been removed by integrating the two teams' activities into a single lifecycle. Can we do the same for security and development to produce DevSecOps?

Devops plus srecurity

The best way for us to answer this question is to look at a real‑world example. For this example, I'm going to use the PCI DSS standard, which is the Payment Card Industry Data Security Standard. Version 3.2.1 has a requirement under clause 11.2 to run internal and external network vulnerability scans at least quarterly and after any significant change in the network. This is a longstanding requirement for the PCI requirements, and anyone operating in a PCI‑certified environment will be very comfortable and used to running vulnerability scans on a quarterly basis. But what if we changed our tanking? You recall in the previous slide we had a movement through different stages in the lifecycle from left to right. You may have heard of the term shifting left when it comes to DevSecOps. What that means is can we move the processes and activities further left to happen earlier in the lifecycle? Looking at the traditional way that this PCI DSS requirement would be achieved, we would initiate a vulnerability scan once per quarter, have a human review the results, and then retest to ensure any high‑risk vulnerabilities were addressed. If we were to shift left for this particular PCI DSS requirement, we would undertake vulnerability scans as part of our software release pipeline, and we would not release if a high‑risk vulnerability was identified. We're not constraining to once per quarter. We could do this once per week or once per day, depending on the frequency of our releases. When we embed activities such as vulnerability scanning as part of our release pipeline, we don't have an extra job every quarter to undertake. This is shifting left, but also automating our security practices. Now let's look at introducing security into the DevOps cycle to produce DevSecOps. Starting with the planning phase, we can introduce threat modeling and code standards, starting by embedding security at the planning stages of a project, shifting left. Then as code is written, we can undertake static code analysis, software composition analysis, and other types of code analysis techniques. This will catch certain amounts of vulnerabilities in code as it's being written or committed to the code repository. Once the software is built and ready for testing, we can immediately run vulnerability scans before we release it to the testers. Any major vulnerabilities can be addressed before the testing function spends their time testing software that may have major vulnerabilities in it. During the testing phase, we can run penetration testing, something that's traditionally run at the end of a lifecycle. During release, we can look at the compliance validation to ensure that the software complies with any regulatory or industry standard required. As we deploy the software, we can sign it cryptographically to ensure the integrity of the software as it's released. Moving to an operate and monitor phase, we can monitor and detect, respond and recover from security incidents. Overlaying the security practices on the DevOps lifecycle means that we can scatter the different processes throughout various phases, and we can work in tandem with the development and the operation stages. All of this encompasses better security, visibility, and control.

Security as code 

If you've watched any of my other courses, you know I'm a big fan of using examples to help us with our learning. Let's look at another example of where DevSecOps can help with traditional security requirements. In this case, we're going to look at what are the benefits of converting security practices that may be traditionally manual activities into automation, code‑based activities, so basically having our security written in software as code. There are many benefits with automating security practice. For example, code reviews could become code previews where we have automated code checkers or code being checked at commit stage so that it never gets turned into a built application that gets deployed. If we can catch the coding errors early, then it means we can reduce our overall security costs and development costs. Patching is a very common practice in good security hygiene environments. Typically, we would patch on a regular basis, either monthly or some other regular interval. However, what if we eliminated the need to patch our environments entirely? What if we were able to automate the building of environments and every new environment that was built came with all the latest patches included? So, we're effectively replacing patching as an activity by automating the building of environments. Final example is where we can replace incident response or maybe minimize it if we can't entirely replace it by increasing our time on threat modeling and other types of scenario‑based activities. If we can use threat modeling as part of our design, we may be able to minimize and reduce the number of incidents we have to deal with. Although not a fully‑automated practice, there are many tools available to help us with a lot of the heavy lifting around threat modeling. These are just some examples of how we can, again, shift left with using security as code and moving our security processes to occur earlier in the lifecycle.

CI/CD pipeline assurance 

The last topic in this module that I want to cover is in relation to continuous integration and delivery you may notice as CI/CD pipelines. They're called pipelines because they are typically a sequence of steps that occur in sequential order, usually highly automated. There are a number of different components associated with every pipeline. For instance, you will have source code as one component in the pipeline where all your code management repositories will be located. Developers will commit and pull code for modification from these repositories. From there, once source code is ready, software will be built, unit tested, and usually incur a certain amount of integration testing. Once all of this has been successfully passed, then the deployment takes place. There can be multiple different environments, including an acceptance environment, staging environment, and production where the software will live and operate for users. So you can see, there is a continuous integration pipeline and the continuous deployment pipeline. Let's now fit security on top of these different pipelines. Security design comes before the code is written, but once code is written, the security review of the code should be undertaken. Ideally, this should be an automated review. During the testing phase, security testing should be baked in an integrated part of the continuous integration pipeline. And finally, during the deployment, all deployments should be done securely by using automation to ensure that all production environments and other environments that are being deployed to are securely configured. What this approach gives us by embedding automation for security into the pipelines is a repeatable and consistent approach. Security becomes embedded in the pipeline. It's no longer an afterthought. And remember the friction points we discussed earlier where security was frequently being labeled as being too late and delaying projects. Well, that friction disappears. More than anything else, this gives us a sustainable security process for your software development lifecycle. Let's move on and sum up this module.

Module Summary 

In this module, we looked at various aspects of traditional security approaches. We identified that there's many different friction points in applying traditional security to what is now a very fast moving software development environment. We looked at how DevOps has already resolved some of the issues in existence between development and operations teams. By adding security and following the same pattern as DevOps, we can create a DevSecOps environment and reduce the amount of friction that traditional security approaches create. We need to move our thinking away from point‑in‑time manual inspection towards security as code or scripting. This will ensure that security is consistent, embedded, and automated. It will also provide better outcomes for you. As you may have concluded, DevSecOps is all about embedding security within the software development lifecycle. If you're responsible for software development teams, then you're going to find the remainder of this course very relevant for embedding and improving your security practices. Up next, we're going to look at identifying the benefits of DevSecOps.

Identifying the benefits of devsecops

You're very welcome to this module where we dig a little bit deeper into identifying the benefits of DevSecOps. In the previous module, we covered the basic concepts of DevSecOps. Now that you have that foundation under you, we're going to move on and describe some of the benefits of using DevSecOps as an approach in your organization. This is going to be a short module, but a very important one. If you're looking to start a DevSecOps initiative or indeed looking to expand it and want to get support from your peers or your managers, we're going to cover off where DevSecOps is most appropriate. It may not be appropriate in every environment, and we're going to look at that in this module. We're also going to call out the benefits of DevSecOps, and some of these benefits may not be so obvious. Finally, we're going to close out this short module looking at the roles and responsibilities within a DevSecOps environment. So let's get started

Where devsecops appropriate ?

You might already be aware of some of the benefits of implementing DevSecOps; however, one thing we must keep in mind when we're discussing DevSecOps is that DevSecOps is more about a culture and high degrees of collaboration and not solely about specific tools or products that you implement. So, for example, if you are a developer in an organization and want to advance the security controls around code that you create, DevSecOps is definitely a good way forward for improving the security posture of your code. You may, of course, need to convince or at least discuss with your manager in terms of implementing any new practices. For the manager, there are definite benefits in implementing DevSecOps also. If you're part of the IT security organization, you will really want to pursue some of the automated and continuous security checking that DevSecOps brings. Maybe you're working in IT operations keeping all the systems running, or maybe you're just tired of dealing with security incidents. In that case, DevSecOps can really bring advantages to your workload. If you already work within a DevOps environment providing automation between the development and the operations part of solutions, then DevSecOps is a natural extension of that workload. And finally, if you're looking at managing the finance and costs associated with software development, you're definitely going to be interested in exploring the benefits of DevSecOps from a cost efficiency perspective. One thing that all these different types of roles have in common is they're all trying to look for a specific justification of DevSecOps in order to provide and build support for their initiatives. So no matter what role you fit into, there is always benefits you can accrue from DevSecOps. So where is DevSecOps most appropriate? There is no hard and fast rule saying that you must have certain methodology or lifecycles in place before you can apply DevSecOps, and likewise, there is no hard and fast rule to say that you cannot apply DevSecOps in certain environments. However, some general guidelines include where you have an Agile methodology, it is highly suitable to a DevSecOps deployment. If you have existing DevOps in place, DevSecOps is a natural extension to this. You may have some of the pipelines for deploying your solutions already in place. Extending those is going to be an incremental piece of work rather than some fundamental shift in your processes. If you're doing multiple releases per year, per day or per hour, DevSecOps is definitely appropriate to your environment. If you have some automation already in place in your development lifecycle, then you already have the railway tracks laid out and you can plug some of the DevSecOps controls into this pipeline. Where you're working under a different type of structure and using a waterfall methodology, I'm not saying that you cannot deploy DevSecOps in this environment, but you may need to consider carefully how best to fit the practices of DevSecOps into that environment. If you work in a highly regulated environment that requires significant changes to any improvements or variation on the software development lifecycle, then this may not be suitable for DevSecOps. If you're only doing one or two releases per year, you may not get the payback that DevSecOps promises. So this is something you need to give careful consideration to. And finally, if you have no automation in place in your current environment, I would recommend you start with building out some automation and starting with the building block of DevOps before you look to move into DevSecOps. I'm not saying that it's not possible to undertake DevSecOps with the environments listed on the right, but there are other considerations you need to factor in in doing so.

A typical business case 

Let's use another example to help us to understand this time the benefits of DevSecOps. In this case, we'll help Ben. Ben is a software developer. He's been working with a fictitious company called Globomantics for three years now. He wants to introduce DevSecOps practices into Globomantics. We will help Ben to identify the benefits of DevSecOps so that he can get support from his manager and his peers for this initiative. So let's talk about Globomantics. They're an organization that uses an Agile methodology for their software development. They have some basic DevOps practices in place, but this is largely around deployments. Someone automated and scripted the deployments. They are open to trying new initiatives, but they have limited resources. Does that sound familiar? Does it sound like the organization you work in? They also have tight release deadlines, and this is mainly due to new successes and new customer wins they've had in the marketplace. Now that we've framed Globomantics and Ben, who works as a software developer within the engineering team, let's have a look at the typical benefits that could accrue to this scenario.

Listing benefits of devsecops

One of the most obvious benefits is that DevSecOps can reduce time on rework for security vulnerabilities. In order to help you to quantify this for your organization, we need to understand how it achieves this time reduction. Time can be saved when security issues are found within the context of where these security issues are being introduced. What do I mean by that? Well, if you have a developer who has just committed or checked in code to the code repository, and that code has a vulnerability in it, it will be far quicker for that developer to make an amendment and fix the bug in the code shortly after they've committed it to the code repository in comparison to if they have to come back weeks later and remember the code they were working on. So security issues found earlier in the lifecycle reduce the cost of remediation and rework. All that memory and brain power required to recall the code you were working on is no longer required. What this means is that more vulnerabilities will be addressed because it's easier for the developers to action them more quickly. Reducing risk for Globomantics as an organization and as customers is a real benefit. If security issues are remediated close to their point of generation, then there is a higher likelihood that the security issues will be remediated in the first place. We all know about customer requests and feature requests for products that end up on a backlog and take weeks, months or years to even get prioritized, never mind implemented. Security vulnerabilities are bugs and feature requests also. It may take weeks, months or years for some vulnerabilities to get fixed. The time it takes for vulnerabilities to get fixed is a risk exposure, both for the organization and its customers. I said earlier in this course that DevSecOps is not all about security tools or processes, but is very much about collaboration, and this is very true. It's one of the spinoff benefits of implementing DevSecOps. When DevSecOps has done well, you don't have a security team that fixes every security issue. Security becomes everyone's responsibility. The engineers that write the code must fix the issues that they have introduced. The operations people must ensure that all systems are fully patched before the deployment can take place. All of this just forces security to become everyone's issue and increases the level of collaboration. Another major benefit of DevSecOps is the introduction of more consistency or standardization through the automation that DevSecOps requires. By introducing automation, we're ensuring that consistent security scans are run at the right time in the lifecycle. We could call this continuous security because these are not discretionary steps. The minute someone checks in code, the security scans will kick off. The minute an application is built for deployment, further security scans will kick off. All of this is hands off, it's automated, meaning that you get consistency throughout the stages within the software development lifecycle. All of this leads for better compliance levels, and this has never been more important. Secure by design is a requirement by law in some jurisdictions. We have seen that the GDPR laws, the General Data Protection Regulation, requires secure by design and privacy by design to be implemented as part of development lifecycles. Other requirements from laws such as the California Consumer Privacy Act are also stipulating requirements within the development of software. Better automated security testing helps compliance with various laws.

Quantifying benefits- an example

But let's assume that Ben's boss is quite meticulous about seeking a business case for any new work or changes to the process. So let's dig into the first item here by way of example of quantifying the true benefits for Globomantics by introducing DevSecOps. We have suggested that there will be a reduction in the time required for rework or fixing security vulnerabilities as a result of introducing DevSecOps. Ben's manager may challenge us and want to know more specifics in terms of the true benefit. What we can do in this case is look at the classic bug cost diagram that has been around for decades. NIST, The National Institute for Standards and Technology, in the US and other research houses has done some significant research into this area. The findings from the research indicate that the longer it takes for a bug or issue to be identified in the code, the more it costs. It is far cheaper to identify and remediate bugs in the design and requirements phase than it is when the product is in production. Security vulnerabilities are just bugs. These are bugs that need remediation, so the same model applies to security vulnerabilities. The longer you leave it to identify and remediate security vulnerabilities is going to cost you significantly more to remediate these issues. DevSecOps is all about shifting left, moving the security testing to be as early as possible in the lifecycle. By doing this, we avoid the expensive deferral of security vulnerability remediation costs. This is a clear business case to make for your manager and your team to help you introduce DevSecOps apps in your organization.

Roles and responsibilities 

I want to talk for a moment about the various roles and responsibilities involved in building a DevSecOps pipeline. There's a number of different roles or activities I've identified here. It's important to understand that these are activities that need to be undertaken. They are not necessarily full‑time roles or team members that you need in your organization. This is particularly important if you've got a small development team. DevSecOps does not mean you need to double or triple the size of your team in order to be successful. What's more important than the size of your team is the activities that your team members perform. Tooling is important because the tooling is central to ensuring consistency and effectiveness of the DevSecOps processes. You extend the DevOps pipeline by including security testing and integrating with various security tools. Vulnerability management is another key activity that must be fulfilled, ensuring that all vulnerabilities are managed, blocking deployment in accordance with your company's risk policy. For example, if you have critical or high vulnerabilities, you may prevent the deployment from going to production. Looking at application security, it is important that you have someone competent with dealing with the application security fixes. And finally, from a compliance point of view, it's always important to ensure that you maintain proper records of the testing and the outcomes of any remediation that are undertaken. This will help you with demonstrating compliance. You should look at all of these activities and see how you can fulfill them within your existing team structure. For larger, more mature organizations, you may have the privilege of being able to hire specialists who are dedicated to each of these activities.

Module summary 

Now that we've completed the benefits of DevSecOps, let's sum up this module. We've looked at how best to leverage your existing DevOps process and extended it to include security processes. Using this incremental approach, it can be the fastest way to get the basic DevSecOps process up and running. It's important to communicate the upside and the benefits of DevSecOps. Remember, you need to involve many people and collaborate within your organization because DevSecOps is more of a culture than a dedicated team that's only focused on security. A lot of effort will be required to ensure that everyone buys in to the DevSecOps culture. And that completes our two theoretical modules on DevSecOps in this big picture course. Up next, we're going to look at more specifics on how we adopt your development lifecycle to incorporate DevSecOps. Let's have a look at that now.

Adopting DevSecOps in your software development lifecycle 

Module Overview 

Hello, and welcome to this module in our DevSecOps: The Big Picture course. In this module, we're going to look at adopting DevSecOps in your software development lifecycle. As we continue our DevSecOps journey, we've already established our foundational knowledge. We've identified the concepts of DevSecOps and also itemized some of the benefits that applying DevSecOps can bring to your organization. This module can be seen as sort of an on‑ramp to where we get into more of the technical specifics around the practices and processes you need to implement in your development lifecycle. In this module, we're going to look at positioning DevSecOps within the development lifecycle. How is your lifecycle going to be impacted when you do introduce DevSecOps practices? We're also going to look at DevSecOps from a maturity perspective. You may be starting a new initiative in your organization and trying to introduce the concept of DevSecOps and bring some supporters along with you. There is various different levels of maturity. We're going to touch on some different models that you could use to help you gauge where you are in a maturity model, but also to give you some encouragement that you don't need to have all the processes in a perfect DevSecOps environment in place from the start. Remember, making progress is more important than aiming for perfection at this stage. So let's get started.

Positioning DevSecOps within  the SDLC

We walked through this slide in a previous module. What I want to do here is take you step‑by‑step through a strategy for helping you to succeed in rolling out your DevSecOps processes. It can be overwhelming. There's lots of different phases within the development lifecycle. If you already have a product that's being shipped to production regularly, people are going to become quite nervous if you suggest making amendments to that build and deployment process, so let's have a look at how we can approach this with the greatest caution, but also being pragmatic. Starting at the planning phase, we could look at the threat modeling and coding standards and try to influence developer activity by introducing these concepts. However, most organizations should have coding standards in place. In this context, we're talking about secure coding standards. Threat modeling is something that requires some investment in time for training and experience before you can become really effective. For that reason, I would be suggesting you leave threat modeling until a later phase in your rollout. Let's move on and look at what we can do in the code section. Where code is being written, it's definitely a good opportunity to get involved. Winning over the developers is always important when you're interfering with their build pipelines. Undertaking some static code analysis can be really valuable in this phase. I suggest you start with taking some static code analysis tools and getting them integrated with your build pipeline. Developers will love hearing feedback about vulnerabilities in code they've checked in. It's far better than hearing about it just before you deploy something to production. Again, start with non‑blocking changes. Don't try to break any builds at this stage. Once you've mastered the code analysis section, moving on to the build activity, it would make sense to introduce some vulnerability scanning on the build code. There's many different scanners available, and these can be very non‑intrusive, capturing vulnerabilities before the system goes to test and before it gets released to production. I would focus on this area to start your DevSecOps rollout. The other phases in the lifecycle have many other different types of activities that can come in time. Don't be trying to attempt to deploy all of these activities at the same time. Start on the left‑hand side and gradually mature your way over to the right‑hand side to cover all aspects of the lifecycle.

DevSecOps progress and maturity models 

One thing you may have realized by now is that implementing a successful DevSecOps process is not a fast and immediate task. It's more like a marathon than a sprint. One of the techniques I really like to apply when implementing a program for change is to look at it in stages. Firstly, you need to invest significant amounts of effort to make change in your organization, but a very commonly overlooked aspect of implementing sustained change is to ensure that you allow time and monitoring to lock in that progress. Very often progress can be lost over time by people falling back into previous practices and not sustaining your new improved practice. Always follow any improvements by a period of embedding to ensure that your new practice is sustained and becomes part of the culture of your organization. There are models to help you with looking at the progress of your initiative over time. These are called maturity models, and aim to map how developed or mature your process is. There are many different maturity models applicable to different areas of our industry. I have called out two here, the first one being the DevSecOps maturity model from the DevSecOps Foundation, and you can go and look at the detail of these specifically if they take your interest. I am not prescribing one over the other, or indeed, that you have to use any of these models, but I do recommend aligning yourself to some maturity model, even if it is a custom model to suit your organization. By way of example, on this model, there are five different tiers or levels ranging from Insanity all the way through to Continuous. These different levels are rated over a number of different categories. In this model, it's Culture, Skills, Program or Outcomes, and Security Priorities. You can get more specific detail on this model at the link at the end of this slide. Another model I like is the OWASP DevSecOps Maturity Model. This covers far more granular detail in terms of measuring your development and maturity in various different areas. I have summarized these into Build and deployment; Culture and organization, you'll notice culture comes up quite a lot for DevSecOps, as it is very much a cultural change; Information gathering; Patch management; and Test and verification. This model has four different levels, which rate the understanding of security practices from Basic through to Advanced, so it's quite easy in terms of measuring the different aspects of this maturity model. You can find out more about the OWASP DevSecOps Maturity Model at the link below. Whatever you decide, I do suggest you identify one that you're comfortable with. There is no hard and fast rule that you must align to any one specific model. Select the model that suits you.

Module summary 

This module was our lead‑in to more specific details on how to implement DevSecOps in the upcoming modules. In this module, we looked at strategies to commence your DevSecOps rollout. Don't try to deploy all the tools at once. Remember, focus on progress. Don't aim for perfection straight off. As you develop and roll out your DevSecOps across the entire life cycle, it's useful to pick a maturity model that suits your organization. This can help give you a roadmap for where you should implement improvements into your DevSecOps processes. Remember, don't be afraid to customize whatever maturity model you select, making it your own. Now that we've set this context, let's move on and look at designing DevSecOps for the planning, coding, and building phases of the software development lifecycle.


Designing devsecops for plan, code, and build sdlc phases 

modules overview

Hello, and welcome to this module in our DevSecOps: The Big Picture course. In this module, we're going to be looking at designing DevSecOps for the Plan, Code, and Build phases of the software development lifecycle. You've made great progress through our DevSecOps journey, already establishing the basic concepts, understanding the benefits, and learning how to adopt DevSecOps in your lifecycle. Now, we're going to drill into more specifics around the activities required for designing DevSecOps into the planning, coding, and building phases of the lifecycle. We're going to look at tools and practices you should implement during the first three phases of the DevSecOps lifecycle, which are planning, coding, and building software. Let's get started.

Threat modeling 

I just want to remind you of this diagram, which we saw in a previous module. There are many different stages or steps in a DevOps development lifecycle. We extend the DevOps lifecycle by adding some security stages along the way. This helps us to create the DevSecOps lifecycle. By way of reminder, during the planning phase, we're going to look at threat modeling and coding standards. After that, we're going to look at static code analysis, and I'll explain the difference between SCA and SAST, and then we're going to look at the third phase, build, and how vulnerability scans can be important in this phase. Let's move on and look at threat modeling. Wikipedia defines threat modeling as a process by which potential threats, such as structural vulnerabilities, can be identified, enumerated, and prioritized, all from a hypothetical attacker's point of view. So what we're trying to achieve during the threat_modeling phase is to foresee weaknesses in the application that an attacker might be able to exploit. We're trying to get inside the mind of the attacker and build our system to be more robust to defend against these weaknesses. There are many different threat‑modeling methodologies; one of the more common is known as STRIDE, which is an acronym for spoofing, tampering, repudiation, information disclosure or leakage, denial of service, and elevation of privilege. These are all different threats that you should assess your application to ensure it's robust enough to defend against them. When attempting to threat model, it's always better to use some specific tools that are designed specifically for the process. It's quite difficult to do manually. Implementing tooling can really help you identify the threats quickly. One freely available tool, which is still being maintained, is the Microsoft Threat Modeling Tool. I've provided a link for you to download this tool. Depending on the type of solution you're building, the Microsoft Threat Modeling Tool will propose a number of different threats that you can assess your application under to determine whether it's robust enough to defend against these threats. In another one of my courses in the Pluralsight library, Getting Started with Data Loss Prevention, I dig deep into the Microsoft Threat Modeling Tool. I'll show you how to download the tool and even work through a case study where I prepare a threat model. Please go ahead and check out this course, and specifically, the Threat Modeling section to get more in‑depth information on how to prepare a threat model.

Security Code Standards 

You may already be familiar with the concept of coding standards. These are rules or patterns that are documented to ensure every developer on the project performs consistent naming conventions and programming style. The aim of coding standards is to make the code more consistent and easier to follow, therefore reduce the number of bugs that might be present in the code. Secure coding standards is similar; in this sense, it's trying to improve the code quality, in this case, reducing the likelihood of vulnerabilities or weaknesses being present in the code. Secure coding standards are language specific. So if you're using Java or Perl, or another programming language, you need to source secure coding standards that are specific to that language. The Software Engineering Institute in Carnegie Mellon University has defined their top 10 secure coding practices. These patterns can be used in conjunction with coding standards to make your code more secure. These can help you as you identify the most language‑specific secure coding standards. In fact, these practices should form part of your secure coding standards. The top 10 identified practices by the SEI include: validating input, defend against SQL injection and other types of attacks; heed compiler warnings. The compiler warning is there to give you feedback at build time to ensure you improve weaknesses in your code. Architect and design for security. It's far easier to make an insecure system than it is to make a secure system. So proper architecture and design decisions, factoring security in early in the process is essential. Keeping it simple is an often overlooked practice, but if something is too complicated, it's going to be very difficult to find weaknesses or vulnerabilities in the code. Default deny. Don't provide excess permission or access. Implement the process of least privilege. Both of these items go hand in hand. Access should only be granted on a least‑privilege basis. Sanitize data from other systems. In other words, don't trust any input, whether that's from a person or another system. Practice defense in depth. This is essential in creating a robust system. If one safeguard fails, you have another one to protect the system. Practice effective quality assurance. Quality assurance is essential to identify bugs early in the process. Having quality assurance testing that also is aware of security testing is an added bonus. And finally, as I have mentioned, adopt a secure coding standard, and make sure that all your development team complies with this standard. These are some of the key practices that will keep your code safe, and they're part of the initial plan for your DevSecOps lifecycle.

Static code testing (SAST) and sca

Now let's move on to the coding section of the lifecycle. I want to clarify something that confuses many people. We have some acronyms, S‑C‑A or SCA, and S‑A‑S‑T or SAST. These are regularly used at this stage in the lifecycle, but they are two very different activities. SAST, or static application security testing, is something that is a process that examines source code to identify weaknesses that can lead to security vulnerabilities. Think of it as a source code review and testing of the source code itself. SCA, or software composition analysis, is quite different. This is a process that looks at the open‑source components that make up your software, and checks all of these components against known vulnerabilities. Most software is comprised of some parts that are open source today. In fact, the volume of open‑source components in software today is increasing. So software composition analysis is growing in importance. The open source elements of your code are compared against publicly available vulnerability databases to identify if any of your open‑source components have vulnerabilities that need remediation. This can help avoid problems later and is always good to get done early in the lifecycle. Let's look at some of the features of SAST. SAST involves reading source code. Similar to coding standards, it's language specific, so you will have to get a language‑specific scanner for the programming language that you are using. Very often, there can be false‑positives presented, so you will have to factor in time for assessing the findings of a SAST scan. False‑positives arise because there isn't a whole lot of context available for the scanner. All they have to work with is the raw source code. SAST can be fast and automated, and integrated into your build pipeline. It finds weaknesses early in the process, which reduces overall costs for remediation. NIST, the National Institute for Standards and Technology, has published a list of source code security analyzers, which you can find on the link below. There are some open source and some commercial products listed.

Vunerability scanning 

The next stage in our lifecycle is build. This is where the source code is taken and compiled into built code. At this point, it's good to look at vulnerability scanning against the build software. I have already spoken about one technique for undertaking vulnerability scans and that's Software Composition Analysis. This will come in at this stage in the pipeline. Another practice is called DAST, or Dynamic Application Security Testing. Vulnerability scanners run on the completed or compiled code. There are many different scanners available, and all of these will compare your code against known vulnerabilities. As you can see now, we're looking at security testing for DevSecOps under various different stages along the lifecycle. We're looking at planning, before any code is written, then we look at testing the code itself as it is being written, and in this stage, we're looking at the code once it has been compiled. What you're getting is the benefit of testing in‑depth, or at various stages as the software matures from concept to delivery.

module summary

In this module, we have outlined the various security checks on your software as it progresses through the delivery lifecycle. This includes security checks at design time, before any code is written. And once code is being written, we then introduce source code testing. Once the compiled code is available, we look to run vulnerability scans. This is providing extensive depth, in terms of testing the security of your software. Next up, we're going to look at designing DevSecOps for test, release, and the operate phases of the software development lifecycle. Let's have a look at that now.

designing devsecopd in your software development life cycle

module overview

Welcome to this module in our DevSecOps: The Big Picture, Pluralsight course. In this module, we're going to look at designing DevSecOps for test, release, and operate phases of the software development lifecycle. We continue building your knowledge of the concepts for DevSecOps, and in this penultimate module, we're going to focus on the last remaining phases of the software development lifecycle, where we focus in on the DevSecOps requirements for the test, release, and operate phases.

devsecops in the test phase 
As we move into the later phases of the DevSecOps lifecycle, we focus more on testing the finished product. We have already completed analyzing the source code, so now we're going to move on to testing the built product. This will include penetration testing, validation, and also looking at what automation can be put in place to help the deployment and operations of the system. There are a number of different types of tests that can be undertaken only on the built software. These include penetration testing. This is a manual test, which attempts to compromise the security of the system using manual attack techniques. Although not automated, it is still an essential part of every delivery, including automated tasks that we've discussed in the previous modules, will lighten the load for any penetration tester, and increase the speed of which manual penetration testing can be completed. Other types of security testing include load testing, to test whether or not the system can defend against distributed Denial‑of‑Service attacks. Fuzzing is a term used for basically bashing the system with various different types of input to see, can you generate unexpected responses for the system, which may cause a security incident? Wikipedia defines fuzzing, or fuzz testing, as an automated software testing technique that involves providing invalid, unexpected, or random data as inputs to a computer program. The program is then monitored for exceptions,, such as crashes, failing, built‑in code assertions, or potential memory leaks. Typically, fuzzers are used to test programs that take structured input. Finally, this is one of the first stages in the lifecycle that multiple components come together to work as one, so a level of integration security testing is needed to have confidence that there isn't a gap in security when everything is put together.
devsecops in the deploy phase
Once we move on to the deploy phase, there are different types of testing we need to undertake. Two types of testing that are common at this phase are testing security certificates. This ensures that all transport security‑layer certificates are valid, indeed, and issued correctly. This is required to be done at this phase because normally, different types of certificates are used in different environments, and we need to make sure the certificates used in your deployed environment are valid and correct. Application hardening is also something that should be decided upon during the deploy phase. Reducing the attack surface area by hardening your servers and applications is an important task. There are many different tools available for checking the configuration of your certificates. I've run SSL Labs testing on my own website, richardharpur.com. As you can see here, a grade is given based on the checks that the Qualys SSL Labs site undertakes. In relation to hardening, it is important to understand that the more that you harden your application, the less attack surface area is available for attackers, thereby reducing the likelihood of a breach. With the Amazon Marketplace, there's a number of readily available, pre‑hardened images that you can use. So, instead of just selecting a standard Linux image, you could select the CIS Red Hat Enterprise Linux 7, hardened image, or a Microsoft Windows Server 2016, hardened image. These images are hardened to the CIS hardening standard, and this means that the attack surface area for these images is greatly reduced. As part of your deployment scripts, selecting one of these hardened images provides you with greater safeguards in relation to vulnerabilities in your environment.
devsecops in the deploy operate phase
As you move towards the operate and monitor phases, the focus changes from ensuring the application is built robustly to checking to make sure that the application is not under attack, and when it is, that we know about it as soon as possible. We also want to make sure that there's no unauthorized changes made to the configuration of the environment. Compliance as code, which checks the configuration of the environment against approved baselines and eliminates any deviation from this baseline in near real time, is important for the operate phase. Verification and monitoring, and ideally continuous checking that everything is operating within expected norms, is another key aspect of the operate phase. If we flatten out our DevSecOps lifecycle into a linear representation, we can see that all along the pipeline there's many different tools required to ensure a successful DevSecOps implementation. It can become confusing with so many different tools available on the marketplace to understand exactly where these tools fit in in terms of the specific phase they're most suitable to. One useful reference guide is this periodic table of DevOps tools, which is regularly updated. You can visit this periodic table at the link I provided on the slide. It provides various different tools for different types of activities in the DevOps lifecycle. It also includes a list of security tools, which you can see on the bottom right‑hand corner of this diagram. This is a very useful guide to help you distinguish between the suitability of various different tools for the different phases of the DevSecOps pipeline.
module overview
Let's sum up this module. What we have learned is that it's very important to focus on automated checks. By utilizing automation, we can ensure we deliver quickly and consistently. We're not relying on manual and slow activities. Automation also helps us to scale, and given the skills shortage within the security industry, scaling is an essential attribute to factor into your pipelines. It's also critical to reduce the feedback timeline between identifying an issue and providing that information back to the originator of the issue. If you can provide a developer with feedback within hours of them writing a particular piece of code, they will be far better armed to make remediation changes to that code. It will be fresh in their minds. Finally, none of this is possible without using the appropriate tooling at each phase. It can get very confusing as to what tools are appropriate for each phase. I've provided you with resources to help you to identify the available tools and see what is most suitable for the phase that you're developing in your pipeline. Up next, we're going to look at some of the myths surrounding DevSecOps, and we're going to debunk them. Let's move on and look at this final module in our course.
Debunking devsecops myths
module overview
Welcome to this module in our DevSecOps: The Big Picture Pluralsight course. In this module, we're going to look at debunking DevSecOps myths, and as this is the final module in our course, we'll have some fun describing some of the myths that surround DevSecOps. Some of the myths we're going to discuss are whether or not you need special teams to implement DevSecOps, whether DevSecOps introduces delays in your development and deployments, and whether you can go out and purchase DevSecOps just to tick the box and achieve the capability. Finally, at the end of this module, we're going to wrap up this course, and I'll give you some pointers on where you can go to get future learning on DevSecOps. Let's get started.

common devsecops myths

In an earlier module, we introduced the DevSecOps Manifesto. Let's take a quick reminder of the Manifesto right now. The DevSecOps Manifesto emphasizes certain activities over others. It's not an absolute contract to say you can do certain things and cannot do other things, but prioritizes the items on the left‑hand side over those on the right‑hand side in this slide. This is similar to the Agile Manifesto approach that has been successfully adopted for years. Let's walk through some common myths that currently exist. You might remember in an earlier module we helped Ben to describe some of the benefits that DevSecOps could bring to his organization. Ben is a software developer. He's worked with the fictitious company Globomantics for over three years. He wants to introduce DevSecOps practices, and we helped him to identify the benefits of DevSecOps so he could get support from his manager and peers. In this module, we're going to help Ben respond to some myths his team members are raising about DevSecOps. Let's have a look at the first myth. We cannot introduce DevSecOps because we need a special team dedicated to DevSecOps. This is not true. Anyone who is presenting this as a barrier to introducing DevSecOps does not truly understand what the objective is with DevSecOps. DevSecOps is all about creating empowered engineering teams, taking ownership of how their product performs all the way to production, including security. This is a quote from Larry Maccherone, a highly experienced professional in implementing DevSecOps in organizations.
more devsecops myths

Another common myth is that the security team still needs to do all security checks for us. Someone presenting this argument is making the case that even when we finish our coding, we need to hand over the code to security teams for their analysis and checks. However, again, this myth is missing the point. By handing over security checks to another team, we're introducing delays and time lag into our agile process. Instead, we should ask the security team to codify their checks so that we can build them into our development process automatically, which has tremendous benefit for time and consistency. Another myth is that we don't have enough resources to do DevSecOps. Just go and buy a tool that does it for us. DevSecOps is not about a capability. It's about a culture. Buying a tool is not culture changing. Whilst tools are required to make DevSecOps possible and efficient, these tools supplement the existing development process and help you deliver DevSecOps, if you have the correct culture in place. Purchasing a tool will not give you DevSecOps on its own. You need to ensure that you're also developing a DevSecOps culture. Next up, DevSecOps will slow down our developers. This is a common myth presented by someone who has not been through DevSecOps implementation. Remember, DevSecOps is about empowering developers to ensure their products get to production with appropriate security built in. Traditional approaches to security require testing after developers completed their coding and before deployment to production. Because this is so late in the lifecycle, it takes longer to fix and retest software compared to identifying the issue at an earlier stage in the lifecycle, so DevSecOps can actually save time and increase developer speed. It also makes it much easier for developers to fix items closer to the time that they were introduced in the lifecycle, so this myth doesn't stand up either. DevSecOps will result in our developers giving up control and won't be able to plan. With DevSecOps, developers gain control by running security checks at the best possible opportunity to help those developers fix the issues quickly and easily. No longer are developers dependent on external teams and gain control of the work and schedule. So we can see that the direct opposite of this myth is the actual result of implementing DevSecOps.

module summary

So in summary, we have learned that DevSecOps is all about empowering developers, providing more consistency and more control to the development teams. And more than anything else, it's all about developing a DevSecOps culture, ensuring that security is part of the delivered product automatically. That sums up this module. I want to give you some pointers on where you can take all the concepts that you've learned in this big picture course and drill down into much deeper implementation, in particular around security testing and other types of security activity that go along with DevSecOps. And finally, I want to wrap up the module with a thank you. Let's have a look first at where you can learn more. Congratulations on completing this DevSecOps big picture course. In this course, we covered the main concepts of why DevSecOps is a good idea and what the benefits are. However, in order to drill down further, there are a number of other courses that you can take to get into specific details on DevSecOps implementation. In the Pluralsight library, select the learning Paths tab and then Security category. Search for DevSecOps to bring up the Pluralsight path on this subject. Also, you can search for my name in the Pluralsight library and take any one of my other courses in the library. I have a number of courses in the library already, and you can also follow me on Twitter @rharpur or visit my website richardharpur.com where I post blogs from time to time on the subject of information security. But more than anything, I'd like to say a heartfelt thank you to you for taking this course and spending the time learning about DevSecOps. Hope to see you soon on another course of mine.

Introduction to information security within cloud computing 

Hi, everyone. My name is Lyron, and welcome to my course, Introduction to Information Security within Cloud Computing. I am an international cloud security consultant, author, and trainer at Profabula. The growth of technology, tools, and services in the cloud is exponential, and you can see evidences of this in your cloud management console daily. With an expanding array of cloud services and technology offerings, it is easy for the inexperienced to lack awareness of security functions in cloud offerings. In this course, we are going to identify and select secure cloud services based upon business requirements. Some of the major topics that we will cover include getting familiar with and using a common taxonomy to describe cloud computing, how to use those definitions to address business requirements through the selection of appropriate deployment in service models, and the use of a tool called the Cloud Control Matrix that will allow you to map common cloud security controls with regulatory and legal frameworks. By the end of this course, you will have the knowledge in order to select a secure cloud service that meets business requirements. I hope you'll join me on this journey to learn how to identify and select secure cloud services with the Introduction to Information Security within Cloud Computing course, at Pluralsight.

Defining cloud computing and essential characteristics

Thank you for joining me in the Introduction to Information Security within Cloud Computing. My name is Lyron. Let's get started. For those who are newly initiated to the cloud, the definition of the cloud can be vague and even a bit ethereal, like a cloud. For those who've been in technology for decades before the advent of the cloud, they can sometimes mistakenly assume that we've always had the cloud. It's just a different name now. Well, while that is not wholly true, there are definitely characteristics of technologies and services before the cloud that are exemplified in the cloud. A brief discussion of these will be helpful. Having a base or fundamental definition of the cloud will allow you to develop a roadmap for consumption that has security as it's essential base. In order to get to that definition, we will make use of leading canonical documents from the National Institute of Standards and Technology and the International Organization for Standardization and the International Electoral Technical Commission. What was before the cloud? Almost everything that we experience as a human man‑made thing was not spontaneously created from complete original thought. Think about it with one example, the smartphone. It was inspired by the cell phone, which was inspired by the phone in the home, which was inspired by the simple make‑break transmitter from almost 200 years ago. It was a generative knowledge and not spontaneous. So is the cloud. So for a moment, we'll consider some things that predate cloud computing that are part of the characteristics that are relevant to our modern‑day cloud computing. The first one is colocation and managed services. A colocation is a data center facility in which businesses can rent space for servers and other computing hardware. Typically, a colo provides the building, cooling power, bandwidth, and physical security, while the customer provides servers and storage. This was a way of deferring the direct cost of maintaining and managing a physical building. The market for colocation activities has diminished greatly over the past decade, but the principles of this have carried over into the cloud. A managed service provider deliver services such as network application infrastructure and security via ongoing and regular support and active administration on customer's premises in their MSP data center. So this could be that they're delivering their own native services in conjunction with other provider services, or that they are a direct market for the consumer and core offerings that the consumer wants. This was a way of deferring direct costs related to maintaining a talent pipeline and full‑time employees and consultants. This also has a direct carry over to the modern‑day cloud. Another model that predates the cloud was the shared service model. The IT department, along with business leads, would assess the organizational needs of departments and develop a list of critically‑needed services that spanned multiple departments in the organization. They would then design the technology platforms that would meet those technical needs, and build out the infrastructure. In some cases, it was an act of remediation, where they would address the build out by re‑configuring our augmenting existing infrastructures. The objective was to control costs and not have wasted or underutilized capacity in capitalized systems, burning electricity and unused depreciation, while waiting for workloads that rarely came. It also garnered control over expenditures or outright shadow IT as related to technology purchases. In this consolidated model, the different department would have their own SLAs or OLAs, operational‑level agreements, describing the services that they paid for through a charge‑back model, where a payment was tracked in a general ledger. A mainframe is the central data repository or hub in a corporation's processing center. It's linked to users through less powerful devices, such as workstations or dumb terminals. The presence of a mainframe often implies a centralized form of computing as opposed to a distributed form of computing. Centralizing the data in a single mainframe repository saves customers from having to manage updates to money and keeping more than one copy of their business data, which increases the likelihood that the data is current. Early mainframe systems were housed in enormous room‑side metal boxes or frames, which is probably how the term mainframe originated. The early mainframe required large amounts of electrical power in air conditioning, and the room was filled mainly with I/O devices. Also, typically, customer sites had several mainframes installed, and most of those I/O devices were connected to all of the mainframe. During their largest period, a typical mainframe would occupy 2,000 to 10,000 square feet of a facility. Starting around 1990, mainframe processors and most of their I/O devices became physically smaller. While their functionality and capacity continued to grow, mainframe systems today are much smaller than earlier systems, about the size of a large refrigerator. While some large‑scale customers outright owned their own mainframe systems, others would lease the systems, and the service was measured in MIPS, or millions of instructions per minute, and this was how customers were charged based upon their use. Hypervisors are the foundation for the creation of virtual machines as guest operating systems that can consume hardware resources with the perception that they are the only system doing so. Later in this course, in future clips, we will differentiate between the two types of hypervisors that exist in virtualization. First though, let's explain some basics. Type II hypervisors are virtualization of a guest operating system in the form of an application or process. These hypervisors predate the cloud and were used on desktops and servers to emulate environments on traditional operating systems being dozens of gigabytes in size in support of necessary application environments. Type I hypervisors, also known as bare metal, are installed on as a kernel system on the hardware as a very small form factor system measuring in size of megabytes to a few hundred kilobytes. This is the hypervisor of the modern day cloud.

Standard definition of the cloud 

We should examine the standard definition of the cloud. We will actually examine two canonical definitions of the cloud that will aid us in establishing a common taxonomy or glossary of terms when referencing the cloud. The first is the elder of the 2 published in 2011. It is entitled The NIST Definition of Cloud Computing. The Digital Object Identifier, or DOI for it, is Special Publication 800‑145. The second was published in 2014, and it is called the Information Technology Cloud Computing Overview in Vocabulary, or ISO/IEC 17788. NIST Special Publication 800‑145 tends toward an economy and brevity of words to arrive at a definition of cloud computing. Whereas the ISO/IEC 17788 document is elaborate and expansive in its definition of the cloud. Depending upon the complexity of an organization and the strategy it chooses to consume cloud services, there could be utility in either definition and efficacy with using elements of both. Note the differentiating characteristics of the cloud according to NIST. Check out the term ubiquitous, on‑demand, network access, shared pool, rapidly provisioned and released. These are actually must haves if a person is to be consuming cloud services. ISO/IEC has in its preamble some other characteristics that will help you to see when you are in the cloud, network access at a scalable and to an elastic pool of shareable physical or virtual resources, the self‑servicing, these are all generic elements of being in the cloud that have some specific implications that we will see in the next clips.

NIST definition of cloud computing 

It's time for us to examine with greater detail the NIST definition of cloud computing. Before we discuss the application in use cases of cloud services, let's clearly define the five characteristics of cloud computing from NIST Special Publication 800‑145. I like to use the acronym BRROM to enumerate the characteristics. Broad network access is a ubiquity of accessing services, which can bypass traditional tools of the web like a browser. Think about how you call your car service. It's not through a web browser, it's a cloud app. It's also the type of tools and capabilities that can be used to access services like mobile phones, tablets, laptops, and workstations. Rapid elasticity means quick expansion of resources and service consumption, but also instantly relinquishing services no longer needed. Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward, appropriated and appropriate to any quantity at any time. For those of you that have worked in technology for a few years, there was a time not many years ago when it would take 2 to 3 months to implement a new server system from initial request to configuration. Now in the cloud, the same effect can be accomplished in minutes. In the past, once you bought that server, you could not return elements of the capacity that you didn't use, but in the cloud, you can release resources immediately that are no longer being used. With resource pooling, the provider's computing resources are pooled to serve multiple consumers using a multi‑tenant model with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. There is a sense of location independence in that the consumer generally has no control or knowledge over the exact location of the provided resources, but may be able to specify location at a higher level of abstraction. On‑demand requires minimal manual engagement between the service provider and consumer. A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human intervention with each service provider. It calls to mind the repeated seeing of the TV series Star Trek, when Captain Kirk calls the engineer of his spaceship and says, Scotty, we need more power. No need to call in the cloud model, you simply consume services directly from a list in a catalog presented as a web browser management console. Measured means that you have tracking and payment due only for services consumed. Cloud systems automatically control and optimize resources used by leveraging a metering capability at some level of abstraction appropriate to the type of service that the consumer needs. Resource usage can be monitored, controlled, and reported providing transparency for both the provider and the consumer. In line with rapid elasticity, measured service is a significant business driver that help an organization change their it costs from capital expenditures to operational expenditures, that is the outright ownership of systems to just simply leasing systems as need be. This can also allow an organization to only pay for the services that they use. Do these characteristics remind you of what was in existence before the cloud? NIST defines what are called three service models. IaaS, or Infrastructure as a Service, where you have raw compute storage and networking, and consumers are responsible for launching and maintaining their own operating systems, PaaS, or Platform as a Service, where the providers are responsible for maintaining the operating system, and the consumer configures a platform like databases above it, and SaaS, where the consumer makes use of particular pre‑configured software. The provider is responsible for managing the underlying systems. These three service models will be discussed with greater detail in the clips to follow. NIST finally speaks of four deployment models. There is private, public, community, and hybrid. These also will be reviewed with greater detail, but it is important to note that the five characteristics, the three service models and the four deployment models, comprise all cloud computing. You can categorize many different details and offerings within this basic definition of the cloud.

ISO IEC 17788: Definition of cloud computing 

As stated earlier, ISO/IEC 17788 adds greater granularity in its definition of cloud computing. To begin with, where NIST includes multitenancy as a possibility or a subset of resource pooling, ISO/IEC 17788 calls it out separately and specifically as a distinct 6 characteristic. So to cover ISO/IEC adds another M to the BRROMM acronym. ISO/IEC see also speaks of cloud capability types. There are three capability types. There is the application capability, which is a cloud capability type in which the cloud service customer can use the cloud service provider's application. There is the infrastructure capability type, a cloud capability type, in which the cloud service customer can provision and use processing storage or networking resources, and finally there is the platform capabilities type, it's a cloud capabilities type in which the cloud service customer can deploy, manage, and run customer‑created or consumer‑acquired applications using one or more programming languages and one or more execution environments supported by the cloud service provider. ISO/IEC 17788 also has cloud service categories. So, it includes the service models of IaaS, SaaS, and PaaS, but it adds additional ones that include Communications as a Service, where a cloud service category in which the capability provided to the cloud service consumer is real‑time interaction and collaboration. There is the Compute as a Service, where a cloud service category displays the capabilities provided to the cloud service customer, so that they can provision use of processing resources needed to deploy and run software. There is the Data Storage as a Service, a cloud service category in which the capability provided to the cloud service customer is the provisioning and use of data storage and related capabilities. And finally, there is the Network as a Service, where a cloud service category displays the capability provided for the cloud service customer in transport connectivity and related network capabilities. These are the same as NIST in an implied way, but not stated explicitly. It is extremely important to be sure that as a cloud consumer, your organization can understand its role in the consumption process. ISO/IEC calls out cloud cross‑cutting aspects that help to understand the imperatives related to consumer due diligence when it comes to selecting and working with the provider. The first one is audit ability. This is the capability of collecting and making available necessary evidential information related to the operation and use of a cloud service for the purpose of conducting an audit. Availability is the property of being accessible and usable upon demand. Another way of saying that is whenever needed and wherever needed. Governance is the system by which the provision and the use of cloud service are directed and controlled. Cloud governance is cited as a cross‑cutting aspect because of the requirement of transparency and the need to rationalize governance practices with service‑level agreements and other contractual arrangements that are developed in the cloud service customer to cloud service provider relationship. Interoperability is extremely important, especially in the multi‑cloud world in which we live. This is the ability of a cloud service consumer to interact with cloud service, and exchange information according to a prescribed method, and obtain predictable results. Maintenance and versioning refers to changes to a cloud service or the resources it uses in order to fix problems and to upgrade capabilities. Performance is a set of behaviors related to the operation of a cloud service and the metrics that would be defined in the SLA. Portability is another important cloud cross‑cutting aspect that requires good due diligence. This is the ability of the cloud service customers to move their data or their application between multiple cloud service providers at a low cost and with minimal disruption. Protection of personally‑identifiable information is protecting and providing assurance of proper protection and consistent collection and processing, and communication of the use and disposal of personally identifiable information related to cloud services. Regulatory has to do with industry‑specific requirements or statutory requirements. These requirements could come in the form of a law or of some governing body within a particular industry. Resiliency is the ability to provide or maintain an acceptable level of service while there is a fault condition. Reversibility is the process of the cloud customer retrieving their cloud service customer data and application artifacts from a cloud service provider, and that cloud service provider to delete it, and for the cloud service customer to re‑purpose it and use it where they select. Service‑level agreements should not be thought of as being an instrument of assurance. Service‑level agreements between the cloud service provider and the cloud service consumer is basically a document of measurement properties that show the commitments that are being made in order to exact a particular service. If that service is not received, then the consumer is able to get credits from the provider. The greatest level of assurance actually comes from the audit function.
Summary 

If we follow the evolutionary track of technology from the past, it may be instructive in helping us to be ready for the future and the certain evolution that is occurring and will occur with current technology and services. Which definition does it make sense to begin using in your organization, the NIST or the ISO definition of the cloud? Do you have a complex environment that calls for a great deal of differentiation of services? Then it sounds like ISO is what you need. Are you a typical consumer that needs a basic architectural roadmap for cloud consumption, and then build details on top of that? Then perhaps NIST makes more sense. It could be a combination of the two works best. Proper definitions make ready appropriate selections of cloud services, and more importantly, set up the path for us to have a cloud experience that is architected and designed around security.
Understanding cloud depoloyment and service models 
cloud deployment models 
Next, we will look more closely at the cloud deployment and service models. As we get into this clip, we will begin reviewing the description, benefits, and responsibilities of cloud deployment models and how they would fit in to a selection criteria. Then we will define the service models, the corresponding responsibilities with them and business drivers to consume a particular type of service. We will then close out that discussion with looking at threats. Finally, we will look more closely at Cloud Security Alliance's logical model. Let's get into the four deployment models of cloud computing. The first one that we will take a look at is the private cloud description. It's important to note that a virtual private cloud, VPC, is not the same as a private cloud. A VPC is simply a representation of network‑based controls that allows public cloud consumption to be done in a logically blocked setting from other public cloud consumers. A private cloud consists of computing resources used exclusively by one business or organization. The private cloud can be physically located at your organization's onsite data center, or it can be hosted by a third‑party provider. In a private cloud, the services and infrastructure are always maintained on a private network, and the hardware and software are dedicated solely to that single organization. In this way, a private cloud can make it easier for an organization to customize its resources to meet specific IT requirements and other strong regulatory practices that may require high levels of privacy. With the private cloud, control is maintained over data, and the underlying systems and the applications. If there is a real or imagined heightened need for security, that need would be met, which is an organization maintaining a very low‑risk appetite, being comfortable with clouds that are private. It could be an organization that is heavily regulated or that comes under strong jurisdictional controls over where data is located. Private cloud responsibilities are the consumer or provider could manage the private cloud. Just recall that if the provider is managing it, then no one else is using that equipment or any of the services. The provider or consumer could own it. It doesn't matter if it's owned by the consumer or owned by the provider, or could possibly be both. The infrastructure could be on or off premise, and there is exclusive access by a single organization. Public clouds are the most common way of deploying cloud computing. The cloud resources, like servers and storage, are owned and operated by a third‑party cloud service provider and delivered over the internet. Microsoft Azure, AWS, Google, Box, Salesforce are examples of public clouds. With a public cloud, all hardware, software, and other supporting infrastructure are owned and managed by the cloud provider. In a public cloud, you share the same hardware, storage, and network devices with other organizations or cloud tenants. You access services and manage your account using a web browser. Most services that we use today, think of web‑based mail, online Office applications, storage, and testing, and development environments are all in public clouds. With a public cloud, your service provider provides the maintenance. There is an illusion of infinite resources and on‑demand resources available to meet your business needs. There is a vast network of servers that ensures against failure. The responsibility of the public cloud are as follows. The provider manages it exclusively. The provider owns the infrastructure exclusively. The infrastructure would always be located off‑premise from the consumer, and there is multitenant access. With a community cloud, here we have a shared platform, along with some common data or information with multiple and possibly disparate organizations that have an exclusive membership. While there are characteristics like a public cloud, it is not public. Governance extends to the relationships with those members of the community, not just the provider and the consumer. The characteristics are a mix of how you would approach public cloud and a hosted private cloud when it comes to governance. It can be a combination of disparate organizations with the need to share this common information that's actually driving that mixture of characteristics. The drivers for a public cloud would include having tools of governance and contracts that would have some economies of scale of a public cloud provider. You would also have an environment that's configurable based on community consensus, as with the hosted private cloud. This also includes community membership relationships, financial relationships, and how to respond when a member leaves the community. With the community cloud, the responsibilities are the consumer or the provider manages the cloud, provider or consumer owns the infrastructure, the infrastructure could be located on or off‑premise, or even both, and there is exclusive access by member organizations. Finally, let's look at the hybrid cloud. It's often called the best of both worlds, where hybrid clouds combine on‑premises infrastructure or private clouds with public clouds, so organizations can reap the advantages of both. For instance, you can use the public cloud for high volume, lower‑security needs, such as we‑based email, and the private cloud for other more sensitive business‑critical operations, like financial reporting. It also allows for a cloud bursting. This is when an application or resource runs in the private cloud until there is a spike in demand or over subscription, at which point the organization can burst through to the public cloud to tap into additional computing. resources. In a hybrid cloud, the organization maintains high levels of control, as in a private infrastructure for sensitive assets. The flexibility is increased. You can take advantage of additional resources in the public cloud when you need them. It is cost effective in that the ability to scale to the public cloud, you only pay for the extra computing power. Ease of use and transitioning to the cloud doesn't have to be overwhelming because you can migrate gradually, phasing in workloads over time. Hybrid cloud responsibilities could be the consumer or the provider is doing the managing. The provider or consumer could own the infrastructure. The infrastructure could be located on‑premise or off‑premise, and you have flexible access based off of need.
Cloud service models 

Next, let's look at the cloud service models. Recall that there are three cloud service models, IaaS, where services are raw, compute storage and networking need to be composed by the actual consumer, PaaS, or Platform as a Service, where the provider is responsible for maintaining the OS and the consumer configures a platform like a database above it, and Saas, or Software as a Service, where the consumer makes use of a particular pre‑configured software, the provider is responsible for managing the underlying systems. With Infrastructure as a Service, it reminds me of going to the market in order to purchase groceries. Normally I like to shop for raw fruits and vegetables and other items that are not pre‑made. When I acquire these items, I must have the knowledge and ability to prepare these things on my own. I cannot expect that the store is going to prepare these raw items for me, so I have to have those capabilities. In the same way, the Platform as a Service needs to have engineers and architects to actually build out the logical environment. In IaaS, the major business drivers for consuming it is an organization has a need for raw compute and storage to manage a diverse and flexible compute and storage over their data. Rackspace is an example of this, Google Compute Engine, or a GCE, DigitalOcean, or Magneto 1. These resources are pooled into an abstraction of orchestration and virtualization. The abstraction, often via the virtualization, frees the resources from their physical constraints to enable pooling. Then you have a set of core connectivity and delivery tools that are tied to this abstraction for further consumption. In IaaS, the provider is responsible for the data center, the equipment, and the hypervisor maintenance and health. A series of physical servers each run two components, hypervisor for virtualization and the management or orchestration software to tie into the servers that connect them to the compute environment. The top security threats are related to a lack of due diligence. The customers should ensure that security partitions reliably isolate tenets from one another. This isolation must be present throughout all IaaS infrastructure components, the host, virtual machines, compute, memory, network, and storage. Platform as a Service reminds me of going to an all‑you‑can‑eat restaurant or a smorgasbord, where there may be diverse food represented from all around the world. Have you ever eaten at a place that could serve Viennese, Vietnamese, Indian, American, Italian? Just as these various food types are prepared for you, so are the many different platforms that could be consumed directly without having to install and configure an operating system. Major business driver for consuming PaaS is to reduce the need for managing and configuring virtual machines. The organization is primarily concerned with developing and deploying applications upon a platform. With Platform. as a Service model, the vendor offers a pre‑composed compute system, where developers can develop and deploy applications. AWS' Elastic Beanstalk, Heroku, Windows Azure, most used as a PaaS environment. Force.com, OpenShift, Apache, Stratos are just a few examples of Platform as a Service environment. The Platform as a Service environment avoids the need to build a virtual server environment to run an application and the need to install a development environment for creating applications In PaaS, the cloud user only sees the platform, not the underlying infrastructure. For instance, a database can expand or contract based off of needs of utilization without the customer having to manage individual servers, networking, and patching. The platform, including how they configure any offered security features, would be a mix of maintenance that's done by both the consumer and in an opaque way from the provider. Top security threats include lack of service and process isolation, user‑level permissions, malware, and backdoor or trapdoor situations. Software as a Service reminds me of going to a highly‑rated restaurant that serves a specific food type. If you go to an Ethiopian restaurant and want to be served a Polish dish, that probably won't happen. Normally the menu is limited to that specific type of food, and within those constraints, there are two things that you need to bring with you to the restaurant, your appetite and your wallet. Software, as a Service has as a major business driver the consumption of a SaaS environment for a specific data need that is being furnished by a specific application. For instance, SaaS environments deliver specific services based upon specific customer markets. The customer pays for the specific service that they require from a specific menu or a catalog. Some examples are BigCommerce, Google Apps, Salesforce, Dropbox, Mailchimp, Zendesk, and DocuSign. SaaS services are full multitenant applications with all the architectural complexities of any large software platform. Many SaaS providers build on top of IaaS and PaaS due to the increase agility, resilience, and potential economic benefits. The cloud provider is responsible for nearly all security, since the cloud user can only access and manage their use of the application and can't alter how the application works. Primary threats with a SaaS environment include lack of granularity with data access controls and lack of in‑point protection mechanisms applied. It is important to understand that there is a shared responsibility between the provider and the consumer related to services being consumed. Generally, data will always be the responsibility of the consumer, but the further up the stack you go, the more responsibility actually falls on the provider, for instance, in a SaaS environment. The further down the stack you go, the more responsibility falls upon the consumer. They're responsible for patching and for maintaining their virtual machines and all of the applications that are built on top of the IaaS that they are consuming. Whereas PaaS is sort of this middle area in what we could term a water line. This water line says that the provider has greater responsibility at the top and less responsibility at the bottom, and the converse is true for the consumer.

CSA'S Logical Model

It's time for us to turn our attention to the Cloud Security Alliance's logical model. The stack of the logical model appears as follows, the Infrastructure, the Metastructure, the Applistructure, and the Infostructure. If you are not indoctrinated in the CCSK learning, you may have never heard of any of the terms listed in Cloud Security Alliance's logical cloud model, except for Infrastructure. All but one of the terms used to describe the logical model, namely Infrastructure, are terms that are coined in depicting Cloud Security Alliance's logical cloud model. While you will probably not experience the other three terms outside of the CCSK, they are important to understand if you intend on testing your knowledge as a CCSK. Let's look at these individually. The first is the Infrastructure logical model. The core components of computing systems, compute, network, and storage, is included in that model. The foundation that everything else is built on in the Infrastructure, for example, both the physical infrastructure used to create the cloud, as well as the virtual infrastructure used and managed by cloud users, are inside of the infrastructure. In a private cloud, the same organization might need to manage both. In public, the cloud provider manages to physical infrastructure, while the consumer manages their portion of the virtual infrastructure. Infrastructure security maps to infrastructure. The protocols and mechanisms that provides the interface between the infrastructure layer and the other layers is known as the Metastructure. The Metastructure binds the technologies and enables management and configuration through cloud controller tools for compute and storage. These functions are centrally controlled through what is called the management plain, and for the consumer, it's the management console. The security mapping is associated with the management console. The Applistructure contains the applications deployed in the cloud and the underlying application services used to build them, for example, Platform as a Service features like message queues, artificial intelligence analysis, or notification services. Application security maps to Applistructure. Finally, there's the IInfostructure. What's the difference between data and information, which one is more valuable? Data is typically seen as ones and zeros, or binary, data is what the computer works with as a foundational input. Information is seen as data put into context and made meaningful for the human. The value of each depends on the circumstance, in one context on structured data could possibly carry more value through machine learning than could information. In another context, the information that is gleaned in a report could carry more value than the data because it allows the organization to make an important decision. In coming clips, we will evaluate the different types of storage, where data and information are retained that will include two major families of volume and object storage as attached to a VM, or in a database, or as a file storage system. Security maps to the Infostructure when we're talking about the data. So data security maps to Infrastructure, also, this would include information.

M3 C4 Summary 

What services will you select if you are in a situation where developers are distributed across a large geographic space? Platform as a Service will give you immediate benefits in that it is a collaborative work environment. If you need more control of your underlying systems and maybe just moved from a data center, Infrastructure as a Service may give you all the control you need, as it is like a virtualized data center. If your organization comes under heavy regulations or legal requirements for data privacy, you may actually be considering a private cloud as opposed to a public cloud, which may get you your quickest return on investment. It's very important that organizationally, there is a consideration of the risk and threats that are associated with each choice. There must be a determination of how you are going to protect the assets despite the systems that are being consumed. Finally, it's important to think of the applications of the logical model and how that will define your consumption. If we do not make the connection between the layers in the stack and the ways in which we need to protect our assets, we could be walking into a situation of high risk and low reward.

CSA Enterprise Archetecture 

Let's consider what it takes to establish a secure cloud architecturer. First, we will look at the Cloud Security Alliance's Enterprise Architecture, and start from there, getting a look at what are the essential elements of good security. Then, we will take a look at the NIST reference architecture and the roles that it assigns for proper cloud governance. Finally, we will understand the tools to help us in selecting a cloud security provider. The Cloud Security Alliance Enterprise Architecture is composed of four pillars. It is both a methodology and a set of tools that enable security architectures. As you see, the architecture is actually composed of previously‑developed frameworks that are standardly used throughout the industry. Each of the four pillars has a basis or influence in pre‑existing architectures. We will break down each of the four pillars. What follows is not exhaustive, but gives a great amount of detail into the underpinnings of the Cloud Security Alliance Enterprise Architecture.

CSA-BOSS Pillar 

Let's consider the Cloud Security Alliance's Business Operations Support Services pillar contained in the enterprise architecture. First, we began with the Business Operations Support Services. It combines the best practices for reference frameworks to align with business and transform the business into a security practice across the organization. It also is a business enabler. While many security architectures may focus on technical capabilities only, this architecture particularly takes into account practices that eventually can enable business‑relevant information about the health of the organization regarding its information assets and business processes. A common concern that an organization will have is how to integrate services with cloud providers and have a sense of security from the provider that offers confidence in their exposure with the data that they have with that provider in a multitenant environment. The Business Operations Support Services helps to identify those aspects that must be considered, besides the technology solutions such as legal guidance, compliance and auditing, and human resources, and monitoring capabilities. The awareness of security throughout the whole of the organization is promoted. The BOSS architecture is based upon SABSA domains. Compliance has to do with audit planning, independent audit, third‑party audit, internal audits, contractual audit maintenance, information security regulatory mapping, and intellectual property protection. Data governance has to do with data stewardship, data classification, clear desk policies, secure data disposal, and handling and labeling of security information. The operation risk management has to do with operational risk committees, crisis management, business impact analysis, business continuity, key risk indicators, and risk assessments. Human resources focuses on employee termination, employees agreements, job descriptions, employee awareness, roles and responsibilities, and employee code of conduct. The security monitoring focuses on some technologies like seen platforms, event monitoring, application monitoring, endpoint monitoring, and event correlation in cloud monitoring. Internal investigations focus on forensic analysis and email journaling, and the legal services focuses on contracts, e‑discovery, incident response, and legal preparation.

CSA-ITOS Pillar 

Now let's review the Cloud Security Alliance's information technology operations and support pillar contained in the enterprise architecture. The information technology operations and support, or ITOS, outlines all the critical services that an IT organization will have in order to support the business needs. This domain provides alignment of industry standards and best practices like a project management office, capability maturity model integration, or the adoption of ISO/IEC 27002 controls, or applying COBIT, or, as is stated here, using ITIL version 3. It is good to note that it is ITIL version 3 and not 4 that is used to describe the Cloud Security Alliance's enterprise architecture. All of these provide a reference from two main perspectives that enables the organization to support the business needs, aligning them with industry standards and best practices. Relationships between technology components, however, are not intended to be one‑to‑one matching, so that the process touch points described in the PMO or the ISO/IEC certification process may not actually line up with what an organization is doing at one time, as opposed to what they may be doing at another time. The domains include IT operations for disaster recovery planning, IT governance, resource management, project management office, and the portfolio management process. It would connect with service delivery, the service level management and maintenance, the information technology resilience, application performance monitoring, and asset management. The service support would include configuration management, knowledge management, change management, incident management, and problem and release management.

CSA=Services Pillar

We will look at the Cloud Security Alliance's services pillar contained in the enterprise architecture. The TOGAF‑based benefits in the pillar of Cloud Security Alliance's enterprise architecture has four different services that it's concerned with. The presentation service domain is where the end user interacts with the IT solution. The security requirements for the presentation domain will vary based on the type of user and the type of service being provided. With the application services, these are the rules and processes behind the user interface that manipulates the data and performs transaction for the user. For the information services, this is all the data needs that have to be transformed into useful information that business asset owners can use to prioritize, strategize, and manage their risk portfolio. And then the infrastructure services has to do with the IaaS stack inside of the service model. It provides the basic core capabilities that support higher level capabilities in other areas of the architecture. TOGAF‑based domains include facility security, asset handling, controlled access with surveillance monitoring, and power redundancy, patch management, which would include compliance monitoring and service discovery, storage services, which is concerned with provisioning, migrating, and sanitizing physical and logical storage, equipment maintenance, which is concerned with assuring that physical infrastructure devices are appropriately maintained to assure their continuous operations, availability services that will allow information to be available wherever and whenever it is needed, controls at this level would include data mirroring between geographically dispersed sites, network services is concerned with managing the security risk posed by the network environment. Controls that this level would include proper network segmentation of, for example, provisioning basic network services such as a traceable and accurate transport of information. And then finally virtualization, which would include server, desktop, application storage, network, database, mobile device, and smart card virtualization. The other domains of TOGAF would include service delivery, the discipline concentrates on proactive delivery of service information for the Information Communication Technology Group, reporting services, which is focusing on the ability to present data in various ways, going from top level aggregated dashboard and drilling down all the way to raw data. There's the service support, this is where groups work for information sources coming from information technology operations and support, user directory services would include a system that stores, and organizes, and provides access to information about users in a lightweight directory access protocol format. With TOGAF, the domain for application services would include programming interfaces, and this is how applications are able to talk to each other. Input validation is a very important component of this process. The security knowledge life cycle is a way of building secure applications and keeping a development team up to date on the latest threats and appropriate countermeasures that they can take in their development process. The development process would have security as its focus, and security would really be treated as a solution with source code scanners and other tools used to ferret out common flaws in the code. The integration middleware focuses on tools like service buses and message queues that allow applications to exchange information without talking directly to each other. The presentation services for TOGAF would include consumer service platform, where you have the container that holds the various types of presentation modalities that is consumer oriented. You also have the enterprise service platform, where this container holds the various types of presentation modalities that are oriented to support the enterprise in the workplace, and finally endpoint and mobile and smart device protection at the presentation layer.

CSA-Risk Management Pillar 

Explore the last pillar, risk management, in the Cloud Security Enterprise's Architecture. With the security and risk management pillar, here we are focusing on the information security program. The idea is to safeguard assets and detect the access and monitoring risk inherent to operating activities within an organization. These capabilities include identity and access management, GRC, or governance, risk management, and compliance, and setting up policies and standards within the organization that would include threat and vulnerability management and infrastructure in data protection. The security and risk management domains that are based on Jericho would include the GRC program. This is the fundamental issues of governance in a enterprise risk management organization for cloud computing, and an organization would need to implement the appropriate organizational structures, processes, and controls to maintain an effective information security governance program. The infosec management is the main objective of the information security management system within an organization. Here, the implementation is designed to have appropriate measurements in order to minimize or eliminate the impact that a security‑related threat and vulnerabilities might have on the organization. There is privilege management for the infrastructure. Here, the identity and access management is following the principles of least privilege. The threat and vulnerability management focuses on the core securities such as vulnerability management, threat management,, compliance testing, and penetration testing. Vulnerabilities really focus in on the known issues, whereas penetration testing could expose the things that you do not know. Data protection in this information age is about data being an asset and really looking at it from its three levels of existence, data at rest, data in use, and data in transit, in always trying to protect it from harm.

NIST cloud computing reference architecture 

NIST Cloud Computing Reference Architecture helps to identify roles and responsibilities in the cloud. Special Publication 500‑292 expresses 5 roles, the cloud consumer, the cloud provider, the cloud broker, the cloud auditor, and the cloud carrier. This cloud computing reference architecture helps to accurately communicate the components and offerings of cloud computing. What we will focus on are the guiding principles used to create their reference architecture, namely by referencing the cloud actors. The first is the cloud consumer. This is a person that maintains a business relationship with and uses the service from the cloud providers. It could also be an organization. The cloud consumer is the principal stakeholder of the cloud computing service, and it represents a personal organization that works with the cloud provider to consume that service. That service is provided and consumed by means of a service catalog from the cloud provider. The cloud consumer requests the appropriate service, sets up the contract with the cloud provider, and then uses that corresponding service that's been provisioned and also pays for what is used. Cloud consumers need service‑level agreements to specify the technical performance requirement that would be fulfilled by the cloud provider SLA can cover terms regarding the quality of service, the quantity of service, the security, remedies that should happen if there is a service performance failure. The cloud provider is a person, or an organization, or entity that's responsible for making the service available to interested parties, namely the cloud consumer. Cloud providers acquire and manage the computing infrastructure required to run the cloud software that then provides the service. For Software as a Service, the cloud provider assumes most of the responsibility for maintaining that service. They have the responsibility of managing and controlling the applications and the infrastructure, while the cloud consumer has a limited administrative controls over the applications. The cloud consumer will always be responsible for their data. In a PaaS environment, Platform as a Service, the cloud provider manages the computing infrastructure for the platform and runs the cloud software that provides the tools, such as integrated development environments, or IDEs, or other types of software like database applications that they can scale up and down. In this case also, the cloud provider is managing the underlying systems like network servers, operating systems, or storage. In an IaaS environment, the cloud consumer, through a set of service interfaces, mostly APIs, gets in contact with the actual infrastructure for raw compute and storage. Here, they will develop or launched their own guest operating systems, be responsible for patching those operating systems. The only thing that they don't manage is the actual hypervisor or the physical equipment that's inside of the data center. The cloud auditor is a party that can perform an independent examination of the cloud service controls with the intent to express an opinion of the results of that audit. Now this audit could actually be designed specifically for looking at the security controls, the privacy impact, the performance. If it is the security controls, they may be looking at them from an operational, from a management, administrative, and technical viewpoint to see if proper countermeasures have been put into place. The cloud auditor can make an assessment of the security controls in the information system to determine the extent to which the controls are implemented correctly and operating in the way that they should. A privacy impact audit could help federal agencies, local agencies, national agencies in compliance with applicable privacy laws and regulations governing an individual's privacy, but it also could help the consumer to understand the level of compliance and obviously help the provider to know if they are compliant. As the complexity of the cloud evolves and the tools and the management of that evolves, it may be necessary to consult with a cloud broker. The cloud broker helps to manage the use, the performance, and the delivery of cloud services, and negotiate the relationship between the cloud provider and the cloud consumer. In general, there are three ways in which a cloud broker can assess the consumer. The first one is service intermediation. This is where a cloud broker helps to enhance a given service by improving specific capabilities and providing value‑added services to the cloud consumers. This improvement could be managing access to the cloud service, or identity management, or performance reporting. With service aggregation, this is where a cloud broker will combine and integrate multiple service into one or more services. The broker provides the data integration, and then ensures that the data is secure as it's moving between cloud consumer and possibly multi‑cloud providers in a multi‑cloud environment And then there's the service arbitrage. This is where the services, just as an aggregation, are made available, except that the services being aggregated are not fixed, and the cloud broker has the capability of arbitraging this service so that there is flexibility to choose from multiple agencies. There could also be a situation where the cloud consumer is taking service in at one level, say IaaS, and then giving the service out at another level, say PaaS or SaaS. The final actor is the cloud carrier. The cloud carrier is the primary intermediary that provides connectivity and transport of cloud service between cloud consumers and cloud providers. The cloud carriers provide access to consumers through network telecommunications and other devices. For example, it could be by means of smart devices, by IoT, by regular laptops, computers, and mobile phones that are being used in order to consume the service in the cloud. Does that remind you of anything? It should remind you of broad network access, The relationship that's managed between the provider and the actual cloud carrier is also done by means of SLAs. There are some very interesting constraints and issues that could come up that the consumer would probably want some transparency on when it comes to that relationship between the cloud provider and the cloud carrier. In many cases, the cloud carriers are becoming so large themselves that they are actually a cloud provider, and vice versa.

Using the cloud control matrix

Let's examine the Cloud Control Matrix. A helpful tool that we can use in determining the efficacy of the controls of a cloud provider is the Cloud Control Matrix. It allows consumers and providers to think in terms of specific controls of the cloud being mapped to specific regulations and frameworks. It provides a common set of expectations between provider and consumer, and for me, it's the best use of an Excel spreadsheet I have ever seen. All of the information concerning a breadth of regulations and frameworks contained in one location are mapped to specific controls. There are 133 controls and 16 privacy cloud control domains in number. Each domain has multiple detailed sub‑categories that helped to form the 133 controls. While we will not entertain all categories, we will give an overview or sample of the mini‑sub categories that exist. The cloud control domains consist of application and interface security, which includes customer access requirements, data integrity, and data security, audit assurance and compliance, which includes audit planning, independent audit, and information system regulatory mappings, business continuity management, which includes planning, testing, data center utilities, documentation, and environmental risk, and change control and configuration management, which includes outsource development, quality testing, and product changes. Data Security and information lifecycle management includes classification, inventory flows, e‑commerce transaction, handling, labeling, security policy, and ownership. Data center security includes asset management, controlled access points, equipment identification, offsite authorization. Encryption key management includes entitlement, key generation, sensitive data protection. Governance and risk management include baseline requirements, data focus, risk assessments, management oversight, policy enforcement, and policy impact and risk assessments. Human resources includes asset returns, background screening, employment agreements, nondisclosure agreements, training and awareness programs. Identity and access management includes audit tools access, credential lifecycle provisioned management, policies and procedures, and segregation of duties. Audit logging and intrusion detection, change detection, information system documentation, and vulnerability management are all part of the infrastructure security. And then a extremely important one, interoperability and portability, which I would suggest looking up the ISO/IEC documents on these before you consume cloud services. It's imperative if you do not want to experience lock in with a cloud service provider due to data format and translation of data elements. The mobile security includes anti‑malware approved application list, approved bring your own device software, awareness training, and device eligibility. These security incident management domain includes authority contact, in cases of emergency contact, authority for maintenance, incident reporting, incident response, legal preparation, and response metrics. Supply chain management includes transparency and accountability, data quality and integrity, and provider internal assessment governments and reviews. Threat and vulnerability management includes mobile code, anti‑virus malicious software, and patch management. Here is an example of what it looks like in the Cloud Control Matrix from the standpoint of a HIPAA requirement. You can see the actual control specified in high tech and the control that corresponds to the cloud element. Here's one for PCI DSS. What if you're an organization that makes use of payment card transactions. Note the mapping and specific cloud control to specific requirements of the PCI DSS. It's an excellent document to use in order to know where the cloud controls are with the cloud service provider.

Selecting a CSP

Finally, let's discuss selecting a CSP. In selecting a CSP, there are some primary considerations for doing this and tools that we could use. Selecting a CSP involves translating business needs into a matching service, delivery or a consumption strategy. Now, while there may be technology elements that enable a specific service, the language that is used with that business unit must be centered around end‑to‑end services. It's not about technology. You will be able to find a great deal of self‑serving information about a provider on their website or perhaps even from clients, but it's really helpful to be able to get empirical information through audit about that cloud service provider that you are potentially going to consume services from. It is not just the delivery of service that will position an organization to succeed in their mission. They also need to be aware of and adhere to whatever laws, regulations, or jurisdictional requirements are mandated for their service consumption and the corresponding data that is maintained. One tool that we could use to refer to the documented capabilities of the cloud service provider is Cloud Security Alliance Security Trust in Assurance Registry. It contains the registration of cloud service provider controls. This is a self service on‑demand location where consumers can go and assess the controls of the potential providers, and then consumers can decide to utilize a provider or not based on differing levels of assurance. The Cloud Security Alliance codifies three levels of assurance in the open certification framework. CSA STAR level 1 is a self assessment that has 2 sub levels, the Cloud STAR Self‑Assessment with the GDPR Code of Conduct and Self‑Assessment and Continuous Self‑Audit. There's also CSA STAR level 2. This is a third‑party certification that has two sub levels as well. And then there is CSA STAR level 3. This is full cloud assurance and transparency. Every time you go from one level to the next, you're actually taking all of the capabilities that were expressed in the previous level and adding to them to come to a higher level of assurance for the cloud customer.

Summary 

So what will be your approach to developing a secure cloud architecture? It is extremely important that we start there, long before we think about the actual design and the actual services that we are going to consume. That cloud architecture, if developed in a secure fashion, will mean that the services also have a greater chance of being delivered in a secure fashion as well. Have you clearly set out the delineation of responsibilities based upon the services that you are looking to consume? Remember, the lower you go in the water line, in the water mark of the stack of IaaS, PaaS, and SaaS, the more responsibility is on you as the cloud consumer. The higher you go up, the more security responsibility is on the providers, say, for instance, in a SaaS application. Delineating the responsibilities and understanding them before you consume a cloud service is key. Now that you have seen all of the ways in which we can consume cloud services, you may be looking at picking a cloud service provider. There are plenty of tools out there for you to get a wide variety of assessments. The one that we considered was the Cloud Security Alliance's STAR registry. Here, you can look up a certification or attestation on the cloud service providers. These certifications are done based off of ISO‑type process documents, and the attestation are done based off of service organization control reports or SOC audits.




Automation Executive briefing 

Introduction Automation 

The idea of automation isn't anything new. We've always wanted to have some kind of machine or some kind of device that could take care of certain tasks for us, and preferably all the boring ones we don't want to do. Now a few decades ago the dream was to have these big physical robot assistants, but it turns out that for most of us it's more useful to have small software helpers that could run on the computer because that's where most of our work is. Now these days when you read any business news, you see the word automation all the time, and not just by itself, but in phrases like robotic process automation, intelligent automation, and hyperautomation. And you might wonder, well, what's the difference, what are these all for, which one should you be paying attention to, which ones can actually help, and how do you get started with any of this automation? Well, we're going to cover all of that here. But this won't just be me reciting a list of definitions at you, I'm going to show you how this is done. And we're taking a business focus here to see the impact automation can have on any size of organization, all the way to the enterprise level, but we're going to begin with just, well, the kinds of things you might automate in your own personal life or some of the most repetitive tasks in your own current job and then expand on that to think about these ideas at perhaps the project level or for your team or for your organization. But beginning with a personal focus is the best way to really understand automation, first because it's practically easier to get started like that, but also because this is the right perspective about all of this. You see, with automation our focus shouldn't begin on the technology, so what vendors to choose or what applications to install, it should begin with the human, the person involved, because the point of automation, in as few words as possible, is to reduce the amount of repetitive work. Let me say that again, to reduce the amount of repetitive work. And does that definition cover everything here? No, but it is a good place to begin. Can we make our own lives a little bit easier? And if so, how? And then can we use what we know and make somebody else's life a little bit easier. So, to get started, we're not going to begin with a specific technology or vendor or application, but we begin with a pair of socks. I will explain. I'm Simon Allardice, and welcome to the Executive Briefing on Automation.


Background and Challenges 
We've all heard the doom and gloom predictions that over the next 5, 10 or 20 years, automation will destroy a third of the jobs or half of the jobs or all of the jobs, it depends on which study you read. Occasionally you hear a more optimistic perspective that also looks at the potential for job creation within all this upheaval, or even focuses on the benefits and how automation can remove all the boring tasks, leaving us to do nothing but express ourselves in new occupations full of boundless creativity and invention. Okay, I wouldn't yet go that far, but automation is great at removing drudgery and repetitive, tedious tasks. But let's be clear, it does affect and will continue to affect existing jobs. It is causing occupations to disappear, and it's also creating new ones. It will make some jobs less valuable and others far more valuable. These things are all true. What shouldn't even be in question is whether automation affects existing jobs. It does. That's the point. That's what the word means. When we talk about automating anything, it implies there is already an existing task that is currently being done by a person or by multiple people. We're not inventing a new process, we are automating an existing one. And I stress this rather basic point because it sometimes seems to get lost, and it's important to recognize and engage with how this all affects people for any kind of organizational change to be successful. Now this issue isn't new, we've been dealing with it for several hundred years. The classic automation in the workplace situation began with things like the stocking frame machines in the textile industry in the late 1500s. These could replace, or at least reduce the need for, all the legions of knitters and sewers who were making fabric for, well, stockings. So, this whole automation push and everything we're talking about all began because people wanted more socks. But from the beginning, there's always been this conflict between the automation and the people whose jobs or tasks or responsibilities were being automated, and it's led to the machine breakers, the attacks on the machinery. It led to riots. It led to new laws penalizing those who would attack the machinery. And let's face it, this isn't over. Automation has and will continue to have not just an organizational impact, but a societal impact, particularly because automation is often perceived as being only about profitability and reducing employee headcount. But there are multiple other benefits. Automation can help improve speed, it can improve customer service, it can remove boredom and tedium from a job, it can reduce errors and help with quality and consistency. And even in those early days, some companies would stress the benefits of the automated machinery and the regularity and the reliability and consistency of the material that their operators could now create because of this modern technology. But here's the thing. These days, there are some important differences. First, most jobs are not as straightforward and single task as someone who knits fabric for socks. If I speak to most knowledge workers and ask, can you imagine a computer program that could replace your entire job and everything you do? It's not easy. We might make and give presentations and write reports, make and answer phone calls, read and answer emails, we create spreadsheets, perhaps we write code and debug programs, install software, we attend meetings where we provide feedback and suggest solutions, we solve conflicts, we manage projects, hold interviews, we help new employees onboard, we mentor people, we evaluate purchases, we deal with emergencies. It is hard to envision just some piece of software that can just do all that. But, if you step away from asking can my job be automated and instead ask is there any small part of my job that could be more automated, more streamlined, a little bit smoother, a little bit easier, usually the answer is, well, yes, of course. So we begin there, not by automating a job, but with the idea of automating one piece of work, automating just one specific task.

Personal Automation

There may be people watching this who think, I've heard about automation, but I've never actually used any kind of automation feature. Well, you probably have. Say if you've ever made a repeating appointment in your calendar app, whether it was for a personal task to remind myself to pick up Jenny from soccer practice every Wednesday at 7:30 for the next 8 weeks, or for business, set up a standing meeting with the team every Monday at 9am, ongoing. If you've ever done this, you're getting the benefits of somebody having automated this task because it's saving you from a repetitive, manual task. If you weren't able to create a repeating appointment, then okay, you just have to create these appointments individually one at a time. Now, could you do that? Well, sure. It isn't even a difficult task. This isn't impressive or complicated. And we're going to come back to this idea that a lot of automation isn't dramatic or impressive, but let's say you did need to create a weekly appointment every Tuesday at 11am for the next 6 months. Well you can do it manually, but it's tedious and time consuming, creating exactly the same thing again and again and again, and there's a high probability that in doing 26 separate appointments you will mess something up, accidentally make one at the wrong time or skip a week, make one of the meetings an hour when it should have been half an hour long or type the title incorrectly, forget to invite exactly the same set of people, and so on. So, the fact that this make a repeating appointment feature exists where someone has identified the situation, has thought about the different aspects of it, because there are a few choices to be made here, sometimes you want to create a weekly appointment, other times it's monthly or every second Thursday, perhaps even daily, and you might want it to repeat 12 times or just 4 times or to end on a specific date, but you need to describe those characteristics. And then you're done, the rest of the task is automated. It saves you from having to set up 26 individual events, and it also saves your invitees from having to reply to 26 separate invitations. Now, okay, what I'm describing here is an automation feature that's already built in to, well, pretty much every calendar application these days because it's such a common requirement. But as an example here, it's useful because, well, most people have done this, it's easy to understand, and it describes three qualities of automation we will see again and again and again. First that what we choose to automate often isn't difficult or complicated or groundbreaking. The task itself might be quite simple, but just a bit repetitive, a bit tedious. Two, there are often a few choices to make within automation, some variety to each task. For example, in the calendar app we wouldn't just want to have a single checkbox to say yes, it's a repeating appointment, and then have the computer decide how often it will repeat and how many times. No, we need to provide that little bit of extra information, those rules to follow. And three, the benefits of automation are often small, but cumulative. They add up. If making a repeating appointment was this rare situation where only 1 person in 1000 needed to do it once every three years, it wouldn't be a feature in the calendar app, but we do it often enough so it might only save a small amount of time and inconvenience, minutes or even seconds each time, we get those savings again and again and again, as does everybody else who makes appointments. So, we can take this idea and then think about some of the situations that are a little bit more specific to us. For any repetitive task, that's a bit manual, a bit predictable, and perhaps a little tedious. And when the feature isn't already built into some application, we can then use personal or desktop automation tools to provide it. And these tools are often already built into the operating system on your desktop or your laptop, or even your phoneSo let's take a look.

Desktop and Automation Tools
You probably have more automation capabilities than you realize available to you right now at your fingertips. For example, if you use a Mac, there's an application called Automator that was already installed on your computer when you first took it out of the box. If you're on Windows 10, there's a built‑in application called Task Scheduler, although that's mainly for automating computer maintenance tasks, but you can now also download Microsoft's Power Automate application for free, and that's very powerful. On an iPhone, Apple has the Shortcuts app. Now these are all just examples of automation tools with a personal focus. They allow you to describe custom workflows or shortcuts that run on that device where you describe a series of tasks with a few rules to follow, and often those tasks involve several other applications, and where you don't need programming skills to create these, you don't have to write any code, they're usually just a series of drag‑and‑drop options. For example, on my phone, I might decide I want a simple time to meditate shortcut that can do several things. First, it'll ask me how long, and I select a time, then it'll turn on do not disturb on my phone, it can connect to my home hub and dim the lights, it will then start playing white noise on my speaker, and it then sets a timer to chime after whatever the selected interval was, in this case 15 minutes, when I will presumably be in a state of clarity and bliss, as if. And if that example doesn't appeal, there are many other personal automation tasks you could experiment with. You might streamline your own email, automatically check that if it's from someone in your family and if it has an attachment that's an image, then you can automatically save that image into your photos application. On my phone, I might find it useful that with just one single press of a button I could automatically get directions home from wherever I am, then send a text message to my significant other with the estimated time, then set my status to away in my instant messenger work application and then just stop playing the current audiobook I'm listening to. As with the repeating appointment feature on a calendar, these are often simple and repetitive tasks, something where, yes, I could do all the steps myself, but they save me a little bit of time, they make it easier to get started, they can make it more reliable and less error prone, and, importantly, they reduce resistance to doing that task at all. Now usually there are a few choices, a few rules to follow, which include deciding when does this thing happen? Sometimes you want it on request where you'll manually press a button to initiate it, but I might also want the task to happen on a specific date and time or triggered by an event, like whenever I leave the office do this thing. And also where, like a repeating appointment, the benefits of these are cumulative. They may be small, but they add up. Now, even if these purely personal automation examples aren't of interest to you, you might imagine how this can start to overlap into your working life. For example, I might decide to make a shortcut on my phone that can help me with filing expense reports and the multiple different steps I need to do for doing that. Now, the built‑in applications on your computers are great, but they are very much oriented to the device itself and mainly useful when everything you want to do is running on that device. But if, for example, the situations you want to automate are more about using various third‑party web applications, like Gmail or Dropbox or Trello or Slack, well you can look at options like ifttt.com or zapier.com. Now these are web‑based automation tools that you can set up personally and use personally, but they're also useful in today's more cloud‑based working environment. Now, while these tools do need a little more configuration so they can connect and authenticate to all the different tools you use, but once they are set up you could then describe a situation like, let's say you receive an email with an attachment, then you will automate taking that attachment, you'll rename it, you'll copy it to a Dropbox folder, you'll then create a Trello card for someone on your team to review it, and then you'll create a reminder for yourself to check that in 24 hours. With all of these desktop or web‑based automation apps, they'll often have galleries of examples you can look through, and these can be very useful when you're starting to build that muscle of recognizing just the huge amounts of different ways you can apply this.

Sidebar- personal workflow automation

A quick sidebar. I wanted to show you one way that I use these ideas. So, I write and record training courses, and when I first started doing this it was just my voice narrating over a screen capture recording. But about 10 years ago, I got to film my first course with some live action of talking to the camera, and to do this I went into a studio with a crew of four other people. One person was operating the camera to keep focus on me as I moved around, someone else was running the sound, there was a teleprompter operator and a floor manager who also ran the lighting. Fast forward to these days, and when I record live video now, I don't have a crew, it's just me. So, I have a camera that locks on my face and can keep focus automatically. I have a robotic slider if I want to add some motion to the video. If I need a teleprompter, there's an app that uses speech recognition to automatically keep the script in the right place. And these are just the physical devices you can see here. There's also other automation happening on my computer. So when I finish recording a clip, it'll take the audio from my microphone and automatically run it through an application to clean it up, remove background noise, and then even out the volume. It then combines that audio with the video from my camera and it then imports those files over into my video editor. And there's a few other things happening too, but basically it's all set up to streamline as much of the tedious manual work as I can possibly do, both in the actual filming and in getting ready to edit. Now, you might think, okay Simon, but did you just put four people out of work by doing this? Well, no, because first, 10 years ago the situation where I could go into a studio was a very rare event. It only happened a few times. I had to fly to another state to do it, the studio was used for multiple different things and was often booked one or two months in advance, so even if I wanted to do it and had the budget to do it, the timing didn't work. I may have been ready to film and the studio wasn't ready for three more weeks. So, I don't think of this as me trying to replicate the studio situation, it's actually the opposite. For me, it allows me to bring the baseline up to do a better job than I would have done before when it was just my voice talking over a screen capture or a PowerPoint deck. So all of these options allow me to create better results and do more with less and in less time and where it's all under my control. I don't have to wait for somebody else. When I'm ready to film, I just film. So these technologies have augmented what I would be capable of by myself. Now I understand this won't match what you do for a living, but the point is this was a custom arrangement for me. There was no single application or gadget or service I could buy to fix this, and that with any unusual or nontypical job, you have to figure this out for yourself, to think about it, to plan it, and to experiment, to be able to get this kind of automation result. And it's still an ongoing process. I expect that a year from now it won't look exactly the same as it does today, but my focus is always to figure out what is most important. For me that's the writing, the sequencing, the editing of the course, and then figure that out so I can spend my time and effort on that part and automate the rest of it. So, end sidebar. Back to the course.

Introducing robotic: Process Automation  

You may know about robotic process automation, or RPA, already, but just for a moment, let's say it was the first time you'd ever seen this term because it's not unusual for someone to see robotic process automation and think this probably means the kind of thing you see in a modern automobile factory with large industrial robots welding metal plates together. But no, that is not what robotic means in robotic process automation. RPA is business focus. This applies in a typical office environment. It's not about creating physical robots, it's about little independent pieces of software that can help with a variety of repetitive, day‑to‑day tasks that we do in any typical office environment. But Simon, you might say, if it's just software, why do you call them robots? Why not call them applications or programs? Well, because that doesn't describe them very well. We're not trying to build a new application with a user interface. With RPA, we're trying to create something that's small and is going to replicate the behavior that right now a person would do. So, the type of situation we're looking for with RPA is when you have these predictable, repeatable interactions between a person and one, or usually more than one, computer application. Now you may hear the phrase swivel chair used to describe some of these situations, any task that involves rotating from one application to another, whether that's actually swiveling between different computers or even just turning your head a little bit. So here's the cliché. This is Sam. And let's imagine if you asked Sam to describe his job today, he would tell you, I wait for an order to come in on the old mainframe system, and when one does, then I take the data and information from that order system and I copy it over into our web‑based internal system. Then, I send an email to Jo in the warehouse and let her know, and then I'd log into our inventory database and change the stock number, and then I make a reminder on my calendar to make sure this order is shipped within 48 hours. Sam is using multiple different applications, and it doesn't sound like those applications talk to each other, which could be for several different valid reasons, incompatible technologies or maybe one is a custom internal application, but the other has been purchased from a vendor and can't be changed. But it sounds like a good candidate for automating because even if this is only part of Sam's job, it's a boring, repetitive, predictable part of Sam's job. Swivel chair work can be something as simple as literally copying and pasting data between applications. But this idea also applies to some larger business processes like onboarding a new employee, any of the situations where somebody might have a well‑worn checklist of all the different steps they have to go through and repeat to make this thing happen. But we can go back to this idea that with automation, we're usually not trying to invent something remarkable and innovative that's never been seen before. We're trying to replicate a dull, predictable, repetitive, human task. But if we implement an RPA solution, well, we can expect that task to be faster. We can expect it to operate 24 hours a day, 7 days a week. We can expect it to be more accurate. We can also expect that the behavior of an RPA solution can be monitored and logged in depth, perhaps to identify better ways to do this thing. And we are going to talk a little bit later about the culture aspect of this because you have to consider the employee that was doing this task. If they feel the RPA was implemented to remove some of the tedious, repetitive busywork from their job, that's great, but if they feel RPA is being implemented to automate their job away, that's quite another message. But to summarize, if the process we're looking at in the workplace is something that is repetitive, something that is computer‑based because if perhaps a step of the process was go to the printer and take something and walk it over to the laminating machine, well, that may not be able to be computerized, but it may be a manual step in an overall automated process. But we're looking for something that is predictable and regimented and controlled and rule‑driven that we can ask exactly how this thing should work and what the different situations are. Then that's a good candidate for RPA. But, you might wonder, even if we have identified a good candidate process, how do we actually do this? How do we make this so‑called robot? Well, if we choose to try an RPA approach, there are several different applications to choose from. So let's take a look at those.

RPA Tools

There are several different RPA tools and platforms to choose from, and your organization may already have a preferred RPA vendor, but first, let's get an overview of what they all typically provide. And I'm sure there's some differences between the vendors and the words they use, but generally speaking, any RPA solution has three main parts to it. First, the robots. Sometimes they're called robots, sometimes called bots. And there are other terms. Some products call them agents or flows rather than a bot, but still it's this idea of defining a self‑contained piece of software that can replicate a human task. The question, where do they come from? What tool do you use to build these RPA bots? Well, the same way that we use an application to create a presentation or a spreadsheet, we have applications to build these robots. So there's applications like UiPath Studio or Microsoft Power Automate where we can define these workflows or job‑specific task sequences, these step‑by‑step operations that include some basic business logic and decisions. Now you might wonder, isn't this just computer programming? Well, there are similarities, but with most RPA tools you don't need to write code. They're considered no code or at least low code solutions. They use a visual designer interface where you drag and drop different pieces of functionality in the sequence that you need them and then customize each step within the designer. Now the instructions do need to be specific and detailed, and sometimes these tools can be installed individually on your desktop or laptop computer or even on your phone. But with other RPA tools, you just use a web application to design the robots, and everything runs on a remote server. But earlier we talked about using a personal desktop application to create automation that runs only on your own desktop or laptop or phone. But with RPA tools, they're more intended for a team or an organization, so we don't want this automation to only exist on one person's laptop. So we also expect some kind of central location, some kind of hub, if you will, to store, control, and monitor these RPA robots. First, it acts as a central repository. It's a place to store the definitions of them so they're not on your own desktop or laptop, but available across the organization, and they're backed up properly. And this controller part also needs to store credentials so that these robots can be authenticated into the different business systems they need to touch so that there's a way to manage that. And they should also support some kind of version control so we could change and edit them and keep track of old versions and understand who made any changes to them and when. But you'd also expect an RPA controller to store the details and collect a lot of data about any activity that's happening, when these bots were being used, what results were generated, and flag any errors or unexpected items so that you can get better auditing and improved governance and reporting about everything that's happening. So, these RPA robots, or bots, or flows, they can mimic most interactions between a human and a computer. So they can log on to different applications, create and move files and folders. They can extract data from documents or screen scrape from applications. They can copy and paste data from one location to another. They can complete forms or scrape web browsers to check social media and so on. Now there are multiple benefits to RPA. I've already talked about things like improved accuracy and consistency, higher availability, and that a bot can run 24/7, better data collection and governance and reporting about it. And yes, we typically expect reduced costs, whether that's directly or indirectly, by freeing up our employees from low‑value, easily automatable tasks so they can focus on other things. And the RPA implementation itself is comparatively inexpensive when compared to custom software development projects. And because RPA doesn't require any changes to the underlying applications and systems, it's considered non‑intrusive, meaning it works with the systems you have rather than requiring those systems to be changed and interconnected with it. But one thing you might wonder is what happens if we have a business process that's a little bit more complex than RPA seems designed to deal with? If it's not just moving files around or performing simple data tasks, is there anything for automation to do there? And that's the next step.

Intelligent Automation 

Beyond traditional RPA, there is a next level to automation that's rapidly becoming more and more important, and because it is a rather new developing area, there are a few competing terms for what we even call it. It's often called intelligent automation, or IA, but it's also sometimes called hyper‑automation, and sometimes cognitive automation, or even RPA 2.0. It kind of depends on which business publication you're reading. The definitions aren't completely identical, but they are broadly similar. So I'm going to use the term intelligent automation here, but if you've been hearing the term hyper‑automation, for our purposes, it's the same thing because regardless of the name, a fundamental idea is that we're building on RPA, robotic process automation, by adding artificial intelligence technologies like machine learning, natural language processing, and intelligent image and document recognition so that we can go beyond these more simplistic rule‑based workflow processes and create automation that's well smarter, more intelligent automation. You see, when we do this, the places that this applies become far more than just these basic swivel chair work of RPA, it starts to apply everywhere. Now in the previous clip, I started to show a few of the RPA tools and how we use them to define these task sequences, the step‑by‑step operations with basic business logic and decisions. Sometimes the result is called a robot or a bot or a flow, but over the last year or two, these RPA vendors have started adding AI features in a way that typically doesn't require deep technical knowledge. So imagine we've identified a business process we want to automate like dealing with incoming documents or managing social media. Let's say someone has been given the task to monitor posts online that use a specific hashtag that's related to your organization, and then we need to figure out if any comment is positive or negative or neutral. So does that comment need a reply? Does that comment need a like or a retweet? Or does this comment over here seem like a public relations disaster waiting to happen? Now with traditional RPA, you could make a bot to manage some of this social media workflow process, but you'd often want this human intervention stage where someone would actually look at each comment and make a judgment about it, but these days, we have other options. You see, the main RPA vendors have all added artificial intelligence functionality. For example, in the advanced licenses of Microsoft Power Automate, within all the drag and drop options we have, there is now an AI Builder section where you can choose from multiple artificial intelligence models that have been pre‑trained for common situations. Some of the options here include detecting languages, recognizing text from images, recognizing business card information, or even sentiment analysis, and this is a way to take a piece of text and look at the language and sentence structure to identify whether it's positive or neutral or negative, even making judgments about tone and emotion to identify happy or angry customers. Now to be clear, this might sound easy, it isn't. Sentiment analysis is something that even very recently would have required a lot of planning and custom implementation from software developers with a machine learning engineering background, and now, it's become a drag and drop option in a Microsoft Office app. And you see, the other RPA tools have similar functionality and they're making it easier and easier to design bots that have incredibly complex and demanding behavior. Now, some of you might be thinking well surely we can't just have anyone start adding all these advanced AI features, but remember this has always been the way with artificial intelligence. As soon as we figure out how to do something, it becomes unremarkable, it just becomes another thing we can do with a computer. Just as a few years ago, facial recognition was this incredibly complex engineering task, now it's nothing special. People don't think of it as AI or machine learning, it's just how they unlock their phone. So the fact that business users could now use these tools to add document recognition or sentiment analysis or to automatically categorize expense reports, this aspect of intelligent automation or hyper‑automation, this is how we're going to see AI becoming democratized across the organization, not just from huge big budget AI projects and big machine learning initiatives and centers of excellence full of PhDs, but also just from the bottom up being added into small individual automation situations, but still, the benefits of these situations, again, like the rest of automation, they may be small, but they're cumulative, they add up. Now, I don't want to suggest that all intelligent automation or hyper‑automation is as just using RPA tools with these new features, but it is a good indicator of the way the technology is moving. And intelligent automation can also involve more substantial custom software development and even creating automation routines that are self‑healing, and what this means is that traditional RPA automation isn't very flexible. If you create an RPA workflow that uses a specific application, but anything about that application then changes, the automation often breaks, but if we have smarter self‑healing automation, it can react and change the way a human would to try and keep that process working. But still, it's this core idea of taking automation and adding artificial intelligence to it. So to do a brief summary, traditional RPA can be implemented faster, it's been going longer, so the tools here are more mature. And RPA, as we've seen, works with existing applications, it's very non‑intrusive. Intelligent automation, on the other hand, is more powerful, but it also takes more effort to identify a use case and make sure you've thought it through. It's more difficult to design and implement, so it does require more testing to make sure that you've thought about every situation and it's also more expensive because artificial intelligence behaviors are much more resource intensive. So even though it might be simple to add one by dragging and dropping, you may be paying more based on how often you actually use that functionality, but one distinction is that intelligent automation isn't just about one step in a workflow that becomes smarter, it can also refer to the idea of designing automations that are much more substantial, that are more complicated end‑to‑end processes and where even the term bot may not be good enough anymore.

Digital Workers 

Another piece of jargon that's becoming more and more common in the automation world is the idea of a digital worker. Now a few years ago if you'd said digital worker, most people would think you just meant any employee who deals with computers or computerized information. But these days it's become something else. If we think of traditional RPA is where we create robots or bots, then with intelligent automation, adding artificial intelligence, we can end up with the idea of a digital worker, it's the next level of bot. So where a bot is focused on one specific task, a digital worker could manage multiple parts of much more complex, multifaceted situations and manage them from beginning to end. As one example, many businesses need some kind of onboarding process for new customers, and there's great potential for automation here because onboarding is and should be a formal, repetitive, predictable, and rule‑driven process. So, onboarding a new commercial client might include, well first, the stage where that potential customer is selecting between different products or services, then they decide to actually initiate onboarding. There'll be requests for different documentation based on what they're asking to do, some kind of background verification stage. There's often terms of credit or accounts payable agreements to decide. Now after those are decided, actually creating those unique legal agreements, legal involvement, approval, due diligence, we hit the stage where they set up accounts, we need to hook that new customer into the system and connect them with their representatives, some kind of ongoing client management, perhaps then analytics and customer support and monitoring. Now, with basic RPA, you might create bots to help with specific pieces of this story, like having a bot to automatically set up the new accounts and create the right entries in the internal systems. Or, you could have a different bot to help with the early stage of a customer making contact, making sure we're adding them to the customer relationship management system, perhaps generating some internal tasks to make sure they will be contacted or send a series of onboarding confirmation emails to that customer. But those two RPA bots, these will be completely independent and unaware of each other's existence. And this is still useful, but if we take this upper level from RPA to intelligent automation, now able to add machine learning, natural language processing, document recognition, then instead of just a few bots to help with specific parts of this onboarding process, we can think more of this digital worker idea to potentially oversee the entire process and often in conjunction with a human employee. So at the very beginning of this, we might use intelligent chatbots using AI for natural language generation to help guide the customer to the right products based on their objectives. As they proceed to the documentation request stage and they're being asked to upload scans of IDs or bills or contracts, we could use intelligent document recognition to examine those uploaded documents in real time as they're uploading them. So even if it's 2 am in the morning, we could immediately say, hey, that document you just uploaded was missing a required page or recognizing when something hasn't been signed and allowing them to correct those issues right then and there without waiting days or even weeks to hear about a problem. We might automatically and instantly match their data against reference data sources like credit databases or public company records to support the background checks. Now I'm not going to go through every stage here, but I hope you get the idea. We move from individual RPA bots into looking at the entire onboarding process management, prioritize different aspects of this, monitor the overall process, and escalate any exceptions. And it can make this workflow faster, more accurate, more available with better data for governance and compliance and analytics. Now it's true that some companies who are in the automation business will even refer to a digital worker as your virtual employee and talk about having multiple digital workers as a digital workforce, but in most situations digital workers aren't often meant to replace a human employee, but to augment or supplement one and act almost as an assistant for a specific job, even being able to ask questions and then perform tasks on the employee's behalf.

Automation strategy

I've said a few times that when you're trying to automate an existing business process, many of them aren't exciting. They're often going to be small, a little boring, a little dull, and it can be easy to be unimpressed and go oh, big whoop, you streamlined your expense report process. Or oh, you made it a little quicker to respond to social media comments, wow. And yes, first that is the kind of thing you're looking for. It's kind of the first goal of automation, as I said at the very beginning of this course was to reduce the amount of repetitive work, to find those dull, repetitive tasks so you can free people up for more high‑value work. So, they often won't feel like a big, important project or technologically advanced or innovative, but they add up. I want to finish by talking about this idea of automation strategy. So whether you're helping to define a strategy for your organization or you need to understand the strategy that's already in place or even recognize that there isn't a strategy, one common issue is that because automation is often implemented on teams and projects first, before there is an actual organizational strategy around the tools and the vendors and the security and the data management and the kinds of tasks that should be automated, you often end up with islands of automation within an organization where there are pockets of knowledge and different approaches to doing this. And many organizations these days kind of deal with this by having some kind of central team or initiative that the rest of the organization can contact. It might be described as an automation best practices team or even a center of excellence. And depending on your organization's structure, it might be a specific dedicated team within the org chart, but it can also be just an informal collection of named individuals who have been working on automation projects in different parts of the organization. But there's a lot of value in having these points of contact and having people that have already worked through some of these issues who can share best practices about making this happen, how to implement it, how to push it out within the organization, and how to communicate successfully to the people it's going to affect. So rather than provide a strategy, I do just want to go over a high level of the kinds of things you should expect any automation strategy to touch upon. First, how are you going to identify the process to automate? And when you examine an existing business process, is it as good as it could be? Because if you're going to model and replicate something, well, like anything else in computing, its garbage in garbage out. If you automate an existing bad process, you just end up with a faster bad process. So if need be, be ready to have a policy to improve the existing process before you automate it. Now on top of that, how are you going to involve the current process owners, what are the impacts on them, and is it worth doing this? And when you're starting to describe an automation, it's very much worth describing the business case in plain language. Why is automation going to help? Are you mainly looking for benefits and cost savings or are you mainly trying to avoid pain points, trying to reduce the number of errors, or improve customer response time? And what are you going to do with the current resources if this works? We then figure out how you're going to build it. What people are you using, what vendors, what tools? Do you have policies around data storage? Do you have to think about certain aspects of governing that kind of information? This of course is going to involve a lot of your IT people at this point. And in planning any kind of rollout, you should consider if there's going to be a proof of concept or some kind of pilot or is the process small enough not to need that? And if it succeeds, what about the current staff? Are they going to be retrained on the new process, are they just going to be freed up to do new things, or do you perhaps need to upskill them into something new? And very importantly, what metrics are you going to measure? You see, a common situation is that when you automate an existing business process, let's say you automate something to do with customer support, you then create a situation where any issues that don't fit in the new automated process are the unusual and more complicated cases, so they get elevated to a person. And what that means is often your employees who were dealing with all kinds of customer issues, from the easy to the difficult, are now only dealing with the more difficult ones. And apart from potential morale issues, that means the average response times are likely to get worse. And if response time is all you're measuring, you may even mark this as a failure when it's anything but. So make sure you are considering the metrics when you automate anything. But one thing to consider about this, an idea called the paradox of automation, which says that over time as any automated system becomes more efficient, humans are less and less involved, but, and here's the paradox part, that smaller and smaller amount of human involvement becomes more and more important, more and more critical. So have you figured out how you're going to elevate those few situations that do need a person involved? Now, I said earlier on that one of the biggest misconceptions about automation is that it's all about cost, it's all about productivity and reductions in headcount. It's not, we can get speed and quality improvements, we can get reductions in errors, improved customer support, better governance, better data collection, and there's a lot to be said for focusing on those aspects. But as much as anything, there's a strategy that applies to everything from the team to the organization to the individual, which is to not think about automation as always requiring these huge projects and major initiatives and expensive projects with buy‑in from various stakeholders across the entire org. Think of it as more part of an everyday approach with more and more people feeling empowered to use these technologies. But does it still need technical support and education and even a shift in attitudes? Sure. But one thing that is only going to increase over the next few years is this continuous rollout of AI and machine learning features into more intelligent automation and affecting more and more roles. So being able to make use of this and thinking about how automation can augment and supplement what it is you do, whatever your role, is going to be an incredibly valuable and incredibly important business skill to have. But with that, I hope you enjoyed the course. I'll see you in the next one.


introduction to rpbotic process automation 

Course overview 
Hi everyone, and welcome to this course, Robotic Process Automation: Getting Started. My name is Tommy van Schaik, a project manager, agile implementation coach, and RPA enthusiast. Knowledge workers in today's organizations find themselves spending a significant amount of time on mundane and repetitive tasks that are right for automation, which is stifling innovation. In this course, we will explore the fundamentals of implementing software robots to aid you in these repetitive tasks. Specifically, we will focus on the basic workings of software robots, how robotic process automation can improve business processes, and how organizations are implementing RPA. After this course, you will have fundamental knowledge of robotic process automation, which will allow you to look for automation opportunities within your organization and understand the applicability of this technology in your day‑to‑day tasks. Let's go ahead and get started on this course, Robotic Process Automation: Getting Started.

Course and module introduction 

Hi everyone, and welcome to this course, Robotic Process Automation ‑ Getting Started. My name is Tommy van Schaik. I'm an IT project manager, agile implementation coach, and RPA enthusiast. Robotic process automation, or RPA, has seen a huge surge in usage in the last couple of months. A lot of organizations have realized that the people in those organizations are spending a lot of time executing non‑value adding and repetitive tasks. Robotic process automation is the use of software robots to automate and improve those tasks and processes within organizations. In this course, we're going to get you familiar with all the concepts related to RPA and how it can improve your organization. And this course is set up as follows. First, we'll do a proper introduction of robotic process automation. We'll look at the concepts and the benefits of RPA, but also the risks that are involved in RPA deployment. Then we'll look at the best areas in which you can apply robotic process automation. We will look at the lifecycle of our software robots, from analyzing the process they're automating, to the development and testing, to the maintenance of the robots. And finally, we'll look at how organizations typically implement robotic process automation within their organization to reap the benefits and make sure that we minimize the risks involved with such an implementation. Now, in this specific introduction module, there are a couple of concepts that we will explore. First, we will answer the question, why do we need RPA within our organizations? What benefit do we get if we implement software robots within our organization? Then we will zoom in on the concepts related to robotic process automation and things you should be aware of from a user perspective. We'll look at the history of RPA and why it's becoming so popular at this point in time. And finally, we will look at the pros and cons of robotic process automation and whether or not it's a good fit for your organization. Alright then, so let's get started on this module, Introduction to RPA.

Why robotic process automation 

So the first question we will have to answer is, Why do we need robotic process automation? And we'll try to look at this from a management but also an operational point of view. So let's try and demonstrate by means of a real‑life example. One international airliner recently had to cancel a lot of their flights because of the Covid pandemic. Now by law, they were obligated to refund all of the flights to their passengers, so the process they set up was simple. The passengers would just send an email to an employee. That employee would then check each individual system of the airline to check whether or not that passenger indeed booked a flight with the airline. If that was correct, then the airline would email their insurance company and, in the meantime, update their financial systems, contact the customer, and make sure the payment was received by the customer. All of this was done by individuals. This was the plan for the airline. If one of the applications was bad, then the employee would go back to the customer and start to work out the issue until they came to an agreement. But the airline quickly set up this process because they were expecting a lot of refund requests to come their way. However, when they looked at the numbers, they realized it would be 120,000 applications within two weeks. On average, it took about four minutes to process one request, so that would leave us with 480,000 minutes in total just to handle these applications. That's about 8000 hours of work, and that loosely translates into $400,000, which is, besides the fact that they had to refund all of these flights, a huge cost driver for the organization. So then they came up with the idea to apply robotic process automation for this process. They looked at the process itself and realized that a lot of these tasks are actually very repetitive and could easily be handled by a software robot. So the robot would take on the refund requests that were received from the customer, automatically check the systems, check the financial systems, make contact with the insurance organization, and make sure that the refunds actually reached the customers. If the process was executed successfully, the customers would be notified digitally by the robot, and the process would be complete. Now, of course, this is the happy flow. There was also a possibility that applications were not filled out completely or there were issues with the application. In that case, the employee would be notified, and they would actually do the double‑check with the system and, if needed, make contact with the customer to work things out on an individual level. This, of course, was a way more challenging job than actually handling the standard forms and repetitively checking all of these systems. So the employee happiness went up because they were allowed to only handle the exceptions, which made the work way more satisfying. Now if we look at the numbers, it turned out that 80% of the tasks could be handled by a robot, and the rest were exceptions. It took about $800,000 to implement. So for those of you who are quick at maths, that's $240,000 less cost for this individual transaction. So this is a good example of effective use of robotic process automation. All of the mundane tasks were automated by a robot, and the actual knowledge workers in the organization got to do more challenging jobs and actually provide more value for their time. It was also a huge cost saver and an overall success story for robotic process automation. Now as you can see, the emphasis of RPA is in repetitive tasks and taking them away from knowledge workers. And this is exactly the problem that robotic process automation solves. In the next clip, let's take a closer look at all the different concepts involved with robotic process automation and how they all tie together.

Defining Robotic Process Automation 

So after that success story, it's important that we clearly define robotic process automation and how it can help your organization. Now RPA has been around for a while, but initially it didn't really take off, and that mostly had to do with the underlying technology not being mature enough. Now over the years, the definition of robotic process automation has been blurred a bit. So in this clip, let's define robotic process automation. And at its very core, robotic process automation are software robots that are meant to optimize processes. So we're not talking real‑life robots, just software robots. And that is the basic definition. But before we can explore robotic process automation, we have to look at the way that organizations usually set up their IT and how users interact with that. So usually we have the hardware layers. And on top of that, we have applications with their own data stores. All of these applications are usually connected to APIs in modern organizations, and the knowledge workers in the organization use these tools for their daily work. Now the idea of robotic process automation is that we take a robot to first learn what the knowledge worker is actually doing on these systems and, after a while, take over the more mundane tasks of that knowledge worker so they can focus more on value‑adding tasks. But this doesn't mean that the knowledge worker doesn't have access to the system any longer. However, they can spend more time, for instance, interacting with customers, coming up with creative ideas, or helping the automation in the organization. The whole value proposition of RPA is that we use Bots to take over the boring and mundane tasks from knowledge workers and free up time so they can pursue more creative tasks. So, basically, RPA is an added layer of robots on top of the application layer in the organization. And one of the things that it has going for it is that it has a low‑tech boundary. Most RPA systems are set up in such a way that even normal business users who don't necessarily have a technical background can automate some of their processes. The more advanced robots will obviously be created by people with technical knowledge that know how to program such a robot in detail. However, the modern tooling that is used allows business users to automate most of their processes on an individual level. So an organization can implement this technology without having to pay expensive programmers. They just need to teach the business users how to use these applications. So it has a low‑tech boundary, which helps with implementation. Now, of course, process improvement systems have been around for a long time. However, those are quite different. If you're talking about business process management applications, those are basically new systems that still interact with the user and also connect to other applications in the organization through APIs. The issue with implementing these is that they do require tech people to implement them, and usually the people that have to work with these business process management applications have to undergo training, which makes the implementation more expensive. So you should not consider robotic process automation the same as a business process management application. Instead of being part of the infrastructure, RPA is put on top of the application layer. Another comparison that we usually find that is not correct is that RPA is the same as macros within applications. In theory, macros can do the same thing except they are limited to one application. RPA sits on top of the application, so it can also do the interaction between applications. This is especially useful for processes that handle a lot of data between applications. So you could consider RPA noninvasive, as it doesn't require new implementation, and its cross‑system, so it can help with multiple applications. And its core strength is to automate basic digital transactions. Later, when we discuss the concept of hyper‑automation, you will learn how robotic process automation fits in with total solutions that can automate the processes in your organization end to end. But RPA is specifically aimed at automating basic digital transactions like the refunding process we explained earlier in this module. Some other areas of application could be copying information between applications, create, read, update, and design actions on databases, log into websites, make calculations and do basic logic, and read forms and documents, and then process the information found there. Now it's important to realize that in RPA, we are talking about software robots who are not visible. It's not a physical robot that goes around and does things for you. It's a digital robot that sits on top of the application layer and helps you with basic transactions. We should also make the distinction between artificial intelligence and RPA, although there are some AI elements. But in most robotic process automation applications, AI is usually a dedicated system that makes decisions, while RPA is aimed more at automating more basic processes that do not require artificial intelligence to make decisions. Again, once we start discussing hyper‑automation, AI is definitely a factor that can be integrated with robotic process automation to also make decisions within your process flow. But we should make the distinction between RPA and artificial intelligence. All right, so that, in short, was the definition and the boundaries of RPA. But what better way to show the effect of RPA than doing a demo? So let's take a look at that in the next clip.

Demo

Now to illustrate the effectiveness of RPA for a very small, but tedious process, we will create a robot that helps Pluralsight authors with creating the primary files and documents needed when starting to write a new Pluralsight course. When we execute this process manually, at 10 times speed, that would look something like this. First you create the root folders and folders for the modules. Then we copy a PowerPoint file over to each map. We change the name of the PowerPoint file to the name of the module. And finally, we open up the PowerPoint, and we insert the new title. And this entire process takes us exactly 3 minutes, and we have to execute this for every course we create. Now, of course, we want to automate this process so we don't have to do it every time we have a new Pluralsight course that we want to start. So in order to do so, first, let's check what files we have. First of all, we have an Excel file, which basically contains all the courses and the modules that we are creating. We also have a PowerPoint file, which is the basic file that we will be copying into the different folders. Now for demo purposes, I'm not going to go through every detail of the robot we are about to create. It's just to illustrate the efficiency that you might gain by creating simple robots for a repetitive tasks that you have. Now what we want to do is we want to start up UIPath Studio, which is a tool used for business users that can actually automate some of their processes that they do on a daily basis. We want to create a new task, and let's just call this PSdemo. And what we want to do is we want to create folders and files based on an Excel sheet that we have been provided by Pluralsight. All right, so let's go ahead and create. Then UIPath Studio has something called a project notebook, which is a file that contains all the references you might need in your project. Now if you look at this sheet under tab File, you will have a location in which you can store the files that you will be using in your project. In this case, all the logic will be in the Excel file, so we'll just be using that. So we'll go to the folder and copy the path of the file over and just paste it in the Excel sheet. Now when we go back to UI Studios, we can start creating our robot. The first thing we want to do is to use an Excel file because that is a type of file that will be steering our robot. So we select the Excel file, which is located on the desktop, which I know is not best practice, but will do for demo purposes. And we want to make sure that we don't create the file if it does not exist. And the next thing we want to do is, based on this Excel file, create a folder. So we select the action Create Folder. And in order to tell the robot what followed to create, we will tell it to look in the project notebook, specifically the file folder. We copy that over because we might need that later. And let me just indicate in the Excel file what folder we want to have created. Now we want to create the title of the course as the first folder. So we select that in the Excel sheet, which is cell B2, and then we just click Save. Now this will make sure that the robot creates the title folder in the root folder where the Excel file is located. The second thing we want to do is create folders for each of the individual modules. So we do that by selecting Excel for each row. This will make sure that our robot executes a command for each row in the Excel file. So in the Excel file, we select the range of the modules we want created. In this case, it's the entire C column. So we select that column, and we make sure that we tell the robot that this column has a header so it doesn't create the first line as an individual folder, and then we simply select the Create Folder command. Then, we build up the folder as follows. First, we take the root folder again, which is the folder where the Excel file is located. And the second part of the path is the title of the course. And then the actual folder that we want to create is the current row called Modules. So we save that, and now our robot should create folders for each individual module in our course. But let's not stop there. We also want the robot to copy over the PowerPoint file and make sure that it has the right titles for us to start creating our courses. So we opened up the PowerPoint file. And in UIPath, we simply give the command Use Application. Then we tell UIPath use the PowerPoint application we just opened by simply visually selecting that. Then, as a parameter, we give it the file that we actually want to work on. So in this case, that's the root folder plus the empty PowerPoint presentation we created. Then within that presentation, the first thing you want to do is issue a Click command so we can focus on the title text we want to change. So we select Click and, visually, just tell UIPath what part of the presentation we want to change. So we select the course module title, and we click Confirm. Then the second thing you want to do is actually type text, so we select the Type Into command. And again, we select the module title, and this will insert the text that we want. And we tell the robot that the title should be the current row in the Excel sheet that we're working on. So that's the Modules column. And we also want to make sure that we keep things clean, so every time we type text into a field, we tell the robot to clean up any text that might have been there beforehand. Then, the last thing we want to do is again issue a Click command to simply click the Save button. So our robot is almost complete. The last thing we have to do is to make sure we copy the PowerPoint presentation over to the individual folders that we created and rename it so it fits the title. So we do this by the Copy File command. Again, this works in the iterations we have in the Excel sheet. So as a source, we select the empty PowerPoint presentation. And as a destination, we build up the path as follows. First, we take the root folder. Then we take the course title from the Excel sheet. Then we'll go to the folder of each individual module. And finally, we give the file the name of the module and the PowerPoint extension. All right, that was it for our robot. Let's click Save and see if it runs correctly. So UIPath is running through all the different commands that we inserted into the robot. All right, so let's see if that was successful. When we go to the folder, we see that the root folder has been created, and we notice that all the individual module folders have also been created. When we open up our PowerPoint, we can see that the title has changed, and we can start working on this course. So we created this robot under 7 minutes. And next time I create a Pluralsight course, I just will execute this command and change the Excel sheet that it's looking at. Now I've created about 40 courses, but imagine we distribute this robot between all of the individual Pluralsight authors. Imagine the time efficiency that we would gain by just implementing this simple robot. So this is a good example of how you can automate a repetitive process so it doesn't take up a lot of your time, and you can focus more on the creative side of things. Now, of course, you might have noticed that there's nothing very specific about this technology. So the question arises, why is robotic process automation only taking off at this time? So next, let's explore why robotic process automation is so popular at this time.

Why Robotic Process Automation now

So after watching the demo, I hope that got you inspired to automate some of the processes in your daily work. I'm quite sure if you look closely, there are enough processes that you do on a repetitive basis that you could automate. However, the question arises, why is RPA only taking off now? It doesn't seem like there's a whole lot of cutting edge technology implemented at play when robotic process automation is executed. And there are a couple of reasons why RPA Is taking off now instead of a couple of years ago, and the first one is called the law of the handicap of a head start. Yes, that is actually a thing. It's a Dutch saying translated into English, but it is official. The other one is tech advancements, regulations, workforce expectations, and cost savings. These are things that drive robotic process animation, and they ensure the big spike in usage that we are seeing now. So when we look at the first one, law of the handicap of a head start, that basically means if you take the graph, Effectiveness and Time, and you look at organizations that are doing average, you will notice that innovators who usually pay a lot of money to get the latest and greatest with regards to technology, and that can be a very costly business. If you implement new technology, that is usually way more expensive than if you are a late follower and implement more standardized and tried and tested technology. So the graph of organizations that are usually trailing new technologies looks more like this, and it's less expensive for them to invest in new technology. Now what this theory boils down to in practice is that when you have legacy applications, there is usually a return on investment on these applications. And a lot of companies know that there is better technology out there and know that their IT department can probably use an upgrade, but because it's so expensive and they did not get the full value out of their investment yet, usually they delay in investing in the technology. However, RPA, because it lies on top of this application layer, can be a viable, cost‑effective alternative. The second thing that is driving the spike in usage at this time is technology advancements. All technologies like screen scraping are now at a maturity level that they can actually effectively be used in organizations. This is one of the underlying technologies of most RPA platforms. Then there is also an increase in low code platforms. And later, we will discover that the combination between RPA and low code platforms is actually a very valuable one, which allows you to prototype your products in a very effective manner. Also, the text and image recognition software is getting better, which is also one of the underlying technologies that is needed for RPA. Then there's AI and machine learning, which are not necessarily implemented in the RPA applications that we see; however, they do provide us with an opportunity so that the robots learn more easily so it can be better suited to automate processes. And finally, there's cloud technology, which also allows RPA to scale out more significantly. So, technology is also one of the driving factors that makes RPA so successful right now. Another one is regulations. RPA has a very easy way of tracking and auditing. So, with regards to regulations, there's way more scrutiny on organizations than there used to be. There's legal requirements, there's privacy requirements, local laws that might apply to you, financial constraints that are put on the organization, and there's usually industry standards that they have to adhere to. RPA, because of its transactional nature, could help you in these regulatory requirements. Another aspect that should not be underestimated is workforce expectations. A lot of new knowledge workers that come into organizations expect to add value by means of creative pursuits. They don't necessarily want to be bothered with these mundane administrative tasks that usually take up a lot of their time. There are numerous studies out there, but it turns out that about 40% of our time is actually wasted on repetitive tasks that can easily be automated. The entire promise behind robotic process automation is also that it makes work more interesting. And, of course, lately, because of the pandemic, the need for cost savings has risen within organizations. So that is also a driver for robotic process automation because all of the manual labor that you don't have to do for these repetitive tasks are actually costs saved. Alright, so those are the five main drivers that make robotic process automation a success at this time. In the next clip, we'll take a look at the pros and cons of robotic process automation, whether or not it's the right technology for each organization, so let's take a look at that next.

Proa and Cons

So obviously there are a lot of benefits to robotic process automation. However, it would not be fair to say that there are only benefits. There is also some risk involved with implementing RPA. So let's take a look at the pros and cons of robotic process automation. The pros can easily be summarized as high return on investment, it's easy to implement, it's easier to reach compliance within your organization, we find that there is better employee and customer satisfaction, and it's very easy to upscale your RPA endeavor. However, there are also some downsides and risks. First of all, the cost of ownership can be high, the risk of technical depth is always present because of RPA, there are some challenges with governance of all the robots when you're using RPA at scale, there are always the security concerns that are related with new implementations of technology, and there are still technical limitations that might reduce the effectiveness of your RPA implementation. So let's first take a look at the return on investment. Obviously, this is one of the great drivers behind RPA, and, as you can imagine having a robot taking away all the repetitive tasks of a human can be a huge cost saver with with regards to labor time. As you can imagine, implementing a new application will require, first of all, the application to be installed in the infrastructure, but also to be connected to the different systems in the organization, and you will require programmers to tailor the system to your specific organization. Usually there's training involved for people using the system, so RPA can provide a huge benefit instead of implementing new applications. So when we look at the cost savings, usually there's money saved because less effort has to be done by the knowledge workers in your organization, there's increased accuracy, so there's less risk of your processes having to be redone because the data is inaccurate, and lastly, because time will be saved on these mundane tasks, there is more time for innovation between your knowledge workers and that will increase the innovation in your organization, which is obviously hard to put a price on. Second benefit of RPA is that it's easy to implement. As stated before, RPA is nothing more than a new layer on top of the application layer. This is very easy to implement because you can basically mimic the actions that a human does and put them in the logic of a robot. So that makes for easy implementation of RPA, which is definitely a benefit. However, for the more advanced robots that do end‑to‑end process automation, there will be process analysis, maintenance changes, metrics, bot creation, and training involved, and those are aspects we will cover later in this learning path. However, the basic premise is that it's very easy to implement RPA in your organization, and you can quickly gain benefits from implementing. These long term larger implementations can be handled as a change process, as any other in the organization. Then the third benefit is better compliance. Again because of the audit trail that is possible and the transactional nature of robotic process automation, better compliance can be achieved by your organization. So we'll make less mistakes, there is an audit trail that can be followed so you can always trace back what happened, and the knowledge in your organization, instead of being in the head of your knowledge workers, is now in the robot that is programmed, so it's easier to retrieve the knowledge in the organization, which can also be a huge benefit. There's also better employee and customer satisfaction. Again, most knowledge workers nowadays expect their work to make a difference, so they'd rather not bother with these mundane tasks that might be necessary but not necessarily add value to the organization. When we free up the time for knowledge workers, we'll have more time to interact with the customer, we'll have more time to look at creative endeavors, and we'll have more time helping in the automation of the organization, so this is definitely also a benefit of RPA. Our customers will most likely experience this by an increase in speed in which their requests to the organization are handled. And a final clear benefit of RPA. Is its rapid upscaling potential. If you have one robot, it's very easy to scale it up into multiple robots, and that can have a huge output in the organization. If you compare that to scaling up when you need to hire persons, you'll need to go through the hiring process, they'll probably require training either on the job or classroom training, and there might be a lot of contractual overhead involved. So compared to human capital, it is definitely easier to upscale a robotic workforce. It's not all benefits. There are also some inherent risk to robotic process automation. The first one is cost of ownership. Now again, the return on investment is usually pretty good with robotic process automation; however, there are some things to consider. You'll have to pay for licenses for the applications that you use, there's employee time initially needed to actually program the robots, the maintenance of the robots and the IT infrastructure will have to be included in your IT maintenance plans in the organization, and although limited, there is still training involved for the employees that will actually be creating the bots. And if you want to go into a more large‑scale implementation, you will require people to code to program the bots in your organization. Now, the second risk that comes with robotic process automation is that of technical depth. If you have your IT infrastructure and you build robots on top of that, the incentive to actually replace legacy systems is gone. So at some point there's a risk of these systems not working anymore, and then the overhead layer of robots will also cease to work, so the risk of technical debt increases because there's less incentive for management to replace these systems that the software robots sit on top on. So that's why it's so important to make robotic process automation part of your total IT infrastructure. But more on that later in this learning path. Another issue you might want to consider is governance at scale. If you have two people that both have a robot that assist them in their daily jobs, there should not be a huge governance issue. However, if you have multiple people in your organization which are all using bots, it might become beneficial to centrally manage all these robots, and most RPA applications still have some challenges in this area. And then there are also some inherent security concerns. Because RPA takes over some of the human transactions, there's always a security aspect that has to be considered. And lastly, there's still some technical limitations that make RPA imperfect. And these mostly evolve around the area of artificial intelligence and machine learning because those technologies are not really mature yet, and if you do incorporate them in decision‑making points in your processes, then you might be taking on more risk in your organization. And because of the screen‑scraping technology, it's also challenging to remotely use RPA applications. Now this has become better in the last couple of months, but there's still some challenges for most RPA vendors out there. Alright then, so that was an overview of the pros and cons of robotic process automation. In the next module, we will take a look at the specific processes that are best suited for automation through robotic process automation. However, let's first do a recap of this module.

Module recap

So a small recap of this module. This course was an introduction to robotic process automation. First, we looked at the reason why you should pursue RPA in your organization, and we realized that RPA can help you free up your knowledge workers by taking away their repetitive tasks and making sure that they can pursue more creative endeavors that might be more value adding than repetitive tasks. Then we did a definition study of RPA, and we realized that RPA are basically software robots that sit on top of the application layer of your IT infrastructure and can be defined as software robots that help increase the efficiency and effectiveness of the processes in your organization. We continued on by providing a demo to explain what RPA is. Then we provided a bit of history on the robotic process automation and why it's only taking off recently. And lastly, we looked at the pros and cons of robotic process automation. The approach mostly has to do with the fact that it has a high return on investment, it's easy to implement, and provides you with an audit trail that can be beneficial for your processes. But there were also some downsides to RPA, which is that the technology is sometimes not mature enough, there might be high licensing costs involved, and governing software robots at large scale can be challenging for organizations. So in the next module, we're going to explore what processes are best suited to be automated by means of robotic process automation. Thank you for watching, and I will see you in the next one.

Module Introduction

Hi, everyone, and welcome to this module, Where to Apply Robotic Process Automation. In the previous module, we learned that robotic process automation is the application of software robots in order to optimize your processes. Because of the specific nature of software robots and the many, many types of software robots available, it's important to know what type of processes are most applicable to be optimized by software robots. In this module, we will learn how to select the processes that provide you with the most benefit when applying robotic process automation. And we're going to approach this module by first classifying robots. There's so many types of software robots out there, and there's so many ways that you can program them that it's important that we classify the robots to give you an impression of the possibilities of RPA. Then we'll go into detail about what processes are best automated by the use of software robots. And finally, besides automation, we will look at alternative uses of RPA that might provide a good business case for your organization. All right, then. So let's get started on this module.

Classifying Robots 

So when we look at the vendors of RPA software, there are many options available to us. There are many ways in which software robots can be configured, and it's important that we classify those robots to see what is possible and what can best be applied in your organization. Now in order to take a structured approach to classifying robots, it's important that we determine some characteristics. So first of all, we will look at the attendance of robots, and basically we have two versions. We have an unattended robot which will operate all by itself, and an attended robot that has some human intervention. Then we'll look at the intelligence of the robot, so whether or not it is based on business rules, whether or not it does analysis over multiple data sources, or that it is a self‑learning system. We will look at the way that the robot operates with data, so whether or not it uses structured or unstructured data, and in what way it does analysis of data or manipulation of data. Then there is the interaction of the software robot with the actual software, so that can be directly on the back end, on the application layer, or mimicking input from the user. And lastly, we will look at the way that processes are defined within the robot, so if we have any specific triggers for this robot and how it handles the flow between different steps in the process. So let's take a close look at each one of these. When we look at the attendance, you have an unattended and an attended robot. To give an example of an unattended robot, let's say we have a process that starts when a file ends up in a specific folder. In that case, the software robot scans the document, and it determines based on some business rules, whether or not this document has sensitive information. If it does, the software robot sends a warning to the user. If it does not, it deletes the document from the folder, and everything is good to go. Now there are two endings to this process. It either ends in a warning to the user or it ends cleanly, and there's no action taken by the robot besides deleting the document. Now in this specific process, there is no human intervention. The moment the file ends up in the folder, the robot takes over and it executes everything unattended. On the opposite side, we have an attended process. If, for instance, after the sensitivity check, if there is sensitive data in the document, we let a human examine the document in question, and if there was no bad intention, then a notification is sent to the user. But if there is bad intention, then a warning is sent to the user. This again results in two different endings. The difference is that in the first process, there is no human intervention, and in the second process there is some human action that needs to be taken within the process and we refer to this as an attended robot. This is an important characteristic to keep in mind when setting up your robot. Then besides attendance, we also have intelligence, and this is the way that the robot receives input from the environment. This can be based on business rules, so that is the logic to be programmed into the robot, but it can also be based on analysis. A good example, for instance, is a robot that scans social media, and based on analytic data, it makes certain decisions. And we can also use machine learning and artificial intelligence within a robot to make it a self‑learning robot. Given the state of robotic process automation at this time and the maturity of the different tooling out there, it's safe to say that most robots are now based on business rules, and there is only limited capability with regard to analysis and learning systems. However, it is important to characterize your robot based on the intelligence it receives. Next to intelligence, we also have the way that your robot interacts with data, so it might handle structured data, but it might also handle unstructured data. As we will learn later in this module, software robots are usually better with handling structured data as opposed to unstructured data, but we will provide more detail on that later. It's also important to specify how your robot will analyze data that is sent to it and how it does data manipulation. You need to specify these things up front in order to select the best tooling for your robot, but also know how your robot will interact with this environment. Another important aspect is the interaction of the robot with the rest of the software systems around it. So you might interact with the back end of applications, so you plug in directly into the API or database. You can operate on the application layer like we did in the demo in the first module. There we operated directly into the Excel application, but it's also possible to mimic the input that is usually given by the user into the application. We also provided an example of that in the demo in the previous module, where we interacted with a PowerPoint application based on clicks and things we typed into the application. And lastly, it's important to identify what the process that the robot follows actually looks like. What are the triggers of the process, in this case, a file that is put into a folder, or whether or not it's sensitive data, and what are the actual steps that the robot has to go through in order to be effective? So the process is also something you want to define when setting up your robot. Now all of the different RPA vendors out there will have different ways in which these aspect of robots are implemented. However, it's important to realize what the needs of your robot are and how you can actually realize those. When we take the demo of the previous module as an example, we can characterize this as an attended robot because it was triggered by us once we selected the Excel sheet, the intelligence was based on business rules that we programmed into the robot, and the data that it used was structured data, and it manipulated data straight into the PowerPoint. With regards to interaction, we interacted directly with the Excel application, and for the PowerPoint application we did that by mimicking input from the user. The trigger for the process was us executing the robot, and the flow that it went through was the first look at the Excel database and then taking an input and inserting it into the PowerPoint. So this overview is a good way to classify a robot and see what characteristics it needs to operate and how you can best implement the robot. Now in the next clip, let's take a look at what processes are best suited to be automated by means of robotic process automation.

What processes are suitable for RPA

So robotic process automation is very suitable to automating repetitive processes. However, when we look at organizations, you will notice that there are lots and lots of processes going on in an organization at any given time. It's important that we realize what processes are most suitable to be automated by means of robotic process automation. It's also important to keep in mind that when we have complicated processes that go through the entire organization, usually one specific individual only has a view on a part of this process. So when your robotic process automation is just based on individual robots, you might not necessarily be able to automate the process end to end. One thing we'll realize later in this course is that most organizations use what we call a grassroots approach, in which multiple individuals have different robots that they apply on an individual level. So they take their part of the process and automate that by means of a robot that they program themselves. However, most organizations also take a top‑down approach. Usually this is done by a center of excellence. Robotic process automation experts are located, and they create large robots that can oversee entire processes, and they focus on automating processes end to end. Now let's explore what type of processes are best suited to be automated by means of RPA. First of all, it's important to realize that a robot likes nothing more than repetitive processes. For humans, it's very boring to do the same thing over and over again. But robots excel in this. So when we take, for example, this graph, where on the one axis we have the effort, and on the other axis we have time, let's say we have the time we spend on normal processes and the time we would spend by creating a robot. Now if you have a process that is not very repeatable and only happens every once in a while, in that case, the total amount of time spent on this process is not really all that much in total. However, if we create a robot initially, that will take us quite some time to set up. We need to look at the process and analyze it, and we need to program the robot. And usually, down the line, you will also have a little bit of maintenance on the robot. So when we take the time that it actually takes to create this robot, and then we look at how many times this process will be repeated, in this case, it doesn't really make for a good business case to apply RPA to this specific process. However, there might also be a process that is repeated many, many times throughout its entire lifecycle. In this case, the total time spent on this process might be significant. If we apply the building of a robot on top of that process, then we get a different calculation. And in this case, it might be very suitable to apply RPA to this process. So when we look at the type of processes in which you could apply RPA, you should definitely look for repetitive processes. Another item you want to take into consideration are processes that take a long time to execute. For instance, there might be a lot of waiting time or errors involved in the specific process. If these processes take so long, but are easily automated, in that case, your calculation would look something like this. The process might not be repeated that often, but creating the robot might still be beneficial in this case. A good example here is when the robot searches through multiple pages of documents to look for specific information. When a human has to do this work, it can be very time‑consuming. But if you can easily program a robot to do so, that might be a significant timesaver for you. So we also want to look at time‑consuming processes to automate. Then the viewpoint you want to take into consideration is that robots like to have business rules and logic in order to operate, but we humans are better at abstract concepts. So you want to apply robotic process automation in processes that have a lot of business rules in them, while it might not necessarily be feasible to apply robots for subjective decisions. So rule‑based processes can definitely be automated. However, subjective processes that require some sort of judgment to be done by a human might not necessarily be very suitable for RPA. But also, when we compare software robots with artificial intelligence, you will notice that software robots have a better time handling structured information, while machine learning and artificial intelligence are better with unstructured data. So that's also a differentiator with regards to processes that are suitable for automation by RPA. One thing to also consider. When we use macros to automate processes within applications, that can be a very suitable solution. However, one of the strengths of RPA is that a mimic humans, and they usually work in multiple applications and so will the software robot. So processes that are executed in multiple systems are usually also good candidates for RPA automation. And the last thing you want to look for in your processes are processes that might be very easy to execute, but are very error‑prone. If an error occurs within the process, it usually takes a long time to execute because you have to put certain safeguards in place. So if you look at a process like this, that usually takes a short time. But if there is an error, it takes a long time to execute, then this also might be a suitable candidate for robotic process automation. When we look at the totals, it might look something like this. We have multiple small iterations of this process, but also some pretty lengthy ones. In that case, it might be a good business case to apply a software robot. So you also want to look at processes that are error‑prone. Now in order for this list to be complete, we will also have to look at some examples of things that are not feasible for RPA. An example of a process that you don't want to use robotic process automation for is a process that is constantly changing. Then the graph would look something like this. We create the robot, and after four iterations of the process, the process changes, and we have to create another robot. This type of process you see a lot in legal environments where the law changes every so often. In this case, it can be very tricky to keep programming robots based on business rules if they constantly change. Then, after a couple of iterations of the process, again we have to change the process, and we have to change the robot. And in the end, when we do the calculation, we might have a lot of iterations of the process that we automated, but it also will take a long time to keep these robots up to date because the business rules are constantly changing. So in short, it might not be very feasible to have a very changeable process be automated by software robots. And to close out this list of things not to automate, it's also physical products. In robotic process automation, we are talking about software robots and not physical robots that, for instance, can move boxes around in your organization. Obviously, there are solutions that can do this, but that is not within the scope of robotic process automation. So this list is a good basis that you can use to assess whether or not the process you have in front of you is actually suitable to be automated by means of a software robot. I will add an overview of typical processes and industries in which software robots are applied. You can find this in the learning material added to this course. But besides automating specific processes, there are also other use cases for RPA. So let's take a look at those in the next clip.

Other use cases for RPA

Now besides the automation of processes, there are also use cases for RPA that might be beneficial for organizations. Now these are not commonplace, and this is an area of robotic process automation that is still under development. But these use cases definitely show promise and might be applicable for your organization as well. The first other use case is rapid process prototyping, and this is basically where RPA is used to quickly look at the best way to optimize processes and look for ways to explore optimization options. So let's say we have a process in an organization, and that process is executed by means of two different systems. Now we have multiple people working on these systems, and they execute this process on a daily basis. Now, if we want to change this process in a classical way, usually we would come up with a big design up front, and we would do an analysis to see how we could optimize this process. Then we would initiate a project, hire developers to make changes to the information systems, and then after some testing at some point in time, we would switch everybody over to the new system and the new process would be executed. Usually these will be lengthy processes and there is a lot of room for delays and errors. However, if we use rapid process prototyping, we would take a subset of the population using the actual information systems and provide them with a software robot. Then the software robot would interact with the systems and make the changes in such a way that the new process would be initiated. So we would have a small design up front that incorporates the different changes that we have, but we don't necessarily have to do an extensive research on the optimization of the process. Then we take an iterative approach where, let's say, every month we evaluate the success of the process and we build upon the success of the previous iteration. So we continue this on until we reach a point that the process is optimized. If we then at some point realize that we are moving in the wrong direction with this particular process, then we would just change the process and continue on a different route and different iterations. This is a very agile way of approaching process prototyping, and because there's not a lot of people using this particular robot, the impact of making the wrong choices with regards to the process are limited. Good examples of this are insurance agencies that are always looking for ways to optimize their processes. They usually take robotic process automation and a small subset of users to test a particular part of the process, and if it is successful, then they will apply it to the rest of the organization. So you can use RPA to quickly see if new process designs actually make sense and are effective for the organization. Another use case is legacy replacement, and legacy is definitely a problem that a lot of organizations are struggling with. See, when we have a bunch of systems in an organization that people are working on, as explained in the previous module, this is usually built on a hardware layer, and there is a connection between the different applications by means of APIs. Now the user operates on each of these systems, and if we use robotic process automation, usually there's a robot beside this person that can also take care of some of the processes. However, if we push this automation to a new level, we can actually take the individual out of direct contact with the applications and have them work only with the robots that we create for them. In this case, we effectively added a new application layer on top of the applications. Now what this provides us with is an opportunity to start creating a new system with new processes and new capabilities, and we connect that system up with the rest of the systems and also make sure that the robot that the individual is working with at some point connects with the new system. So now we are in an interim state where the user can use the legacy applications but also the new application, and then we simply start turning the old system off. This is an approach that actually uses a new layer on top of the software to abstract away some of the issues that users have with migrating to new systems, and it provides you with an approach to replace legacy systems in a relatively risk‑free manner. Because you have the new application layer that is based on both the old system and the new system, you can take a phased approach into getting rid of your old legacy systems without doing a big‑bang approach. So this is another use case for robotic process automation with regards to replacing legacy systems. All right, then, that was it for this module. Let's do a small recap before we go into the different phases of a software robot lifecycle.

Module recap

All right, for a small recap of this module. First, we looked at classifying robots, and we learned that it's best to classify robots based on their characteristics. So we can look at the attendance of robots and whether or not we need humans to be available when executing the process. We can look at the intelligence of the robots, so that is the means by which it makes decisions. And we learned that in the current state of the art, most software robots use logic as a way to interact with the environment. Another viewpoint is the rate that the robot handles data. We can also look at the way that the robot interacts with different applications. And finally, we will have to specify what the triggers and the different process steps are that we program into our robot. Then we looked at the type of processes that we can automate. And we learned that repetitive processes that usually take a long time to execute are best suited for automation. We also learned that it's feasible to have your robot operate based on logic instead of subjective criteria that usually require human input. Finally, we learned that the robots we have today still operate best based on structured data instead of unstructured data. Finally, we looked at two alternative use cases for RPA, one of which is rapid process prototyping where we use RPA to quickly see if processes can be optimized effectively. And we explained the use case in which you can use RPA to help replace legacy systems in an organization. In the next module, we're going to take a look at the lifecycle of software robots and what steps you need to go through to effectively apply a robot within your organization, ranging from analyzing the processes to developing the robot to the maintenance phase. Thank you for watching, and I will see you in the next module.

Module introduction

Hi, everyone, and welcome to this module on Software Robot Lifecycles. As we learned in the previous modules, it can be very beneficial to have a software robot aid us in repetitive tasks. Some processes are better suited to be automated than others; however, when creating software robots, it's important to focus on the entire lifecycle of a robot. Most companies fall into the pitfall of creating software robots as fast as possible to help their employees with automating their repetitive tasks; however, as we start building more sophisticated robots, there's also best practices that we need to follow in order to effectively deploy and maintain them. This will provide the organization with the lowest possible cost of ownership. So when we look at the robot lifecycle, there are a couple of different steps we have to discuss. First of all, there's the discover phase in which we look for the processes that are best suited for automation. Then we have to design phase of the robot where we actually take the different steps from the process and look at which ones we should automate. Then we actually develop the robot, we test the robot, we deploy it into our infrastructure, and finally, we maintain the robot. Now, during this entire lifecycle, it's also important that we keep measuring the performance of our robots to see if it actually provides benefit to the organization. So we will go through each of these steps in this module, looking at the best practices and tooling with regards to this part of the robot lifecycle. Alright then, so let's go ahead and get started.

Process Discovery

Now in the robot lifecycle, process discovery is the first step. This is where we identify and assess the processes that are best suited for automation with our software robots. Now because of the vast number of processes that usually exist in large organizations, we basically have two approaches to determine what processes are most suitable for RPA. The first one is where we use the logs of the organization and execute a process called process mining. And the other one is where we do process modeling and where we basically model the processes that exist within the organization. Now in process mining, we take a look at all the different logs of all the applications that are used in the organization, and we make the connections between them. This is a very sophisticated way of looking at processes, and it will give you a very rich insight on the processes that exist within an organization. Process modeling, on the other hand, is where we use workshops and interviews to determine what the processes in the organizations look like from the eyes of the business users. Now process mining requires a very specific skill set, so usually this is executed by the robotic process automation center of excellence that exists within the organization. The process modeling does require some know‑how with regards to modeling techniques, but it is feasible to be executed by the different business users in the organization. So when we compare the two techniques, both of them will give you an insight of the processes that exist within the organization. The upside of process mining, however, is that it will give you a lot of extra information with regards to usage and the different roles and responsibilities within processes that usually take a longer time to figure out when you apply the process modeling techniques. So one of the upsides of process mining is that it will give you the different actors in the organization, whether or not that is a human worker or an application that does the work; it will give you specific metrics about the process, so how many times it's executed, how long it usually takes, what the waiting times for each individual step are, all of these things can be extracted from a different logs in our organization. The downside, however, is that it does require tech knowledge, so that's why it's usually executed within the center of excellence in an organization. And it does require specific tooling because you'll need some sort of software tool to go through different logs in the organization to discover the best processes to be automated. When we look at process modeling, the upside is that this is a process that can be executed pretty rapidly, but it is very prone to error because usually people don't have a complete overview of the process that they are actually working on, and mapping a process end to end is definitely a skill set, and it does require the individuals that do the modeling to have some modeling knowledge. But depending on the software and modeling techniques that you use, the output of both methods is usually something like business process management notation diagrams, UML diagrams, or flowcharts that actually capture the processes within the organization. However, there is one aspect that we also need to mention, and that is that there are a lot of processes available within organizations that could be suitable for automation. When we look at a top‑down and a grassroots approach to actually implementing RPA, the center of excellence will usually take care of the very complicated end‑to‑end processes that exist within an organization. These are processes with multiple stakeholders and a lot of actors in them, and it could take a long time to actually map these processes. This is why the center of excellence usually takes the process mining approach because it's more time effective when looking at these large processes. On the grassroots level, we usually have smaller processes, but we do have great insights on where we can apply the robots in order to optimize these processes. The trick here is to focus on process sourcing. So what that means is that within the organization, we actually allow the business users to make contact with the center of excellence and provide them with insights on the processes that they are working on. This will make sure that the center of excellence in an organization has access to all the process knowledge that exists within the knowledge workers of the organization. This will give you the best chance of automating the most suitable processes within the organization, so process sourcing is also something that most RPA vendors provide tooling for within an organization. Now one of the aspects of this process discovery is also assessing the process, as we discussed in a previous module. You want to look at how repetitive the process is, whether or not it's time consuming, rule based, and whether or not it uses structured data or unstructured data, and if it is applicable to multiple systems and error prone. If you apply the process mining technique together with modeling techniques and you source the way and which processes are discovered within the organization, you will have an effective process of identifying and assessing the best processes to be automated by RPA. But after we have identified the processes, it's time to design the robots that will actually take care of the more repetitive tasks. So let's take a look at that next.

Designing Your Robot 

So now that we have identified the processes that are most suitable for automation, it's time to design our robots. Now what we want to identify is obviously whether or not this will be an unattended or an attendant robot, what type of intelligence the robots should have, the data and the interaction that the robots should display, and the process that we're trying to follow now. The approach you want to take when designing your robot is that you take the process that was identified in a previous step, and then you start detailing it out. So basically what that means is that each trigger, decision, and action that is taken within a process should be assessed for feasibility for automation. So we took the processes from the discovery phase, and we want to make sure that we add more detail and see where or not it's feasible to automate the specific step. So with regard to triggers, we need to identify whether or not the trigger is a human that initiates the start of the robot process or that it is some electronic way of activating the process. This is a very detailed step that needs to be identified in detail. Later in this learning path, we will have a lot of opportunity to look at the different steps in the process and the different assessment criteria we have for making sure that it is suitable for automation. But one of the things you want to be absolutely clear about is what the triggers for the process are and what the triggers for ending the process are. So with regard to decision moments in the process, we have to look at the data but also whether or not it requires physical input or digital input and what the business rules are for actually making that decision. So if they're objective criteria for making decisions on this process, or is it subjective criteria, objective criteria usually meaning business rules that exist within applications and subjective criteria are usually decisions made by humans in the process. This is very important because it's hard to automate subjective decisions in the process. Then we need to look at the data sources that provide us with the input needed to make the decisions; we have to look at who has decision‑making authority, so if there's actually legislation that determines that humans should make the decision; and we need to clearly identify the decision‑making criteria, so that means the business rules that are associated with making the decision. The decision moments in a process are usually the hardest to automate. And then there are the actions that need to be automated. Now obviously, if there is a physical component to the actions, it's hard to apply a software robot. But the suitability of software robots or human interaction is also important to be determined. So we definitely want to look at the data that is used to make the action and whether or not it needs to be manipulated. We want the actual action to be performed to be explained in detail and see whether or not a software robot can actually handle this action. We want the desired outcome of the action, so the updating of a file or, again, the data that is manipulated. And we want deviations to the action, so what happens if something cannot be manipulated and what happens if the unhappy flow needs to be initiated within the process? So the actions in the process that need to take place also need to be defined and we can determine whether or not a robot it's most suitable to do so. Now with regards to tooling to designing your robot each RPA vendor will have their specific tooling to design your robot. However, most RPA vendors will provide you with the opportunity to go from process modeling to actually data rules, to implementing the robot. So they will have tooling in which we can use process models and then put another layer on top of that process model to see whether or not it is suitable for robot automation. After that, it's usually a simple step to actually develop your robot based on the input of the process model and the business rules we initiated there. So in the next clip, let's take a look at how we can best develop and test robots using the software that is provided by most RPA vendors.

Robot Development and Test

So after we have designed our robots, it's time to develop and test them. Now this is obviously where vendor‑specific software comes into play. Now it's important to realize the difference between top‑down robot development and grassroots robot development. When we have the center of excellence within an organization that does top‑down development of robots, usually they create these big RPA robots that look at significant processes within the organization that are suitable for automation, and it takes a long time to develop these robots with a lot of testing involved. However, when we look at the grassroots approach, there are usually smaller robots, and they require less technical know‑how. So bot development tooling usually comes in two forms. One is a sophisticated, technically driven application that allows for sophisticated robots to be created. However, on the grassroots level, we have a what‑you‑see‑is‑you‑get type of approach with low‑ or no‑code platforms that allow business users without technical know‑how to create robots for themselves. This is where the automation on a desktop usually happens. Now bot development tooling that is technically driven is used for the most sophisticated robots, while the what‑you‑see‑is‑what‑you‑get or low‑code or no‑code platforms are better suited for small robots that help with specific parts of specific processes. And with regards to test tooling, most of the vendor software out there that has test tooling embedded in it is very technically sophisticated. But the testing that goes on on the grassroots level usually consists of users checking each other and see whether or not the robots work. So we see on the grassroots level, there is usually a more user‑driven approach to testing because the processes are less sophisticated and there's a lot less errors in these processes, while the big robots that are created by a center of excellence usually involve different test cycles and specific test tooling that exists within the development suite of the vendor. Now as organizations start to develop more robots, it's quite possible that one of the robots that is created on the grassroots level that after we do some metrics on the robot, it seems to be a very effective robot that has a very high return on investment. If this happens, typically, we see the robot being taken into the maintenance concept of the center of excellence, and they start developing more sophisticated tests and development cycles on this robot in order to best exploit it and perhaps even distribute this specific robot to multiple users within the organization. This is where a grassroots robot becomes a top‑down robot, and it's part of the integration phase of an implementation. But we will talk about the way that organizations implement RPA in the next module. It's important to realize, though, that it's quite possible for robots that exist on the grassroots level to eventually be distributed within the entire organization. Now with regards to the approach to development, we definitely advise to take a more agile approach. So instead of letting the center of excellence operate as a specific project, it's important to involve the users of the process within the development cycle of the robot. So we make a dedicated team that exists within the technical people of the center for excellence, but also the business users that will be using the robot in order to take the most agile approach. That's why it's so important to realize that the robot lifecycle usually exists of multiple cycles where we develop and test and deploy a specific robot multiple times and make incremental improvements so that it becomes more effective over time. More on the approach of developing robots later in this learning bad. For now, it's important to realize that close collaboration between the center of excellence and the users of the robots is vital for effective development and testing of the robots. Of course, after testing, it's also important to deploy and maintain the robot over time. This is where we reduce the total cost of ownership of robots, so let's take a look at that next.

Robot Deployment and Maintenance 

After robot development and testing, obviously, we want to deploy the robots. Now, there are not a lot of flavors with regards to deployment of robots. We either deploy the robots in the cloud or we deploy the robots on‑premises with software that is provided by the specific RPA vendor for implementation on the infrastructure that exists within the organization. Now, we will go into the details of deployment of RPA robots and the technology behind it later in this learning path. It's important to realize that with regards to deployment, on‑premises brings with it more sophistication and probably more dedicated tooling; however, a cloud‑driven approach definitely makes for easier management of the robots within your organization. The important part, however, is not the deployment of robots, which is basically just another layer of software that exists within the organization. It is the maintenance of robots. One of the main pitfalls that exists with most RPA implementations is when we reach a certain number of robots in the organization, it becomes very difficult to maintain those robots because they operate on an individual level, and it's hard to scale up the number of robots in order to most effectively automate away the repetitive tasks in the organization. Most RPA vendors will provide you with an orchestrator that specifically looks at maintaining the robots and distributing them within the organization. So what an orchestrator does, it looks at the health of the different individual robots that exist within the organization, it monitors the connection between the robot and the applications in the organization, and it looks at the applications themselves and whether or not they are running effectively. It does this by plugging in directly with the robots, but also the APIs that exist within the different applications in the organization. So an orchestrator is basically your monitoring tool that exists within the center of excellence that monitors all the robots that exist within the organization and looks at their health status and whether or not they are still effective. An orchestrator is vital to scaling up RPA within your organization. Now, of course, next to managing the robots, you also want to make sure that you measure their effectiveness, so let's take a look at how you could do that within large organizations.

Measuring robot effectiveness 

The last important piece of the robot lifecycle is measuring how the robot is doing. Now, as we can see in the graph, measuring happens in each phase of the robot lifecycle, and it is an important aspect of the implementation of robots within organizations because it allows you to monitor your return on investment, but also to celebrate successes within the organization that will help you tremendously during the implementation. So obviously, we want to know how our robots are doing, and there are a lot of different ways we can measure the success of our robots. But typically, when we look at the way we should measure progress, we can use metrics that are collected manually at the beginning of our automation and at the end of the automation and see whether or not there is a difference. We can use the orchestration tooling that exists within the organization because usually, besides monitoring, it also allows you to collect metrics for the robots, and we can use process mining to see whether or not our automation made the processes more productive and effective within the organization. Now, the typical metrics we want to look at is the frequency in which processes are executed, the effort that it takes to execute the process, and the number of actions that need to be taken by humans, either before or after automation, we want to, of course, look at the cost, so that is the cost gained by doing away with the manual labor, but also the total cost of ownership of the robots, we want to look at deviations in the process and where processes are not being run as effectively as they should have been, and we look at errors and quality, so that is, how effective our robots are it actually executing the different processes and if there are a lot of errors involved. Because as we will come to learn throughout this learning path, errors are definitely a great source of cost within robotic process automation, so you want to limit them as much as possible. Now, during this learning path, we will provide a lot of detail on the metrics that we will use to effectively monitor our robots, but it's important to keep in mind that this is definitely a part of the robot lifecycle and should be taken into consideration from the get‑go when organizations start implementing robotic process automation. Now, when we look at the different phases for measuring robots, there are a couple of phases where we want to pay special attention. One is on the discovery phase, we want to baseline the process that we are currently examining. If we have to metrics down of that particular process without it being automated, it will give us a good baseline in order to measure the effectiveness of the robot over time, and during the design phase, it will allow us to create a business case, and eventually, in the deployment phase, we will be able to measure the benefits of our actual robot deployment in the process. So that's why it's important to keep measuring the effectiveness of your robots throughout each aspect of the robot lifecycle. Alright then, that was it for this module. Let's do a small recap.

Module recap

For a recap of this short module, we took a short overview of the software robot lifecycle, and we realized that in the discovery phase we have to identify the processes that are most suitable to be automated by means of software robots. This is done by either process mining software, or it is done by modeling the processes that exist within organizations. Then we looked at the design phase, and we realized that it's important that we identify each specific step in a process and determine whether or not that specific step, trigger, or decision is suitable to be automated by means of software robots. Then we looked at the development phase and testing phase of robots, and we realized that this is mostly dependent on software created by RPA vendors. The deployment of software robots either happens in the cloud or on premises, and the maintenance of robots is usually done by orchestration tooling that is provided by RPA vendors, which allows for the scaling of robots and the measuring of performance of robots. Lastly, we learned that it is important to measure the effectiveness of your robots throughout each phase of the robot lifecycle. This will allow you to create a baseline of the process, and once you have implemented the robots, you will be able to measure the effectiveness of your robots in the process but also the effectiveness of the robots over time. Now throughout this learning path, we will go into great detail for each of these aspects throughout the robot lifecycle. But for right now it's important to realize that it's not just about developing robots but also discovering the right processes and deploying and maintaining those robots within the organization in order to maintain the highest return on investment. All right, then, that was it for this module. In the next module, we will look at how organizations typically implement robotic process automation and what the different steps are for a successful implementation.

module introduction

So now that we have covered some of the basics of robotic process automation, it's important that we zoom in on how organizations are implementing this new technology in order to increase their productivity. And in this module, we will zoom in on what a typical roadmap for an implementation of robotic process automation in an organization looks like. We will look at approaches at how organizations maintain their robots, which provides some unique challenges because it's not a typical software system that has to be maintained. And finally, we will look at the impact that robotic process automation will have on the workforce in the long run. All right then, without further ado, let's go ahead and get started on this module.

Roadmap

Now one of the core strengths of robotic process automation is that it is so‑called bilateral IT. In other words, you can implement it top down or with a grassroots approach. So a top‑down approach is usually when an organization initiates a center of excellence, and within the center of excellence we have large software robots that are intertwined with the primary processes of the organization. These large and complex robots usually require some technical knowledge and therefore a center of excellence where all this knowledge is gathered is very useful when implementing these large robots. A grassroots approach, however, is when different individuals in the organization who do not necessarily have a lot of technical knowledge, all have their individual robots that they can use to automate the processes that are important to them and to take away some of those repetitive tasks so they can focus on other things. Now the good thing about this dual approach is that it has a unique value proposition, so let's take a look at an example. In the classical top‑down implementation, the implementation typically looks like this. We have a planning phase, a design phase, a build phase of the robots, we have a test phase, and then we implement the robot into the organization. Now this is not specific to the implementation of a robot; it can also be any other piece of software in the organization. And it is only at this point that there will be a large spike in value where the organization actually benefits from the entire implementation that took place. So this is a typical implementation roadmap from a top‑down approach. When we take a grassroots approach, usually you create lots of small robots and you see the value increase over time. Now, obviously, the value doesn't increase as much per robot as it would in the larger implementations. However, it does provide value straightaway, which might be beneficial for the organization. Some organizations call this the low‑hanging fruit, and when we look at processes that are typically automated early in the process by means of individual robots that do not require a center of excellence, these are usually things like invoicing, taking notes, and automating small office tasks. Now when we combine the grassroots approach with the top‑down approach, the graph looks more like this. We have the small robots that are being implemented that provide value over time each time a robot is created, but we also have the large implementation that runs in parallel. Once that implementation has been completed, we see a huge spike in value, and again we continue with the normal robot implementations that happen on the grassroots level. This type of implementation has the benefit of small robots that instantly provide value and the large implementation of larger processes and more complex robots. Now how does this translate to an implementation roadmap? A typical roadmap for an organization looks like this. We have a management layer, we have an integration layer, and we have the teams layer. And usually it all starts on the team layer. There in multiple iterations, we select the best process to be automated, we create the robot, and then it goes into maintenance. But for these small robots, the maintenance actually means the individual that created the robot, usually a business user, takes control over the robot. And once the process changes or the underlying software changes, then the robot is maintained by the individual user. We call this stage the isolation stage because all the robots are isolated. Usually there's not a central approach to robot management, and it has not been seen as a strategic initiative yet. Then the next thing that happens is that there is a tipping point in the organization. This tipping point is when management becomes aware of robotic process automation and sees the benefits for a strategic approach. This could either be the success of individual robots that are created on a team level, as shown in this graph, but it could also be the need to replace legacy systems or some other critical business need that requires management to act on robotic process automation. Then management forms a so‑called coalition. You will also see this in the implementation of agile. This is a coalition between subject matter experts and management to make sure that there is top coverage for the large implementation of robotic process automation. And right around that time, it's also necessary to start training people on the RPA. So that means awareness courses from management, technical courses for the people implementing the RPA, but also courses on how to get started at RPA for the regular business users. So the training will be available for both management and the experts in the organization, but also the people creating robots on the local level. And part of the coalition is also setting up the center of excellence. The center of excellence is a gathering of all the experts on RPA, so usually these are business analysts, but also software engineers that can create the robots. The business users on the working level continue to build the robots, but now they have the availability of the center of excellence and the training to help them in their path to create better robots and maintaining them. This is what we call the supported phase of the implementation. Then after the organization matures further in the robotic process automation implementation, we see the number of robots increase on the working level, but there are also deployments of strategic robots initiated by management that require a more comprehensive approach and usually touch more systems in the organization. An important difference from the previous phases is that there is an integration phase where we integrate the local robots with the larger robots in the organization and make sure there is a comprehensive approach to maintaining and integrating these robots throughout the organization. And lastly, we will focus on the strategic implementation of robotic process automation. This is the highest maturity level. So after we have top‑down and grassroots robots implemented and they integrate on a continuous manner then it's time to go for a strategic approach when we select, create, and maintain robots in an integrated way throughout the organization. So people have the ability to create robots either locally or as a strategic initiative and make use of the center of excellence and the training facilities within the organization to automate their processes and make sure that the organization becomes as productive as possible. Now, the exact order in which all of these things take place might vary from organization to organization. It's important to realize, however, that all of these steps need to be executed when the organization is going through an RPA implementation, but in the end, the goal should be to have a strategic approach to robots in order to maintain them in the long run and keeping them maintained and adjusted to the processes in the organization. All right, so this is a typical roadmap. But another challenge that organizations face is how to maintain all of these robots, so let's take a look at that next.

Maintaining Robots

Now as you might remember, in one of the previous modules we discussed that maintaining robots, especially at scale, can be a difficult challenge. And this is mainly because there's a different nature to robotic process automation, as opposed to other software systems in the organization. So when we take the typical implementation of RPA, on the abstract level, robots exist on a layer above the normal software, so they interact with the different software elements in the organization. However, in reality, it's also the RPA software itself that is part of the IT organization. Now this can either be physically in the organization, or we can have the RPA software in the cloud. In either case, it is important that this software is maintained. In the cloud the vendor does it for you, but if you have the software in your own organization, you will have to make sure that you update and patch that software itself. So the first thing to worry about is the RPA software itself. Then when we have the different robots in the organization, if we zoom in on the larger and strategic robots we'll notice that they have connections with multiple systems across the organization. They interact and automate some of the processes that might stretch between all the siloes that you have in your company. Now the issue arises when one of the underlying pieces of software breaks, for instance, a server breaks or some piece of software crashes. In that case, if you zoom in on the links between the robots and this particular application, you will notice that these links will usually break. Now, of course, with well‑programmed robots they're always _____, but the robotic process automation software is generally dependent on the underlying software in order to operate well, so maintaining the target software should also be part of the maintenance of your RPA organization. But there's another unique challenge to robotic process automation, and that is all the small robots that don't necessarily have a centralized maintenance concept. All of these small robots are also connected with all the applications in the organization. Again, if an application breaks in that case, some connections to robots might be lost, and some robots might malfunction. However, if these robots are not centrally managed, you will get a lot of angry customers that you might have a hard time identifying. So third challenge is uncontrolled robots and how you make sure that those fall into the maintenance concept. Now these type of problems are not unique to IT organizations, but they become very clear what you implement RPA. So most professional IT organizations have an IT service management system that they use. There are lots of frameworks out there, and the most common one is ITIL, but there's also the COBIT framework, the ISO certification framework, the Microsoft Operations Framework, and the US body of knowledge. All of these frameworks are used to professionally manage an IT organization. Now there's lots of different flavors out there. When we look at the typical approach to these type of methods, we notice a couple of central themes that are vital for good management of robots. The first one is incident management. So the moment something goes wrong in the entire IT infrastructure, it's important that we capture the incident, and we communicate well with the business users in the organization and other IT elements. This is especially important for uncontrolled robots because people need to be aware that a part of the system goes down that their robot might have a connection with. Because we don't necessarily track all the robots in the organization, it's important for people to be kept up to date on incidents that happen in the organization. Of course also, the closure of these incidents is something that needs to be communicated well. Another element is problem management. Because we have a lot of uncontrolled robots but also the target software and the RPA software itself that needs to be handled, problem management becomes important because the root cause analysis might find themselves also in these layers of the IT systems. Change management, as we will notice, will become more comprehensive, so usually change management is something that is done to align the business and IT with security. However, if we practice change management within a professional IT organization that has a lot of robots involved in it, we need to make sure that the changes we make to the IT infrastructure do not influence all the robots that might be dependent upon it. So this is an additional check you have to do in your change management process. Now project management is also a capability in most ITSM methods. Project management is there to implement large changes in the organization. You might try project management useful while implementing these larger robots within the organization that are centrally managed. And probably the most important part of IT service management with regards to robotic process automation is asset management. We need to know exactly where the robots are and what their dependencies are with different parts of the system in the organization. If you do effective asset management, you can identify these dependencies, so the moment there is an incident or there is a change to be made, we know exactly what robots will be influenced and what where we have to make adjustments in our infrastructure. Now there's lots of tooling out there from different RPA vendors that will help you with asset management, and we will zoom in on those later in this learning path. And last but not least, there's knowledge management. It's important to keep the knowledge of the robots up to date, and there should be an integrated approach that keeps all knowledge of the robots in a central repository where it is easily accessible to all. Also, the technical details of the robots will be kept in the knowledge management system that is used by the organization. Later in this learning path, when we go in the more technical details of the robots, we will zoom in on maintaining robots in way more detail. But it's important to realize that for an organization to effectively implement robotic process automation, it's important that they include this in the total management of their IT systems. So that's why integrating robotic process automation with the IT service management approach that the organization has is so vital for strategic RPA initiatives. Now, of course, RPA will also influence the workforce in the organization. A lot of tasks can be automated, so that will automatically have an effect on the people working in the organization. So let's explore that in the next clip.

Impact of RPA

So one of the questions we receive a lot when implementing RPA in organizations is what it will do to the workforce and how it will affect people's job in the long run. Obviously, in the short term, people are busy creating the robots to automate tasks. However, in the long run, it does have an effect on the workforce and the organization. But first, let's take a history lesson with regards to productivity. So in the 1600s, in the stockings industry, there was a new way of working that increased the productivity of creating stockings that usually increased the productivity of the knitters of these stockings. Rumor has it that Queen Elizabeth the first rejected the patent on this machine just because she feared that the knitters would be out of work in the long run. The same thing happened in the 1800s with the industrial revolution. There in the textile industry, we would have the textile riots where a lot of people will be angry that they will be out of a job because of the automation of their labor. In the 1900s, we would have the car industry that really took off, and a lot of tasks there would be automated. Again, people would fear for their jobs. And the same thing would happen at the end of the 19th century when we would introduce computers into the workforce. And now we have artificial intelligence and robotic process automation. Again, people fear that their jobs will be automated, and they will be left without a job. However, when we look at history, the fact is that there were not less jobs when these things were automated. The main thing that happened is that productivity shot up by a factor of 1,000. So all of these automation techniques didn't necessarily create less jobs. They just increased productivity for organizations a lot. So this is what happens on a society level. However, when we look at an organization, let's take, for example, the Globomantics organization, our fictional organization for robotic process automation. Before RPA, we would have people working in this organization, and we would have a normal influx of business and a normal output of the organization. And of course, the budget would be between support staff and the IT department. Now the moment we introduced robots into this organization, a couple of things might happen. Now let's say we are very successful at implementing robots in this organization. As a result, the productivity will shoot up. In that case, we have a couple of options. First of all, we increase the intake of the organization and the output of the organization. So we increase the client count and, because we can produce more, we can also increase the output so we can actually deliver on the products that we're creating. A second thing that will happen is that relatively the budget for support will decrease because we need to keep less people on the payroll there. But on a relative scale, the budget for the IT will have to increase because we have to keep track of all of these robots and maintain all the robots in the organization. And the third shift that we see is that people go from a support function in the organization to actually managing robots and focusing more on the innovation and process side of things. So there's also a change in the expertise that is needed within the workforce of an organization. Now when we look at the implementation of RPA, one of the most important parts there is training. So when we have the individuals in the organization, let's say, for example, they were able to handle four clients between the two of them at any given day. However, because of automation, they can now handle 20 clients. Now this is, of course, not reasonable. So it is only natural that over time people will transition from the supportive role of clients to a more dedicated technical or process‑focused role. This is where training becomes important for the organization. Now zooming in from the organizational level to the individual level meant a lot of tasks in your organization are being automated, and you find yourself in a position that is very suitable for automation, let's say you do a lot of repetitive tasks that might be complex, but can be automated over time, then my advice would be to look at the opportunities that RPA provides because at some point, all of the tasks will be automated. It's not like we have a choice, but this is something that will undoubtedly happen in the future. So that's why it's important that you arm yourself with the knowledge about the processes in your organization and on robotic process automation and automation in general so you can apply this knowledge to make sure you help your organization in the transition to more automated processes. All right then, that was it for this module and for this course. So let's do a small recap.

Module Recap

So for a recap of this module. First, we looked at the roadmap that organizations typically used to implement RPA. We noticed that the strength of RPA is that you can use a dual approach, one where you do a grassroots approach that implements robots on the working level that interact with the applications and provide a boost of productivity there, but we also have strategic robots that will be implemented comprehensively throughout the organization. In the roadmap, we typically start out as two tracks, one on the operational layer on one on the management layer to eventually end up in a phase where we have an integrated approach between all the robots in the organization and where the robots are part of the maintenance concept of the organization. Then we zoomed in further on the robot maintenance. First of all, these robots are dependent on software systems that exist elsewhere in the organization, so we have to include that in our IT maintenance system. But we also have to make sure that the software that actually manages and creates the robots is also managed within the IT infrastructure of the organization. And lastly, we talked about the impact on workforce. And we noticed that on a society level, RPA will increase general productivity. On the organizational level, there will be changes in output and input in the organization and a different type of skill set needed in the organization. And on the individual level, it's important to keep up to date and trained in RPA and the processes surrounding you in the organization so you can be of benefit when a transition is made to more automated processes and higher productivity. I thank you all for watching. This is the end of the module and also the end of this course. I hope you enjoyed this course, and we will continue to go more the detail about robotic process automation and how you can implement that in organization in the rest of the courses of this learning path. If you enjoyed this course, please add me on LinkedIn to keep up to date on any other Pluralsight material that I will be releasing in the future.


Demonstrating the business value of power automate

course overview

Hello everyone. My name is Vlad Catrinescu, and welcome to my course, Demonstrating the Business Value of Power Automate. I'm a Microsoft MVP from Montreal Canada. Business process automation is the goal of most organizations, and Microsoft Power Automate can help you reach that goal. This course will introduce you to Power Automate, part of the Microsoft Power Platform, and show you why it's one of the leading business process automation tools in the world. Some of the major topics that we will cover include the basics of Power Automate, creating approval flows in Power Automate, as well as the different types of processes you can automate with Microsoft Power Automate. By the end of this course, you will know what Microsoft Power Automate is and you will be able to demonstrate the business value of Power Automate. Before beginning this course, you should be familiar with what the Power Platform is and, at least at a high level, the goals of the products inside. I hope you'll join me on this journey to learn about Power Automate with the Demonstrating Business Value of Power Automate course at Pluralsight.

module introduction 
Hello, and welcome to this Demonstrating Business Value of Power Automate course. My name is Vlad Catrinescu, and I'll be your instructor for this course. I'm a Microsoft MVP from Montreal, Canada, and you can find me on Twitter @vladcatrinescu or follow my blog at VladTalksTech.com. In this module, we will do an introduction to Microsoft Power Automate. We will first introduce what Power Automate is as a product and what it can do for you. We will then learn some of the terms and concepts in Power Automate such as triggers, actions, as well as the different types of flows or automations we can create with Power Automate. Lastly, we will learn the basics of the Power Automate user interface and know were to go to create our flows or to get information. By the end of this module, you will know what Power Automate is and have an understanding of the basic terms and concepts, which will allow you to create different types of flows in the next modules.

introduction to power automate 

So let's start by introducing Microsoft Power Automate. First of all, what is Power Automate? Power Automate is a cloud‑based service, part of the Microsoft Power Platform, that makes it practical and simple for line‑of‑business users to build workflows that automate time‑consuming business tasks and processes across applications and services. So who is Power Automate made for? I kind of said it a bit in the definition before, but the goal of Power Automate is to enable anyone with a very basic knowledge of technology to automate business processes. So this applies to everyone from line‑of‑business users to, of course, IT specialists. It really enables everyone to create powerful workflows without writing any single line of code, but if needed, developers can also take it to the next level with powerful integrations, but the dev part is the topic for another course. I also want to share a quick bit of history. Microsoft Power Automate was called Microsoft Flow originally, and it got renamed to Microsoft Power Automate in November 2019 in order to better fit with the Power Platform branding. Before, it was natural that in Microsoft Flow you created flows. Well, now, even in Power Automate you still create flows and we usually spell those with a lower case f. This way we avoid some confusion with the previous product name, but I wanted you to know this just in case case you find any previous blogs or even project documentation in your own company that references Microsoft Flow. Just know that this was the past name for Microsoft Power Automate. There are three types of flows that we can create with Power Automate, and why it's important is that even if it's the same product, the goal and creation experience is very different between them. The first type are cloud flows. Cloud flows are the most popular type of workflow in Microsoft Power Automate, and they enable you to trigger flows either automatically, instantly or on a schedule, and connect to over 300 different services. Our second type of flows are business process flows. Business process flows provide a guide for people to get work done. They provide a streamlined user experience that leads people through the process their organization has defined and a list of different interactions that need to be done in order to arrive at a conclusion of some kind. Last, but not least, are desktop flows, which is the Power Automate offering for robotic process automation, which enable you to automate tasks on the desktop or on the web, even with legacy applications that have no APIs. In this course, we will really take a look at examples of each one of them. This way, you can really see the difference and know what business problems you have can be solved by what type of Power Automate flow.

power automate terms and concepts 

Now that we know what Power Automate is, let's start by learning the terms and concepts of Microsoft Power Automate. The first thing we need to understand in Power Automate are connectors, also known as data connectors. Those are actually used in many products part of the Power Platform suite, not only Power Automate. Connectors are like a bridge between your data source and your flows, and allow you to interact with your data sources without any technical knowledge. The Power Platform has over 400 connectors available, and you have connectors for applications internal to Microsoft such as SharePoint, OneDrive, Outlook, and more, as well as external systems such as Twitter, Mailchimp, and Salesforce. From a licensing point of view, there are two types of connectors, Standard and Premium. We won't go too deep into licensing in this course, as Power Platform licensing could get its own dedicated course given how complicated it can be, but generally speaking, Standard connectors are included with any Microsoft 365 or Dynamics 365 subscription at no extra cost, and they usually include all Microsoft‑owned applications, as well as some free third‑party apps such as Twitter; while Premium connectors are mostly third‑party enterprise applications such as Salesforce, for example. If we dig deeper, there are two types of operations that you can do with connectors, and they are called triggers and actions. Triggers only apply to Microsoft. Power Automate, while actions can be used in both Power Apps and Power Automate. So let's dive deeper and talk about triggers. The trigger is the event that starts a flow in Power Automate. That can be an automated trigger, which is simply an event from another system, for example, when a new item is added in a SharePoint list, start this flow; when a new file is added in a OneDrive folder, start this flow; or when a new tweet with the Globomantics hashtag is out there, start this flow. So automated are really triggered, so an automated trigger is really an automated trigger that starts when something happens into another system. A trigger can also be time based, for example, start this flow every day at 5:00 A.M. Finally, you can have an instant trigger, which means simply trigger that workflow when needed. A good example might be start an approval for this selected SharePoint document. So maybe you don't want to have it automatically as soon as the document is uploaded or edited, instead, you want users to manually go in when they feel their document is ready, click and start that flow themselves. When creating cloud flows, you will see Power Automate ask you which type of cloud flow you would like to create, Automated, Instant or scheduled? Well, those are actually based on the three types of triggers that we just saw. That will be the only difference. Once we get to doing it in the UI, even if you see three options, it's still the same Power Automate. The trigger is the only thing different between those three options. Okay, now let's talk about the actions. Actions are any type of interaction with your data source, whether that's getting information, adding or updating. Some sample actions can be start an approval, get a user's manager or send an email. Okay now, what does it all look like in a Power Automate cloud flow? And what you see here is really the user interface when we create Power Automate flows. At the top of every cloud flow, you will have the trigger. It will really be at the top every single time, because this is the thing that will trigger the flow. Next up, your flow will most likely have multiple actions. Those are all of the actions that your flow is going to do, such as start an approval, send an email, copy a file, and so on. Finally, something that I haven't really talked about yet, you will find different controls that help you with the flow logic, such as conditions, loops, switch statements, and more. In this example, I have a condition that helps me add different actions depending on the approval outcome. So just to recap, the trigger is the event that starts the flow, and actions are everything that your flow does. In a flow, we can also have different logic operators such as conditions, which allow you to do different actions depending on a value. Finally, we talked about connectors, which are kind of like a bridge to all of the different services that Microsoft Power Automate connects to. Microsoft Power Automate also comes with a wide array of templates that you can use and customize to build the workflow capabilities that you need. The templates serve as both a quick start guide and a learning tool. You can really use templates as is, or customize them to fit your needs.

Power automate user interface 

Now that we know the basic terms let's get familiar with the Power Automate User Interface. Most Power Automate flows are created from the Power Automate web interface, which you can find at make.microsoft.com. There is also a desktop tool for Desktop flows called Power Automate Desktop, and we will showcase it later in this course when we'll cover desktop flow. Once you go to Power Automate, in order to see your flows, you will go to the My flows on the left navigation, and you will have three tabs at the top. The first one is called Cloud flows, and here is where you will find all of the different cloud flows that you have created that you have not shared with anyone. The second tab are the Desktop flows, which remember are for Robotic Process Automation. And finally, under the Shared with me tab, you will see all of the different flows that you have shared with other people, or they shared a flow with you. It's important to remember that let's say you create a flow, you work on it, and then you share it with a colleague. It will not show up under cloud flows anymore, but it will go under the Shared with me tab. This has really got me a few times as I would work on a flow for one week, get it ready, test it out, and then share it with some colleagues, so they're also owners of it, come back on Monday, look at cloud flows, and then I can't find my flow, when, in fact, it's now under the Shared with me tab since I shared it with somebody. When clicking on a cloud flow, you will see multiple types of details about this flow. First of all, you will see the basic details about the flow, such as the name, the description, the owner, what type of flow it is, as well as the created and modified dates. After that, on the top right, you will see the connections or connectors used, and this can really give you a good overview of what services this cloud flow connects to. Right underneath at the bottom right, you will see all the owners of that flow. And finally, on the middle bottom, you will see the 28‑day run history for that flow, so you can really see all of the different runs that flow had, how long each flow took to run, if it succeeded or failed, and you can click on each one to see more details.

Demo Power Automate User Interface 

Now that we have seen the theory in the slides, let's head over to the lab environment and explore the Power Automate user interface. We are now in the demo environment. Let me open up the browser. Now, there are two different ways you can get to Power Automate. The first one is you can always go to your Office 365 user portal and then take a look at all of the different apps, and you will see Power Automate in here, which will bring you to make that powerautomate.com. The other option is, of course, simply type in the URL yourself, and you will end up in the same location. Now, once you open up Power Automate, you will go on the homepage where you will see a few different templates that Microsoft proposes for your new flows for emails, files and documents, notifications and reminders, and more. On the left, we have a navigation. This navigation can either be maximized or minimized depending on what you're working on. Let's maximize it back so we see everything. Other than the Home, we have Approvals, where we can see all of the different approvals that are waiting for us in a single location. Now we will see how to create those approvals in the next module, but for now know that Microsoft Power Automate has an Approvals center where you can go and really see all of the different things that are waiting on you to approve or reject. Next up in the left navigation are My flows. So here we will see our Cloud flows in the first tab, our Desktop flows in the second tab, and the flows that are shared with me or created by me and shared with somebody else in the third tab. Let's go back to Cloud flows and take a look at the details we see right away. The first thing we will see is the list of flows. Now you might notice that some of the flows are turned off, such as the Maintenance Notification flow. Right now you can see the logo is a bit grayed out. That tells you that this flow is currently turned off. Something else to see is that some flows will have a little diamond next to the name, like the three in the middle. This tells you that this flow uses premium connectors. We've talked about connectors before and standard and premium. Well, at first glance, I can already see that the three flows in the middle use at least one premium connector. Great! Now let's go in the Purchase Approvals flow. In the middle of the screen, I will see the flow details. So I can see the flow name, flow description, flow owner, as well as the status, when it was first created and last modified, as well as the plan, so the licensing that applies to this flow. I can always quickly edit it by using the Edit button here at the top right of the square. Great! Then at the top right, I will see my connections, so I can see right away that this flow uses the Approvals connector, the Office 365 Outlook connector, the Office 365 Users connector, and more. Then I will see the owner, so here I can, for example, edit and add a new owner to this flow, for example, and remember, once I do that, this flow will now go to the Shared with me tab. And lastly, something that is pretty new on the right side here, we also have Process insights preview, which might require a premium license, which will give you insights into your flow. Okay, we're done with the right side of the screen. Let's go back to the middle where we have our 28‑day run history. Here is where I can see all of the flow runs in the last 28 days. Right now you see I only have two of them. Both of them started on up August 22nd at 03:36 AM. I can see that the 1 on the bottom has succeeded, so the flow completed successfully, and it took 11 hours, 15 minutes, and 49 seconds for it to complete. The top flow here is still running, and it has been running for 14 hours and 46 minutes, so we really have a lot of information at first glance. Now I can go inside of a flow here. Let's click inside. Once here, you can really take a look at what this flow did. So if I open up, for example, the Get manager action here, I can see that the inputs to this action were vlad@globomantics.org, and then the outputs where the display name is Vanessa Le, given name is Vanessa, and really all of the properties that this action got back. So, as you can see, really a lot of details we can look at, at flows that have finished, or even ones that are currently in progress. For example, I can take a look at the one that is currently running, and then I still have a ton of information on all of the actions that are completed, as well as I can take a look at where this flow is currently stuck at. Who's it waiting for? So right now I can say, okay, the trigger when a new item is created has been done. We got the manager here, and we are currently waiting for the approval. Awesome! Let's go back to My flows here. Now the next item on the left navigation is called Create. You can either go here to create a flow, and it will tell you that you can either start from blank, and it gives you quite a few options, or you can start from a template. Another way you can do this, the way that I usually do it, is from My flows. I click on New flow here on the top left, and you actually have the same options. Great. Now let's go back on the left nav. If we go to Templates, we will see all of the different templates that are offered in Power Automate. There are thousands of templates, and you can see who published them. So this one has been published by Microsoft. It's a scheduled type of flow, and it has been used 1715 times. So, really quite a popular one. And again, there are thousands of templates, and you can always either search for them, or simply go to, for example, let's take a look at all of the templates that are about approvals, and then I can see those here. Next up, we can go see our Connectors over here where you will see all of the available connectors in Power Automate, search for them. You even have different filters so you can see only the standard connectors or the premium connectors. This has really come in handy for me when somebody asked me, hey Vlad, is there a connector to connect to this third‑party system that personally I had no idea even existed. So what I will do is I'll open up Power Automate, go to Connectors, and then search for a connector. I'll give an example. Let's search for Adobe here. Of course, I knew Adobe existed, but just wanted to show you how it looks like. And you see we have three different connectors with Adobe. We have Adobe Acrobat Sign, which is a standard connector, and we also have two others, which were premium. Let me go back here, Adobe Creative Cloud and Adobe PDF Services. So this is the Connectors page. Next up, under Data, we have all of the Dataverse information. As you can see, it actually brings us to consume it from the Power Apps Maker portal. So I can see my Dataverse tables, my connections in Power Automate and Power Apps. I can also see custom connectors, so if you have created any custom connectors in your organization, as well as any gateways to connect to on‑premises, if your IT department has created any. Great! Next up, we also have a monitor. Here I can see all of the recent activity for my flows. So I can see right now the Purchase Approval flow ran successfully 1 time 3 hours ago, but here, of course, this is a demo tenant, so there's not a lot of activity, but here is where you can go to see all of the activity with your flows, and you can even filter, for example, for the failures, for example, just so you can monitor and see if any of your flows have failed that you need to take a look at that and see what happened. You also have the same thing for Desktop flow runs. So we have the Cloud flow activity, Desktop flow runs, as well as the Machines. This is really for desktop flows again. Awesome! Next up, we have the AI Builder over here where we can get access to all of the different AI Builder functionality. We won't go deep into the AI Builder in this course, but if you're interested to learn more, it's actually covered in another course, part of the Power Platform Fundamentals path, and the course is called Core Components of the Power Platform. Great! Then we have the Process advisor here, which is a tool that gives you insights inside your Power Automate flows. It's important to know it exists, but we will not really cover it in this course, as it's a really more advanced feature. And finally, we have all of our Dataverse solutions. So if your company is really creating solutions in the Power Platform, maybe business apps that combine Power Apps, Power Automate, Dataverse tables, and things like that, you might find them in here, but as a user you will probably not end up playing with solutions that much. It's really for developers and IT professionals that create solutions to deploy them in multiple environments. Great! This is it for this quick overview of the Power Automate user interface. We have really done a generic view of everything. This is the last demo of this module. Let's go back to the slides, finish the module, and then we'll come back here to actually get started and create some awesome cloud flows, business process flows, and desktop flows in this courserse.

Creating a cloud flow from a template 

Let's start by learning how to create a cloud flow from a template. Microsoft Power Automate templates are a great way to get started with Power Automate. Those pre‑built flows give you the logic, as well as follow Microsoft's best practices, and furthermore, they can even be customized to your needs. You really get a starting logic. However, you can add and delete actions, and you have hundreds of templates offered right out of the box for you to use right now. Once you navigate to Power Automate, on the left side, you have the Templates section, and you can browse them or use the top bar to search for templates, filter by categories, and change sorting as well. Before we go into the demos, I just wanted to point out that Microsoft Power Automate templates also include a Shared with me section, which are all of the templates that somebody in your organization has shared with you. This functionality allows your organization to create kind of an organizational gallery for Power Automate templates. This way, everybody can get started a lot faster, so make sure you check that, see if anybody in your organization has created any templates that you can use.

Demo: Send an Approval e-mail when a new item is added to a sharepoint list 

Let's get started building our first cloud flow from a template, and this is probably one of the most popular types of workflows, an approval when a new item is added in a SharePoint list. We are now in the lab environment, and let me open up the browser where I am already in the Power Automate Maker Portal. For this demo, we will start from a template. So, let me go to templates here in the left navigation, and let me search for the template that I want, which is send an approval email when a new item is added, and this is the one that I need. We can see that it has been created by Microsoft, it's an automated cloud flow, and it has already been used 28,000 times. So, it's a fairly popular one. Let's select it. Once you select a template, you will see that Microsoft Power Automate shows you two different squares. The one on the left, which will always have one single icon, this will be what system triggers that flow. So we know that is SharePoint. And then we will see what are all of the other systems that are used or connectors that are used in this flow. In this case, we know they are the approvals and Office 365 Outlook. Awesome. Now I'll scroll down a little bit, and here we see again all of the systems that this flow will connect to. We have SharePoint, Office 365 Outlook, and Approvals. Right now, I have a green checkmark near all of them. However, if you ever connect to a new system, you might not have that connection already created. So then you'll just have to click a button. If it's a Microsoft 365, you'll probably already be signed in. But if it's a third‑party system, you will need to provide your username and password or whatever credentials you use to create that connection. Great. Let's click on Continue. Now remember what templates do. They already give us a guide. They give us the skeleton of our cloud flow, but we still need to add the details on what sites we want to connect to and so on. So first thing, when a new item is created, that is my trigger. What I have done for this demo, I have a SharePoint site over here called Power Automate Demos. And inside, I have have a list called Purchase Approvals that simply has a title, an amount, and a justification. So I'm keeping it pretty simple. So now let's go and copy the URL of the site here. We might need it; we might not need it. You will see why I'm saying that in a few seconds. Once I go into Site Address over here, Power Automate will show me what are the sites that I use the most often. And it's really using its machine learning AI capabilities to show me all of the sites that it thinks I might need. But a lot of the times, especially if it's a new site, you see we do not have Power Automate Demos here unfortunately. So what we do then we just go and click on Enter custom value. Let me enter the URL. And then, I will go under the list name, and it will show me all of the different lists inside that site. I only have one right now, Purchase Approvals, so I will select it. Great. For most connectors, you also have advanced options over here, but I just wanted you to know that that is a topic for a more advanced course on Power Automate. Remember, templates are customizable. So right now, in this template, it's basically asking me to all always have the same people approve the purchase request. But you know what? We want to make this workflow more dynamic, more fun. So what I'm going to do is I'm going to add a step here. And what I will say is I want the manager of the user that is submitting the purchase request to approve it. So now let me search here for manager. Now remember, there are over 400 connectors. Some of them have over a dozen actions each. There is no way you will know them all by heart, but this is why I love the search functionality. I search for manager, and then I see I have a Get manager action under the Office 365 Users connector. Let me select it here, and it will ask me okay, Vlad, who's manager do you want me to find? And when I click on it, I will have the dynamic content pane that will appear here. So the dynamic content, I can see all of the different properties that Power Automate gets from all of the actions and triggers above my current action. Right now, of course, I only have one. This also has a nice search functionality. So what I can say is find the manager of the Created By Email. So, you know what? I will give the input, the email, of the person that created the item. That's what I will give to the Get manager action, and this action will then return me the manager, and I can use that later. So in my Start an approval here, under Assigned to, you see now because we get the properties from all of the actions and the trigger above, I can get dynamic content from the Get manager, as well as from the, and let's scroll down a bit, from the When an item is created. So that is quite awesome. But what I want, let me just search for mail. I will give okay. This approval will be assigned to the mail of the manager. For the approval type, let me go back a bit. You actually have multiple options, either approve or reject. So those are your options that the approver must do. Everybody must approve. First to respond. So you could assign an approval to three different people, and you can decide do we need all of them to approve it before this is considered as approved. But if one of them rejects it, the whole thing is rejected. Or I'll assign it to three people, and whoever is the first one that takes a decision, approved or rejected, that is the final decision. In our case, we only have one approver, so it doesn't really matter. They are all the same thing for us right now. But it's great that you have those options. Actually, approve and reject are not your only options. You can also have custom responses if you want to have other options than approve or reject. Okay, now let's customize our approval a bit. Right now, it just says approval request. But imagine being a manager, and all of the things that you have to approve, they have the same titled, which is Approval Request. Probably not productive as a manager, right? So we can customize them. I can say okay, a Purchase Approval Request by, and let's get the created by display name. So this way, in the approval title, I am already adding some information. So I'm saying, hey, this will be a purchase approval request by Vlad, by John, by Vanessa. So we can already give more information using dynamic content in those approvals. Let's go and actually, in the details, let's add a bit more. I will delete everything that is there, and let's go add some text. So I'll say hello, and then let me get the display name of the manager. We're really going to make a great description and a great email that we send to our manager. So I'll say Hello, manager display name, please approve or deny this purchase request by created by display name, and this way, we have the person that requested it, to purchase a, and now let's get the title of the item from SharePoint. And we'll put for the amount of, and I think we had a column called Amount, so let's add that in there. And I'll say here is the justification provided, if any. And then, we will add the justification from the SharePoint list. This way, again, when that manager gets that approval, they might even be able to make the decision without even going to the shipping list item because we have empowered them to be so productive directly from the approval. Then, I can add a link to the item here. Let me search here for link. I have a link to item directly in the dynamic content. And for the description, I will say just the title. Let's put the title here. Perfect. Great. After that, we have a condition. So what does the condition say? And again, this is all provided to us by the template. If the response is equal to approve, send an email to Created By Email. The item title was approved. Of course, in production, you would add quite a bit more details there to make it more enterprise‑friendly. But we are not going to do it for this demo, but I still want to add an email if the item is denied because we also want to inform the user if the item is denied. So if not, let me add an email here. So I'll search for send email, and then I'll pick send email Office 365 Outlook. Who do we want to send it to? What happened here? And this is a reflex that I always personally fall for. It's one of the things that I know I shouldn't, but I still click three times every time. I click on it, and then it doesn't pop the dynamic content. For some connectors, like a send an email, when you click, it doesn't automatically pop out the dynamic content pane. Instead, I have a button here that I can click, and this will show it. So here, for the to, I'll put created by email. Lets me put the created by email. And here I'll say Your Purchase Approval was denied, and we're going to add a sad smiley face. And then here, in the body, let's just put the title of the actual item, and then you can even have things such as Here are the comments of the approver. And inside the approval, you actually have some comments that have been added, so you can add those if you want as an example. Let's not do it for now. Let's keep it simple for this first demo. So we have our flow. When a new item is created in the Power Automate demo site, Purchase Approvals list, get the manager of the person that created that item, then start an approval with that manager. The title will be Purchase, and if you spell it right, it's always better. And this is why it's important to always test and double check your flows, especially before you launch them enterprise‑wide. So Purchase Approval Request by the person that created the item and their display name this time. And the details will be Hello, manager display name, please approve or deny this purchase request by, created by display name, to purchase a title of the item in SharePoint for the amount of whatever the amount in the column in SharePoint. And then here is the justification provided, if any. We'll add a link to the item. And if the response is equal to approve, we will send an email saying, hey, your item has been approved. And let's just add a smiley face here just to keep it the same and have a little bit of fun. And if the answer is not approved, we will send an email saying your purchase approval was denied. Great. Let's save it. Something that I also want to tell you that you should do, you should always rename your flows because right now my flow is send approval email when a new item is added. This is fine because I don't have a lot of flows right now in this demo environment. But imagine I created 15 of those, and then a site owner is asking me to change something in that flow. Well, I would have to look through all of the flows with the same name to actually find which one is which. So this is why I recommend always giving a meaningful name to your flow. So I can say, for example, Power Automate Demos ‑ Purchase Approval. This way, I know that, hey, it has to do with the Power Automate Demos and it's a purchase approval. Great. So our flow is ready. Now let's try it out and hope we did a good job and it works. So let's go here in the list. Let's create a new item. I need, first of all, a new office chair. The amount will be $250. And my justification will be My current chair broke. Great. So we have one. That is a decent purchase, an office chair. Now let's add another one that will maybe be denied. So I'll say I need an ultrawide 49‑in gaming monitor that runs about $2,000 right now. And the justification is it will make me so much more productive. Great. Let's click on Save. So, we have done the action that should trigger the flow. Now let's go to Power Automate here, and let's click on refresh to see if this actually started. Now remember, it might take anywhere between a few seconds if you're lucky to maybe 1 or 2 minutes before the flows actually start. There is no set exact timeframe. Okay, great. I just refreshed, and here they are. They just started. Remember, something that is cool is you can go on any of the flows, and you can see where they are currently waiting at. So I can see for this one, when a new item got created, I can see the inputs of the connector, so I can see what we provided it, as well as the output. So I know the exact information that this action or trigger is sending back to Power Automate. So I know the information from this one is the SharePoint list ID is 2. The title is Ultrawide 49" Gaming Monitor. And I really have all of the different details. Same thing for the manager. So I can select, hey, the input we provided, remember it was dynamic content, was vlad@globomantics.org. And then the output is Vanessa Le, and we have the mail and all of the different information here. So, if you're ever working on a flow and you're not sure what the flow sees or how come you're not getting the information you expect, you can always take a look at a flow that's running or has finished, and you can see all the current information that Power Automate sees in the back. Great. Now we know that those flows are waiting for the manager, and the manager is Vanessa. Okay, so now let's open up the browser as Vanessa Le where I'm in the Power Automate Maker Portal. What I will do now‑‑‑ And remember, one of the cool things with Power Automate is that as an approver, I can go to the approvals over here, whether in desktop or even in the mobile app, and then I'll see what are all of the approvals that are waiting for me. I can see right now I have one of them that's waiting for me for a week ago and two of them that are waiting for only a few minutes ago, so those are the ones I want. Let me go in the first one here, and let's see what it says. Purchase Approval Request by Vlad Catrinescu. Remember, that was created by display name. Now if we go inside the details, Hello Vanessa Le. Please approve, so remember, Vanessa Le, manager display name, please approve or deny this purchase request by Vlad Catrinescu, created by display name, to purchase an office chair for the amount of 250. So see, that's something that we can improve. We can make sure that it shows as a currency with the dollar sign for future improvements to this flow. Here is the justification provided, if any. My current chair broke. Great. So we will approve this, and let's click on Confirm. Great. The other way you can do approvals is directly from email. So, inside my email here, you see I have an email that says Purchase Approval Request by Vlad Catrinescu, and the other one just came in a bit late. But you see, by the way, since we approved it from the Power Automate Approval Center, it says that I already completed this request. But if we go to this other one here, the Ultrawide 49" Gaming Monitor, it's not answered yet, so I can approve or reject it. So let me reject it. Click on Submit. Now if we go to my flows here, they have both succeeded. As you can see, they both completed successfully. And then, in my email as Vlad Catrinescu, I can see that the item office chair has been approved, and then your purchase approval was denied for the gaming monitor. So this is it for this quick demo in which we have created a Power Automate cloud flow from a template that whenever a user adds an item to a SharePoint list, we get that user's manager, we start an approval, and depending if it's approved or not, we will send different emails. This is it for this demo. But before we move on to creating our own flows, let's go back to the slides and talk about another flow we will create from a template, so we practice a bit more.


Demo: Copy Files between Dropbox and SharePoint 

Now, let's do another cloud flow from a template, but this time, we'll use a third‑party service, and we will look at how to automatically copy files from Dropbox to SharePoint. We are back in the lab. Let me open up the browser here. Let's go to Templates, and let's search for simply Dropbox and SharePoint. And we see we have a template by Microsoft Copy files between Dropbox and SharePoint, so let's select it. It will, again, show me the two services we're using. Dropbox is the trigger, and then in the actions, we have only SharePoint this time. At the bottom, you'll see this is where we'll connect to Dropbox and it will show at what account it's connecting with. You can always double check because right now, you see I'm logged in with globomanticsvlad@gmail.com, however, the template wants to connect with vlad@globomantics.org since I have used this account with Dropbox in the past in Power Automate. You can actually have different connections to the same service with different accounts, and this might happen often, especially as you work with third‑party services. So I'll make sure that I tell it to use the right account globomanticsvlad@gmail.com. Let's click on Continue and some templates that are fancier, as I like to call them, they will actually ask you for some of the information right away before even going to the full edit view of the Power Automate flow. So it'll ask me, okay, in what Dropbox folder should we monitor? Let's click on the Folder icon. And if everything goes well, I should see a folder called Power Automate Demo. Let me go back here, Dropbox, and then I have my Power Automate Demo folder. Perfect. Then it will say, okay, in what SharePoint site should we put them? Do we have, let me search for Power Automate demo, do we have it? Yes, we have Power Automate Demos here. Perfect. And for the SharePoint folder path, I will just put it in the Shared Documents, so really the Documents library here. Awesome. Let's click on Create. This will create the flow from a template. And if we click on Edit, again, we have the full cloud flow. We can edit, templates are just a starting point, but we can always go see exactly what they do and then modify them as needed. But okay, this flow is ready to go, so let me go into Dropbox here and drag and drop some PowerPoint file here, the PowerPoint file called Some Slides. Great. It has been done. Now, remember, it might take a few minutes until the Power Automate flow runs, so let me just pause the recording until it's done, and we'll come back as soon as it's started. Great. So our flow just started 42 milliseconds ago. It should run quite fast. I feel like if I do a refresh right now, it will already be done. And as you can see, it only took two seconds to run. Now let's see if it actually worked. If I go back to the SharePoint site, you see I have the PowerPoint file Some Slides here, which got created, and it actually works because I'm able to view the file, so it was copied successfully. So this is it for this quick demo in which we have used an existing template to copy files that get added to Dropbox automatically to a SharePoint online document library, and we did this in under five minutes. This is awesome. Now let's go back to the slides and really take it one level further and learn about more advanced scenarios, and even creating our flows from scratch.

Creating a cloud flow from scratch

We have now created two cloud flows starting from templates, but now let's create our own flow from scratch. Before we go into the demo, I wanted to point out that Microsoft Power Automate makes it easy to create cloud flows from a blank slate as well. It even has a nice first screen, allowing you to search triggers depending on the type of flow you have selected. In addition, I wanted this flow that we're building now to be even cooler than the ones from a template. So we will talk about some logic operators that we have available in Power Automate. There are three main ones we will talk about, loops, switch statements, and expressions. Let's start with loops. Loops are a block of actions that will repeat over and over again until a condition is met. There are two types of loops that we can use in cloud flows. The first one is an apply to each and the second one is a do until. The apply to each is useful for, let's say, going through all of the items in a SharePoint list, while the do until will do an action until a condition is met. So think at sending a reminder until an approval is completed. Next up, we have expressions. Expressions in Power Automate are a set of functions that enable you to return or modify data. The goal is really allowing you to write a formula that manipulates data and then returns it in the format that you wanted. Expressions can help you perform calculations, concatenate strings, or manipulate dates. We have used dynamic content throughout this module, which is content from the flow, but with expressions, we can now modify that content in the format that we want. Finally, switch statements. In our previous demo, we have already used conditions. Switch statements are a bit different. Switch statements are limited to only being able to evaluate a single property; however, they allow you to have multiple potential results. Some examples where I have used switch statements recently was a provisioning flow with multiple template options. So I was able to evaluate the switch case on the template provided and with over five template possibilities, a switch was the best control. Another one can be an expense report with different options, depending on the value of the expense report.

Demo Creating a cloud flow from scratch 

With the theory out of the way, let's head over to the lab and create our own cloud flow from scratch. We are now in the demo environment. Let me open up the browser, and in this demo, we will play with a list that I have in this SharePoint site over here called the work progress tracker. I have three different items. They both have a Progress column, which can either be Completed, In progress, or Blocked, and since we're tracking work progress, they also have a Start date and an End date. What we will do in this demo is we will use some of the different things we just learned in the slides. We will create a flow that will loop through all of the items of this SharePoint list, and then if the due date is passed and the progress is not completed, we will send a reminder email to the person this task is assigned to to remind them to either update the date or actually finish their task. So let's get started. I will go in Power Automate here. Let's do a New flow. In production, you would probably make this on a schedule, let's say, run it every Wednesday and Friday, for example, or something like that, but in our demo, we will just make it an instant cloud flow. This way we can trigger it right away, and I will select to manually trigger a flow. Awesome! Let's wait for it to load. Let's add a new step, and what I will search for is Get items. So I want to get all of the items from a SharePoint list. Let me select the site. This time it's the Getting Started with Microsoft Power Automate site, and then for the list it will be the work progress tracker. So now we have actually got all of the items in that list. Now we actually need to go and loop through each one of them. So what I will do here, I will add a new step, and you can either search for a loop, or just so you know, all of those logical controls are under the control connector. So if I go under the control connector, I can see my Condition, Apply to each loop, Do until loop, Switch statement, things that we have talked about in the slides. Let's do an Apply to each loop. And then what are we going to loop on is we're going to loop on the list of items we got previously. Great! Now I need to add an action, and what I want to do is I want to make sure I get the information on the specific item I'm looping on. So I will search for get item without the s at the end, so I'll make sure I picked a good one here. The second one, it will ask me, okay, Vlad, what site? Still the Getting Started with Power Automate. The list name will be the work progress tracker. And then the ID under Dynamic content, here I have the ID, and it even says in the description, use this value for specifying the item to act on. Great! So now I have the item. Next up, I need to add a condition. So again, I'll go under Control, let's select the Condition, and what I will say for the condition is, if, and let's go into Dynamic content, say if the progress value, and make sure we're on the Get item here, is not equal to, and we have to make sure we get it right so it's completed, let's type in completed, and then in Power Automate we can really make advanced conditions. So what I can add here, I can add another row and say, okay add, so I want both these comparisons to be true before the condition is true. Let me select the due date, so create the due date by when the work is completed. Must be less than, so this way it has passed. And then let's choose the value, which, wait, we don't have a value in Dynamic content because we do not have the date. However, if we go into Expressions, we can create content, we can get content that is not necessarily from the trigger, or from the other items. So I can use the utcNow function to say, hey, right now, so if the due date is less than now. So if both of those are true, it means somebody has a task that is not completed and that is past due. So I'll go here, let's send an email to that person. Let me search for send email, Office 365 Outlook, let's get the dynamic content, we'll get it assigned to email, we'll say "Your task is late!" And in the body, let me just add the title of the task. If there is nothing to do, I will not send any email or anything like that, so we will just save it as is. Great! Let me click on Save here, and then let's try it. Something I forgot to do is make sure that you rename your flow. If not, it will give it a very fancy name like this, Button, Get items, Apply to each, Get item. So really, get all of the actions and the trigger we do and put that in the name. We do not want that. So what I will say here is I'll just name it reminder for overdue tasks and like save it. Awesome! Let's wait for it to save again. I do have the flow checker. Now, this is something that is pretty cool with Power Automate when you save it. You do have a flow checker telling you if there's any errors, but right now it's telling me like, hey, there is a warning when you do get items, if you have a lot of items, you could implement an OData filter to increase the performance of your flow. Now, this is a more advanced topic, but for us with only three items, it doesn't really make sense, so I will not do anything for it, but just know that there is a flow checker that will give you warnings or tell you if there is anything wrong with your flow. Great! So let's actually go and run it. Let's continue. Let's run the flow. And what should happen is I only have one task here, the Demo Plan that is in progress, and the due date is 5 days ago. So I should be getting one single email which has to do with the demo plan being late. Let's take a look. Nothing in my email yet. There we go. Of course, as soon as I clicked away, it just arrived. Your task is late! Demo Plan. So, it worked. And if we take a look back to the flow, it should be done. It has succeeded. So this is it for this quick demo in which we have really created a flow from scratch and showcased some of the more advanced logic operators we can add in Power Automate cloud flows such as loops and expressions in order to really follow the logic of your business process. This is the last demo in this module on cloud flows. Now let's head back to the slides and finish off this module.

introduction to business process flows 

Let's start by doing an introduction to business process flows. Let me start with a quote from Microsoft that I really love, and it kind of sets the stage. A business process flow is a series of ordered work steps that a user completes within a business process. If we go more in depth, business process flows provide a visual guide for users to get work done. It ensures that everyone follows the same steps, that all the required data is filled, and it allows you to add logic depending on data, as well as to offer different experiences depending on security roles. Business process flows also provide a visual indicator to users of their progress in that process, and as they are super easy to use, it reduces the amount of training required for your users. Now let's see what they look like. This is a business process flow that's displayed inside a model‑driven Power App. On the top, I have what we call stages. Those are all the different stages that a process must go through in order to be completed. At the top, I have HR, which is my first stage, and then another stage for the Travel Team, and one for the Manager Follow Up. For each stage, we have steps. Those are the steps that must be done in order for the user to be able to go to the next stage. A step can include information like what we have in the screenshot right here, as well as giving you the ability to run cloud flows, such as an approval, as part of a stage which is called a flow step. If we take a look at the benefits of business process flows over cloud flows, first of all, they offer a visual guide for a user to complete a process. I also sometimes describe it as a road map or a recipe towards a process that's very easy to follow without any real training needed. Business process flows allow you to split up a complex process into smaller stages and steps, making it easier to accomplish for the end user. Business process flows also offer stage gating, meaning a process can only go to the next stage if the previous one is complete, again, making sure that users follow the process in order. Business process flows also offer us conditional branching, meaning that depending on the answers in the steps, you can go to different stages. Finally, business process flows have a very deep integration with Dataverse, the data from the steps is saved in Dataverse, it uses Dataverse security, and really overall has a ton of integrations with Dataverse. There are two types of business process flows. The first one is called embedded. This is the one that we have so seen in the screenshot before where the business process flow appears along the top of a model‑driven Power Apps app. The second type is immersive, which is a stand‑alone solution. This will also change how you start the business process. With an embedded process, you would simply do it all from Power Apps. As with an immersive one, you need to go to Power Automate and start it from there. Also, the creation experience is the same. Whether it's for embedded or immersive, the creation is the same, but where you consume it is what changes. I have mentioned it a few times, but really wanted to emphasize it, that business process flows are really well‑integrated with other Power Platform components, such as Microsoft Dataverse and model‑driven power apps. So really, they are a lot more advanced in a way from the cloud flows that we have talked about before. Furthermore, some artifacts need to be part of Dataverse solutions. Now, I know some of you might already have a good knowledge of model‑driven Power Apps Dataverse solutions, but not everyone who wants to learn Power Automate has the skills in those other products. So while we will not cover those other concepts in this course, there is plenty of other content on Pluralsight that you can watch to really know everything about those other products as well.

Our Business process flow scenario 


Now that we know what business process flows are, let's take a look at one in action. Before we get started I just wanted to mention one thing that in general for business process flows, the tech part is fairly easy; however, designing the business process on paper with your stakeholders is the hardest part. It's really important that you design your process on paper before creating the business process flow because it's a lot harder to just wing it with business process flows. From a technical requirements point of view, you need to make sure that the Dataverse table and columns are created beforehand as this will make everything a lot easier, and of course you can use any of the existing built‑in tables as well. Okay, so let's take a look at our scenario business process flow. We have been asked by Globomantics hotel to build a business process flow to ensure that a common process is followed when room attendants clean hotel rooms between guests. The process should also make sure that maintenance is notified if anything is broken and needs to be replaced, directly from the same business process. Also, the process needs to be done from an easy mobile interface with almost no training needed as the users for this process will not be technical users.

Demo : Business Process Flows in Action

Now that we know the scenario, let's head over to the lab and check out the business process flow in action. We are now back in the lab environment, let me open up the browser, and let's actually go to the edit or creation view of business process flows. As you notice, it's really different than cloud flows. We have our stages at the top, and inside side we add steps. So for our hotel room cleaning process our first stage is the Bedroom Cleaning. And then we have different data steps that we bring in from Dataverse, such as, did you check for misplaced items, did you clean the sheets, did you clean the pillowcases? And we really add all of the different things that we want to capture or validate that have been done. For each one of them, again, we give it a step name, but it's also associated with a field inside Dataverse since business process flows are so integrated with Dataverse. And I can also say is it required or not? So, we really have those rules in place. After that I can have a condition here. So I can say, hey, if the maintenance is needed, so if in the Bedroom Cleaning stage the maintenance needed is marked as Yes, then show the Hotel Room Maintenance stage here. If not, just skip to the bathroom. So I can really add different fields in here. So for example, let's add a data step just to show you high‑level how it's done. We are going to add it here, and we can add, for example, clean the coffee machine, which of course it's something that we must do, and for a hotel room it's off there in the bedroom, so let me add it in there, mark it as required, and then Apply. So, it's that simple to add different data steps. Now what does a business process flow look like in action? Remember, our goal for our business process flow is to create something that is simple for a non‑technical user to follow and for us to make sure that they follow the required process. So if we take a look here, I have a model‑driven Power App, and at the top I have the different stages. So I have my Bedroom Cleaning, my Maintenance, my Bathroom, and finally Appliances. But remember, there is a condition for Maintenance, so that's why I don't see it right away. So let me add a room number, 830 here, let me just click on Save, and now let's advance into our process. Let me go into Bedroom Cleaning. I can say, did I check for misplaced items? Yes. Did I clean the sheets? Yes. If I try to, for example, go to the next stage without saying clean pillowcases, you see it tells me there's an error. I did not follow the recipe. I did not follow the business process. So I have to mark it as Yes. Also something to remember is that because it integrates with Dataverse, it also follows the business rules for Dataverse. Look what happens when I have lost items to report. If I put it to Yes, I will have a field that appears, that is Lost Items Description, in which I need to add the information. So I can add, for example, I found a cell phone charger. And if I do Yes for maintenance, the Maintenance stage appears at the top. If I do No, then it disappears. So business process flows can also adapt to your logic and really make it easy for a non‑technical user to only follow the steps that they need to. So if I go Next now, perfect. Filled all of the information here. It will save it. You can see the Bedroom Cleaning is marked as done in our recipe. Now it will ask me, okay, enter the Maintenance Description. And you can also run cloud flows from business process flows. So you can, for example, inform a team, send an email, get an approval, and things like that, directly from the business process flow. Even if it looks different, it's still Power Automate in the back, part of the Power Platform, so you really have all of those integrations with Dataverse, which are awesome. But this is it for this quick demo of a business process flow in action and how it's like following a recipe. You really follow the processes, you cannot go wrong, which is the really amazing thing with those. You follow the process, you can cannot go wrong. It will not let you go to the next step until you fill all of the required information. So it's amazing for publishing it to field workers that may or may not be super technical that still need to follow a process. This is it for this demo on business process flows. Now let's head back to the slides and finish off this module.

MODULE conclusion 

Now that we know the scenario, let's head over to the lab and check out the business process flow in action. We are now back in the lab environment, let me open up the browser, and let's actually go to the edit or creation view of business process flows. As you notice, it's really different than cloud flows. We have our stages at the top, and inside side we add steps. So for our hotel room cleaning process our first stage is the Bedroom Cleaning. And then we have different data steps that we bring in from Dataverse, such as, did you check for misplaced items, did you clean the sheets, did you clean the pillowcases? And we really add all of the different things that we want to capture or validate that have been done. For each one of them, again, we give it a step name, but it's also associated with a field inside Dataverse since business process flows are so integrated with Dataverse. And I can also say is it required or not? So, we really have those rules in place. After that I can have a condition here. So I can say, hey, if the maintenance is needed, so if in the Bedroom Cleaning stage the maintenance needed is marked as Yes, then show the Hotel Room Maintenance stage here. If not, just skip to the bathroom. So I can really add different fields in here. So for example, let's add a data step just to show you high‑level how it's done. We are going to add it here, and we can add, for example, clean the coffee machine, which of course it's something that we must do, and for a hotel room it's off there in the bedroom, so let me add it in there, mark it as required, and then Apply. So, it's that simple to add different data steps. Now what does a business process flow look like in action? Remember, our goal for our business process flow is to create something that is simple for a non‑technical user to follow and for us to make sure that they follow the required process. So if we take a look here, I have a model‑driven Power App, and at the top I have the different stages. So I have my Bedroom Cleaning, my Maintenance, my Bathroom, and finally Appliances. But remember, there is a condition for Maintenance, so that's why I don't see it right away. So let me add a room number, 830 here, let me just click on Save, and now let's advance into our process. Let me go into Bedroom Cleaning. I can say, did I check for misplaced items? Yes. Did I clean the sheets? Yes. If I try to, for example, go to the next stage without saying clean pillowcases, you see it tells me there's an error. I did not follow the recipe. I did not follow the business process. So I have to mark it as Yes. Also something to remember is that because it integrates with Dataverse, it also follows the business rules for Dataverse. Look what happens when I have lost items to report. If I put it to Yes, I will have a field that appears, that is Lost Items Description, in which I need to add the information. So I can add, for example, I found a cell phone charger. And if I do Yes for maintenance, the Maintenance stage appears at the top. If I do No, then it disappears. So business process flows can also adapt to your logic and really make it easy for a non‑technical user to only follow the steps that they need to. So if I go Next now, perfect. Filled all of the information here. It will save it. You can see the Bedroom Cleaning is marked as done in our recipe. Now it will ask me, okay, enter the Maintenance Description. And you can also run cloud flows from business process flows. So you can, for example, inform a team, send an email, get an approval, and things like that, directly from the business process flow. Even if it looks different, it's still Power Automate in the back, part of the Power Platform, so you really have all of those integrations with Dataverse, which are awesome. But this is it for this quick demo of a business process flow in action and how it's like following a recipe. You really follow the processes, you cannot go wrong, which is the really amazing thing with those. You follow the process, you can cannot go wrong. It will not let you go to the next step until you fill all of the required information. So it's amazing for publishing it to field workers that may or may not be super technical that still need to follow a process. This is it for this demo on business process flows. Now let's head back to the slides and finish off this module.



Module introduction 

Hello, and welcome to this Demonstrating Business Value of Power Automate course. In this module, we will learn about robotic process automation and desktop flows in Power Automate. We will first do an introduction to robotic process automation, what it is as an industry term, and what challenges it can help us solve. We will then talk about desktop flows in Power Automate and introduce the options, as well as talk a bit about licensing. Afterwards, we'll take a look at desktop flows in action. By the end of this module, you will understand the third type of flow we can create with Power Automate.

Introduction to robotic process automation 

Let's start by doing an introduction to robotic process automation, and let's start with a small definition that I really like. Robotic process automation, often called simply RPA, is the technology that allows anyone today to configure a computer software or a robot to emulate and integrate the actions of a human interacting within digital systems to execute a business process. Now let's expand into that a bit. Traditional workflow's automation tools, such as different Power Automate flows we have seen so far, use APIs to communicate with different systems. In Power Automate, those are wrapped into connectors to make it easier for us. But what if you want to automate a task in the legacy application that has no APIs? Think of inventory applications, or sometimes old healthcare systems that only run locally. How many hours are wasted because of data recorded in an application that needs to then be manually copied somewhere else? This is where RPA comes in. Robotic process automation systems watch a user's mouse clicks and keyboard interactions in a user interface and then replicate it over and over. They don't use APIs or things like that. They simply click on buttons, copy paste content, exactly like a user would.

Introduction to Desktop Flows 

Now that we know what robotic process automation is, let's take a look at desktop flows, which is Microsoft's robotic process automation solution, part of Microsoft Power Automate. Desktop flows are the newest type of flow in Power Automate. They were actually made available in April 2020, and their original name was UI flows. So if you still see UI flows in blogs or past project documentation, just know they are now called desktop flows. By releasing desktop flows and Microsoft enabling robotic process automation, Power Automate really becomes the go‑to tool that allows you to automate all of your business processes, whether they interact with modern applications or legacy applications with one single product. Desktop flows do require premium licensing, so we will talk about it in a few minutes, but they are not included with your normal Microsoft 365 licenses. A question you might have is when do we use connectors and when do we use desktop flows? The answer is pretty straightforward on this one. If the application has an API, you should always use connectors and cloud flows, and you might need to build your own connector if there isn't one built in. But if the application has an API, you should still look at using cloud flows. For legacy applications that do not have an API, that's the only ones you should be using desktop flows for. So, really, in my opinion, try to use cloud flows whenever possible. There are also two options for running desktop flows. First one is an attended desktop flow. To run an attended desktop flow, you need to have an active Windows user session that matches the name of the user configured in your connection. The session must not be locked, and when an attended UI flow starts on the target machine, you should avoid interacting with your device until the run completes. The second type is unattended, which is way better for high volume tasks, as it's fully automated without any user needed to be logged in. The desktop flow takes care of logging in the remote computer, do its job, and then log off. While an unattended flow runs, the screen is locked, meaning that nobody can see the flow while it runs, which of course increases the privacy and security of it. While the unattended option might always sound better, well, there is a big difference in pricing, and pricing will always change. But here is an example from August 2022. With the per user per plan attended RPA, a user can create attended desktop flows at $40 per user per month. For unattended, you're looking at an add‑on that is per bot instead, and it's an add‑on of $150 per bot per month. So as you can see, it can get quite expensive, so you will often use those in situations where you have high volume tasks, and then the ROI will be there for those bots.

Creating a Desktop Flow in Power Automate

Now that we know what desktop flows are, let's check them out in action. Before creating our own flow, there are a few things we need in order to get ready. First, make sure you have the required licensing to run Desktop flows. Next up, you will need to install Power Automate for desktop, which is the tool used to create Desktop flows. You can download it directly from Power Automate. In the Desktop flows tab, you will notice an Install button at the top right, and you can install Power Automate Desktop from there. When installing Power Automate Desktop, you also have the option of configuring the machine‑runtime app, which can allow you to trigger desktop flows from Cloud Flows, for example, so I highly recommend that you select that option. Basically, it allows the Power Automate service to connect to your machine. Also, if you're on Windows 11, Power Automate Desktop is actually preinstalled for you, which is awesome, but it might need a quick update the first time that you open it. Even if Power Automate Desktop is installed, the machine‑runtime app is not. But you can easily install it from the Power Automate Desktop settings. With Power Automate Desktop installed, let's talk flow creation as it's a bit different than the other two. There are two ways to create your flow in Power Automate Desktop. You can either add it from the hundreds of possible actions, which you will see in the left navigation, or you can just tell Power Automate to record you doing the process and then it will add the actions for you. In my experience, most of the times it will be a mix of both. You will first record the process and after that, clean it up and optimize it with manual actions. Power Automate Desktop also includes multiple logic controls such as variables, conditions, and loops, which enable you to create advanced workflows that follow your business logic. Okay, so before we go into the demo and check out our desktop flow in action, here's the scenario that we have built it for. Our client, Globomantics, has a need to automate the business process around creating invoices. However, the process uses a legacy application with no APIs. So you need to automate invoice creation in the legacy invoicing application using robotic process automation. For the purpose of this demo, we will actually use the Contoso Invoicing app, a simple legacy app provided by Microsoft in order to be able to practice desktop flows. You can download it from Microsoft GitHub at the link provided in the slides. And remember, you can download the slides for this module from the Exercise File tab on Pluralsight.

Demo: Desktop Flow in Action 

Now that we have seen the theory, let's head over to the lab and check out the desktop flow in action. I am now in the demo environment. Let me open up Power Automate desktop over here, which is the tool we use to create and manage desktop flows. Let me go and create a brand‑new flow. I will call it RPA Demo. Let's click on Create. It will take a few seconds. It will open up in a pop‑up window. Let me bring this to the middle here. And as you can see, the creation user interface is a bit different. I have all of the different actions on the left, for example. On the System, I can say, okay, you know what, run an application. I can tell it what to do, so still, we're creating a workflow, we're telling the system what to do, but we're not necessarily connected to online system with APIs. So let me go and tell it what application to open. I want the Contoso invoicing application here. Great. And then I can say, for example, do I want the window style to be normal, hidden, minimized, maximized? Do I want to wait for the application to load before continuing? Probably. Let me click on Save. I can just run it like this if I want to and see if this works, for example, and as you can see, it did, it actually opened up the app. Now, remember, I told you there are two ways to actually tell the system what to do. You can either add the actions one by one, like this, and go, for example, into UI automation here and say, okay, you know what, click UI element of a window and really add all of the things you wanted to click, hover over, type, and things like that, or you can use the recorder. So let's actually use the recorder to show that as an example. Here I have the recorder. Let me bring it next to the Contoso Invoicing application, and once I click Record over here, it will really record every step I make, every breath I take. The recorder would log the actions, and then, after that, I will get a list of actions. I can pick out the ones that I do not need any more, and then we'll see how the flow works. But let me click on Record right now. Great! You will see if I click even on the title bar here, it will log an action. Let's click on Invoices. So you see on the left, it says Left click on Text 'Invoices,' so it really will add everything. Now I will click the New Record button. It logged that as well. Let's click on account here. We will enter the word Globomantics. For the contact, we will enter vlad@globomantics.org. Then for the Amount let's enter $10,000. For the Status, let's put it to Invoice, and finally click on Save. Did it take this one here? Let's do it again just to make sure. There we go. Click on Save, and it's done. Now let me click on Done, and what this will do, it will add all of the different actions that we just did back to my workflow. Let's close the application for now. Remember I clicked in the title bar just to show you the action. We don't need that anymore. So now I can go and I can say, hey, delete that action. Let's actually just run it now. If everything goes well, it should just go in and create a new invoice with the same information. Let's click on Run and see what happens. So it's opening the app, clicked on Invoices, clicked on New, it's adding everything faster than I can talk. It will set it to invoice, and finally it clicked on Save and it's done. So really, we're able to use robotic process automation to watch me do something, and then now it can repeat it over and over again with just a button without me having to do anything and at a way faster speed than I can possibly do it on my own. And that is really the beautiful thing about robotic process automation. Now let's take a look at something a bit more advanced. This is the general concept of robotic process automation, but I want to show you something even cooler. Let's not save this flow, and we will take a look at the Globomantics Invoicing flow, which I already created here a while ago. It's very similar, but what we have done in this flow a bit differently is that instead of actually always typing in the same information, we also used variables, for example, here in order to be able to decide what kind of information we give it. Also, at the end, we actually get what is the invoice number from the internal system, and we output that. We could run that on its own. So if I click on Run over here, it will ask me what do I want to enter? Let me change things up a bit. Let's put Pluralsight for the client name, and then the invoice amount $15,000. Let's click on OK, and then it will run the app, and at the end, now we're creating invoice number 1038. It should actually give me that as an output. At the end, there we go, the invoice number is 1038. If we take this one level further even, you can integrate desktop flows with cloud flows. So, your system doesn't have to be all modern or all legacy. Your system or your process can touch both modern and legacy systems. So what I have created here is a flow that when an item is created in a SharePoint list, it will actually go and run a flow built with Power Automate for desktop. So it'll run my robotic process automation flow. Then, it will send an email and it will update the list with the internal invoice number from our legacy system. So this is a great example on how we can merge and combine both legacy and modern systems by using Power Automate. So let's try it out. Let me go in SharePoint here. Let me add the ExternalInvoiceNumber. Let me put Globomantics‑2022‑01, Client Name Globomantics, Client Contact john@globomantics.org, and the Invoice Amount will be $49,000. It's a really big one. Now let's click on Save, and you will notice the internal invoice is still empty over here because that's something that we want to get from our robotic process automation, so from our system. It will take a few seconds for it to run. Let's give it some time here for it to get triggered. Again, as you know, it takes between a few seconds to a few minutes before it starts. It should start any time now, but I'll pause the recording until it starts and then I'll restart it as soon as the workflow has started. There we go. The flow just started. Let me go inside. Let's see what's happening. So when an item is created, now it's waiting to run a flow built on Power Automate for desktop. What I will do now is I will stop touching the mouse, and we will kind of let the magic do its thing. And for this demo, I'm actually running an attended desktop flow, so I'm in attended mode. That means I need to have my session open for it to run. You can see it's creating invoice number 1039, adding the right information from the SharePoint list that we were able to pass to Power Automate desktop. It should get updated pretty soon. And in about 10 seconds, I guess, we will be able to go to SharePoint and check out the information. There we go. The flow has finished successfully. Now if I go to SharePoint, I have my invoice number 1039 that just got added. So this is a good example on how you can combine desktop flows and cloud flows together. This is it for this demo in which we have showcased what desktop flows are and what robotic process automation is, especially in Microsoft Power Automate. This is the last demo of this module, so let's head back over to the slides and finish off this module.
Module conclusion

This is it for this module. But before we're done, let's review what we have learned. We have first done an introduction to robotic process automation, which, unlike modern automation tools with APIs, is just robots that copy the mouse and keyboard actions of a human user and do them over and over. This is very useful for legacy applications that have no APIs. We have then looked at Microsoft's RPA solution, which is desktop flows in Power Automate. By introducing RPA inside Microsoft Power Automate, enterprises can really automate all of their business processes using a single product. And after that, we have witnessed a desktop flow in action. We have also learned the differences between attended and unattended flows. This is it for this module, which is also almost the last module of this course. But before we're done with the course, next up, we have a small conclusion module in which we will review everything that we have learned and also offer more resources to help you learn more about the Microsoft Power platform.

Course Conclusion 

Hello, and welcome to the final module of this Demonstrating Business Value of Power Automate course. In this Course Conclusion module, we will do a quick review of everything that we have learned and then share some other courses which might be interesting for you to learn more about the Power Platform. This course was all about Microsoft Power Automate, a cloud‑based service part of the Microsoft Power Platform that makes it practical and simple for line‑of‑business users to build workflows that automate time‑consuming business tasks and processes across applications and services. We have learned, as well as seen firsthand how anyone with a basic knowledge of technology can automate business processes and can create powerful workflows without any single line of code. IT professionals and developers have the tools required to take it to the next level, but Power Automate is really made to empower everyone to become a problem solver. We have learned about the three types of flows in Power Automate, cloud flows, business process flows, and desktop flows, and we have actually seen all of them in this course. We have learned many different terms that are useful as you work with Power Automate, such as connectors, triggers, and actions. And we have actually started by talking about cloud flows, and we have learned about the three types of cloud flows, automated, instant, and scheduled, which, as we learned, actually only depend on the trigger. And we have learned how to create cloud flows starting both from a template, as well as from scratch. We have then learned the use cases for the different logic operators in Power Automate cloud flows, such as loops, expressions, conditions, and switch statements, which allow your flows to really respect and follow your business logic. After we learned about cloud flows, we dive deeper into business process flows, which is a series of ordered work steps that a user completes within a business process. We have learned the different terms, such as stages, steps, and flow steps, and we have also learned about the benefits of business process flows over cloud flows, as well as when we would use a business process flow. If we recap, it's when you want to provide a visual guided experience, like a recipe, in order for users to complete a business process by breaking it down into small stages and steps that users have to do. Features like stage gating, conditional branching, make sure that your process is always followed. Afterwards, we have talked about robotic process automation. We have started with an introduction and then talked about how Power Automate desktop flows can enable us to implement RPA. We have also talked about when to use what, and while each case is unique, generally speaking, you should use cloud flows, which use APIs and connectors whenever possible, and desktop flows for legacy apps that cannot be automated without robotic process automation. We have also learned about the two methods for running flows, either attended or unattended. Attended flows run on active Windows user sessions, while unattended flows are able to run on a dedicated machine and take care of logging in, doing their job, and logging out. We have also briefly talked about the licensing differences between those two ways of running desktop flows. Okay, now that we have reviewed everything, what's next? Something that can be very interesting to learn are the rest of the products in the Power Platform, Power Apps, Power BI, Power Virtual Agents, as well as Microsoft Dataverse. There's plenty of content for all those products on Pluralsight. Something else that I highly encourage you to do is to go use the Power Apps developer plan and practice for free. Use this Microsoft offering to really try everything out in your dev environment where you can connect to different services and, as I like to joke, break everything that you want because that's what dev environments are for. Something else that is usually very beneficial is to join different communities and forums that share content around the Power Platform. On this slide, you first have the Power Automate Community, which is a great active forum around Power Automate. Second, there are many Power Automate and Power Platform user groups around the world. This is a fairly new portal that Microsoft launched to make finding user groups easier, so make sure you check it out. There might be a user group in your area. Lastly, check out the official Power Automate blog where Microsoft shares the latest news around the Power Platform, so you can make sure that you see what the latest innovations are. Lastly, if you're passionate about the Power Platform, and I really hope this course got you excited, you should check out the different Power Platform certifications. There are three main certifications I recommend looking at, Power Platform Fundamentals, Power Platform App Maker Associate, and Power Platform Functional Consultant Associate. There is a certification learning path for all of those on Pluralsight, and actually, this course is part of the Power Platform Fundamentals certification path on Pluralsight, so I, of course, recommend checking out the rest of the courses in this path. And even if you don't want to do the other certifications, each one of those certification packs has a Power Automate course. So here we are really in the fundamentals. In the PL‑100, you will get a Power Automate course that is a bit more advanced, and for the PL‑200, you will see a Power Automate course that is even more advanced than that, so even if you don't want to do the certifications, the content in those packs will be useful to your Power Automate and Power Platform skills. Finally, I would also like to introduce you to a really nice feature on Pluralsight. You can now follow authors, so if you have enjoyed this course and want to get a notification when I create new courses, please go to my profile and click on Follow. On the last note, I just want to say a huge thank you for listening to this course. I really hope you have enjoyed listening to it as much as I enjoyed creating it. You have all of my socials out there, Twitter, LinkedIn, YouTube, blog. Please follow me, or connect with me on LinkedIn. I love to see all of the things you are creating, and I do my best to share interesting stuff as well. If you ever see me speak at one of the conferences you're attending, don't be a stranger and come say hi, and don't forget to check my Pluralsight author page to see all of the courses that I have on the platform. Thank you very much again for listening to this course!




the toolkit and distribution plan outlined, has every core component for soloprenuerships:
- **Development & Testing**: CursorAI as a versatile IDE, with Replit for cloud-based development and testing, plus Snyk for security, means you’ll have a well-rounded environment.
- **Brainstorming & Coding Assistance**: Claude Pro and GPT Pro give you strong support for brainstorming, code generation, and problem-solving.
- **UI/UX Design**: V0 by Vercel (or Figma if you later decide to add it) handles front-end design, which is key for delivering polished, professional apps.
- **Snippet Management**: Pieces or GitHub keeps your reusable code organized, helping you improve efficiency over time.

Your workflow will allow you to handle all stages from ideation through production, including scaling up as needed. This setup empowers you to operate as a fully independent software developer, handling every aspect of development yourself!



